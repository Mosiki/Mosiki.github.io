[{"title":"【对你无用请揍我系列】必会 Linux 技巧","date":"2020-03-28T10:46:37.000Z","path":"linux-command.html","text":"前言本文非单纯网上介绍 Linux 命令及参数的雷同文章，而是真正在实际工作中能派上用场的 Linux 实用技巧。主要介绍日志排查相关的技巧，捎带常见的其他 Linux 操作以及 vim 技巧。 本文首发于个人博客：http://nullpointer.pw/linux-command.html 日志相关：在日常工作中，经常需要在服务器上通过查看日志的方式来定位 bug 问题，掌握一定的日志排查技巧则是很有必要的。明明有些可以通过很基础的命令就可以实现，但网上有些人偏偏喜欢炫技，故作高深弄出一些特别复杂的用法来完成，其实真的没有那种必要。本文只讲最常用实用的用法。 笔者截取了一些日志作为示例，以供读者练习。 Tips: 命令中包含于双引号 “ “ 中的都是关键字 示例日志点我下载 静态日志查找 根据关键字查找日志 1grep \"1584345069121\" info.log 根据关键字查找日志，并显示查找结果附近 5 行(前5行与后5行)日志 1grep \"1584345069121\" info.log -C5 根据关键字查找日志，并显示查找结果后 (After)5 行日志 1grep \"1584345069121\" info.log -A5 根据关键字查找日志，并显示查找结果前 (Before)5 行日志 1grep \"1584345069121\" info.log -B5 根据多个关键字查找日志，多个关键字为或关系，满足其中一个条件则匹配，关键字之间用\\|分隔，表示或 1grep \"1584345069121\\|1584345066236\" info.log 根据多个关键字查找日志，需要满足其中全部条件才匹配 1grep \"76953334\" info.log | grep \"1584349673402\" 查找不包含关键字的日志，支持多个关键字，关键字之间用\\|分隔，表示或 1grep -v \"handle\" info.log 根据关键字 A 查找，并且过滤包含关键字 B 的日志 1grep \"76953334\" info.log | grep -v \"handle\" 滚动日志查找除了以上在静态日志中查找之外，有时候调试需要看指定日志，而日志又存在滚动刷新较快的情况，此时就可以通过 grep 相关技巧对刷新的滚动日志进行过滤筛选，来达到我们的目的。 滚动日志中根据关键字搜索日志 1tail -300f info.log | grep &quot;76953334&quot; 滚动日志中根据【关键字 A】搜索日志，且过滤掉包含【关键字 B】的日志 1tail -300f info.log | grep &quot;76953334&quot; | grep -v &quot;2450&quot; 解压缩压缩解压也是比较常用的一些操作，常见的压缩包就是tar.gz与zip两种，下文分别介绍这两种格式的压缩与解压方式。 gzip 压缩文件或目录，具体参数含义见参考链接 1tar zcvf tempdir/ fileName.tar.gz 解压 tar.gz 格式 (gzip) 压缩包 (与压缩命令比较只是参数 c 改成了 x) 1tar zxvf fileName.tar.gz zip 压缩文件或目录（-r 表示包含子目录） 1zip -r fileName.zip tempdir/ zip 解压到指定目录（-o 表示重名文件覆盖时不提示，可选参数） 1unzip -o fileName.zip -d tempdir2/ 进程相关工作中偶尔会需要看一下应用进程的信息，比如什么时候启动的，启动了多久这些信息，以下命令可以告诉你答案。 查看端口是否被占用 1netstat -l | grep 8080 查看进程启动时间 1ps -p 8080 -o lstart 查看进程运行多长时间 1ps -p 8080 -o etime VIM 小技巧列举了几个 VIM 小技巧，批量注释行、替换操作 批量注释行 control + v 进入列模式 选择要注释的列 shift + i 进入 INSERT 模式 输入# 或其他你想要插入的文字 连续按两次 ESC 批量取消注释行 control + v 进入列模式 选中要删除的文字 d 键进行删除 搜索并替换关键字 (INFO 替换为 ERROR)全文替换： vim 正常模式下 :%s/INFO/ERROR 回车确定当前行替换：vim 正常模式下 :s/INFO/ERROR 回车确定 拷贝与清空全文内容拷贝全文内容：vim 正常模式下 :%y清空全文内容：vim 正常模式下 :%d 系统相关 从路径 /home 下开始向下遍历查找名称包含 nginx.conf 的文件 1find /home -name nginx.conf 查看磁盘占用 1df -h 查看当前目录下文件或目录占用空间 1du -h --max-depth=1 其他文章笔者写的一些在 Spring 容器中关于设计模式的实践文章 设计模式之策略模式实战 设计模式之状态模式实战 设计模式之责任链模式实战 设计模式之观察者模式实战 参考 每天一个 linux 命令（28）：tar 命令 Linux zip 命令 在 Vim 中优雅地查找和替换 那些鲜为人知的 Vim 小技巧","tags":[]},{"title":"设计模式之观察者模式实战","date":"2020-03-05T14:18:10.000Z","path":"design-patterns-observer.html","text":"本文介绍观察者模式在项目中的实际应用。 类型：行为型模式 意图：一对多关系依赖的多个对象，当一个对象状态发生改变，所有依赖的对象都可以得到通知并自动更新 主要解决：降低对象间的关联依赖性 观察者模式也称为发布订阅模式，监听器模式。 设计模式系列文章目录 角色 抽象主题（Subject）角色：抽象主题角色提供维护一个观察者对象聚集的操作方法，对聚集的增加、删除等。具体主题（ConcreteSubject）角色：将有关状态存入具体的观察者对象；在具体主题的内部状态改变时，给所有登记过的观察者发出通知。具体主题角色负责实现抽象主题中聚集的管理方法。抽象观察者（Observer）角色：为具体观察者提供一个更新接口。具体观察者（ConcreteObserver）角色：存储与主题相关的自洽状态，实现抽象观察者提供的更新接口。 UML Java 提供观察者模式的支持一般在真实项目之中，不会完全手动实现一个观察者模式，因为在 JAVA 语言的 java.util 库里面，已经提供了一个 Observable 类以及一个 Observer 接口，构成 JAVA 语言对观察者模式的支持。直接使用提供的 util 即可。 1234567891011121314151617181920212223public class ConcreteObserver implements Observer &#123; @Override public void update(Observable o, Object arg) &#123; System.out.println(\"接收到更新\"); &#125;&#125;public class ConcreteSubject extends Observable &#123; public void change() &#123; setChanged(); this.notifyObservers(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ConcreteObserver concreteObserver = new ConcreteObserver(); ConcreteSubject subject = new ConcreteSubject(); subject.addObserver(concreteObserver); subject.change(); &#125;&#125; 实战除了 Java 自身 提供的观察者模式支持外，Guava 也基于观察者模式实现的 生产/消费模型，在使用上，比 Observable 相对简单，如果需要订阅消息只需要在方法上添加 @Subscribe 注解即可。使用 EventBus 的 post 方法分发事件给消费者。 本文以用户注册为例，当用户注册完成后，之后可能会有一系列的耗时操作，比如发送消息通知，同步数据到缓存等操作。为了给用户提供好的体验，这里使用 EventBus 来进行异步化。 定义抽象主题123456789public abstract class AbstractProducer&lt;T&gt; &#123; public static final AsyncEventBus eventBus = new AsyncEventBus(\"_event_async_\", Executors.newFixedThreadPool(4)); public void registerAsyncEvent(EventConsumer consumer) &#123; eventBus.register(consumer); &#125; public abstract void post(T event);&#125; 定义抽象观察者123public interface EventConsumer&lt;T&gt; &#123; void consume(T event);&#125; 封装 Event 事件对象Event 事件对象用于事件宣发时参数传递使用。 12345678910111213@Datapublic class UserRegisterEvent &#123; private UserDto userDto;&#125;@Data@ToString@AllArgsConstructorpublic class UserDto &#123; private Long id; private String name; private LocalDateTime registerTime;&#125; 定义具体的主题与观察者对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 主题对象，提供注册主题方法@Componentpublic class UserRegisterProducer extends AbstractProducer&lt;UserRegisterEvent&gt; &#123; @Override public void post(UserRegisterEvent event) &#123; eventBus.post(event); &#125;&#125;// 观察者对象，监听主题事件@Componentpublic class UserRegisterNotifyConsumer implements EventConsumer&lt;UserRegisterEvent&gt;, InitializingBean &#123; @Resource private UserRegisterProducer userRegisterProducer; @Override @Subscribe // 监听事件 public void consume(UserRegisterEvent event) &#123; System.out.println(\"接收到用户注册事件，开始推送通知\"); System.out.println(event); System.out.println(\"接收到用户注册事件，通知推送完毕\"); &#125; @Override public void afterPropertiesSet() &#123; userRegisterProducer.registerAsyncEvent(this); &#125;&#125;// 观察者对象，监听主题事件@Componentpublic class UserRegisterSyncCacheConsumer implements EventConsumer&lt;UserRegisterEvent&gt;, InitializingBean &#123; @Resource private UserRegisterProducer userRegisterProducer; @Override public void afterPropertiesSet() &#123; userRegisterProducer.registerAsyncEvent(this); &#125; @Override @Subscribe // 监听事件 public void consume(UserRegisterEvent event) &#123; System.out.println(\"接收到用户注册事件，开始同步 Cache\"); System.out.println(event.getUserDto()); System.out.println(\"接收到用户注册事件，同步 Cache 完毕\"); &#125;&#125; 注意，观察者对象需要在类进行实例化的时候，进行注册事件，所以实现了 InitializingBean 接口，监听事件消息，需要在监听方法上添加 @Subscribe 注解。 测试12345678910111213@RunWith(SpringRunner.class)@SpringBootTestpublic class UserRegisterProducerTest &#123; @Resource private UserRegisterProducer userRegisterProducer; @Test public void post() &#123; UserRegisterEvent event = new UserRegisterEvent(); event.setUserDto(new UserDto(1L, \"张三\", LocalDateTime.now())); userRegisterProducer.post(event); &#125;&#125; 结果输出： 123456接收到用户注册事件，开始推送通知接收到用户注册事件，开始同步 CacheUserDto(id=1, name=张三, registerTime=2020-03-05T22:12:26.386)接收到用户注册事件，同步 Cache 完毕UserRegisterEvent(userDto=UserDto(id=1, name=张三, registerTime=2020-03-05T22:12:26.386))接收到用户注册事件，通知推送完毕 因为观察者模式是无法控制消费顺序的，可能每次的输出结果都是不一致的。 示例代码 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns 参考 https://juejin.im/post/5bcf53f351882577e5120b99 https://www.cnblogs.com/java-my-life/archive/2012/05/16/2502279.html","tags":[]},{"title":"设计模式之责任链模式实战","date":"2020-03-03T23:29:43.000Z","path":"design-patterns-chain-responsibility.html","text":"本文以电商系统订单金额计算为例，电商订单最终的金额可能是这样的 应支付金额=订单金额-优惠券优惠金额-促销活动优惠金额-会员权益优惠金额 当然也有可能还会增加其他的计算步骤，使用责任链模式来实现订单金额计算，若增加了其他计算步骤，直接将步骤加入到链中即可，而无需改动以前的代码，最大程度减小出错的可能性。责任链分为纯责任链与不纯责任链，在日常开发中，很少有纯的责任链，所谓纯的责任链，就是单个链上处理器要么独立处理，要么处理不了交给下一个处理器进行处理。 设计模式系列文章目录 类型：行为型模式 意图：为请求创建了一个接收此次请求对象的链，让请求沿着链传递请求。 使用场景： 需要多个对象处理一个请求 角色： AbstractHandler: 抽象处理者 ConcreteHandler：具体处理者 UML 实战开发本文参照 Tomcat 源码中的 Filter Chain 实现责任链，使用多个订单处理器对订单进行处理。 本文示例 UML 图 实现抽象处理者为了简化示例，代码中关于优惠金额的计算都采取写死的方式。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * @author WeJan * @since 2020-02-11 */public abstract class AbstractHandler &#123; protected abstract void doHandle(OrderHandleContext context, OrderHandlerChain chain); /** * 订单处理器的权重 */ protected abstract int weight();&#125;/** * 封装处理器链处理元素上下文 * * @author WeJan * @since 2020-02-11 */@Data@Accessors(chain = true)public class OrderHandleContext &#123; /** * 当前处理器位于处理器 chain 上的位置 */ private int pos; // 订单号 private String orderNo; // 订单金额 private Double amount; // VIP 等级 private Integer vipLevel; // 优惠券 private String couponNo;&#125;/** * 封装处理器权重 * @author WeJan * @since 2020-03-04 */@Getter@AllArgsConstructorpublic enum OrderHandlerWeightEnum &#123; COUPON(1, \"优惠券\"), SALES(2, \"促销活动\"), VIP(3, \"VIP\"), ; private Integer code; private String desc;&#125; 初始化订单处理器链12345678910111213141516171819202122232425/** * @author WeJan * @since 2020-03-03 */@Componentpublic class OrderHandlerChain implements ApplicationContextAware &#123; private List&lt;AbstractHandler&gt; chain = new ArrayList&lt;&gt;(10); public void handle(OrderHandleContext context) &#123; if (context.getPos() &lt; chain.size()) &#123; AbstractHandler handler = chain.get(context.getPos()); // 移动位于处理器链中的位置 context.setPos(context.getPos() + 1); handler.doHandle(context, this); &#125; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; Map&lt;String, AbstractHandler&gt; beans = applicationContext.getBeansOfType(AbstractHandler.class); chain.addAll(beans.values()); // 根据处理器的权重，对处理器链中元素进行排序 chain.sort(Comparator.comparingInt(AbstractHandler::weight)); &#125;&#125; 实现具体处理者12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Componentpublic class CouponOrderHandler extends AbstractHandler &#123; @Override protected void doHandle(OrderHandleContext context, OrderHandlerChain chain) &#123; if (context.getCouponNo() != null) &#123; context.setAmount(context.getAmount() - 10); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return OrderHandlerWeightEnum.COUPON.getCode(); &#125;&#125;@Componentpublic class SalesOrderHandler extends AbstractHandler &#123; @Override protected void doHandle(OrderHandleContext context, OrderHandlerChain chain) &#123; Double amount = context.getAmount(); if (amount &gt; 80) &#123; context.setAmount(amount * 0.9); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return OrderHandlerWeightEnum.SALES.getCode(); &#125;&#125;@Componentpublic class VipOrderHandler extends AbstractHandler &#123; @Override protected void doHandle(OrderHandleContext context, OrderHandlerChain chain) &#123; if (context.getVipLevel() &gt; 2) &#123; context.setAmount(context.getAmount() - 5); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return OrderHandlerWeightEnum.VIP.getCode(); &#125;&#125; 测试123456789101112131415161718@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)public class ChainTest &#123; @Resource private OrderHandlerChain chain; @Test public void name() &#123; OrderHandleContext order = new OrderHandleContext() .setOrderNo(\"123\") .setAmount(100d) .setVipLevel(3) .setCouponNo(\"111\"); chain.handle(order); System.out.println(\"订单最终金额为： \" + order.getAmount()); &#125;&#125; 输出结果为：176.0 处理器链调用 handle 方法，逐个调用处理器链中的处理器的 doHanle 方法，对订单进行处理，当前处理器处理完毕后，可以选择是否继续交由下一个处理器进行处理，即设置 chain.handle(context);，如果不需要继续往下处理，不调用此代码即可。 网上流传的代码都是直接在抽象处理器中包含下一个处理器的引用，这样导致在客户端使用的时候，就需要手动去逐个 set 下级处理器，手误很容易造成处理器死循环的情况，也可能出现缺失某个处理器的情况，因而本文参照 Tomcat 源码中 Filter 的作法，引入了 Chain 类，统一对处理器封装为链，减少客户端使用时出错的可能。 总结链式处理的好处在于增加减少新的处理器不会影响其他处理器的逻辑，各个处理器之间相互独立，可以减小耦合带来的影响。 源码下载 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns 参考链接 设计模式 | 责任链模式及典型应用 https://www.kancloud.cn/sstd521/design/193560","tags":[{"name":"结合Spring实践责任链模式","slug":"结合Spring实践责任链模式","permalink":"http://nullpointer.pw/tags/结合Spring实践责任链模式/"}]},{"title":"设计模式之门面模式实战","date":"2020-03-02T22:35:41.000Z","path":"design-patterns-facade.html","text":"类型：结构型模式 意图：定义一个统一的系统接口，隐藏多个子系统的复杂性。 主要解决：降低访问系统客户端内部子系统的复杂度。 使用场景：解耦合客户端与具体子系统，子系统与客户端无需感知对方，客户端不需要亲自调用子系统的接口，把原本需要与多个子系统的交互全部封装到门面类中，客户端只需要和 Facade 类交互即可。 设计模式系列文章目录 角色 Facade： 门面角色，委托客户端的请求到具体的客户端 SubSystem：子系统角色 UML使用门面模式封装之前 UML： 使用门面模式封装之后 UML： 可以看到客户端由依赖三个对象变成依赖一个对象，通过门面对象封装子系统，对于客户端，屏蔽子系统内部的复杂性。 需要注意的是，门面类不要参与任何具体的业务逻辑。 实战在大量使用 Dubbo 进行接口调用的项目系统中，有很多 Dubbo 方法是属于公用的方法，虽然客户端类可以注入不同的 Dubbo 服务类进行接口调用，但是明显客户端与其他子系统的依赖过于紧密。 当一个子系统需要进行修改时，需要修改所有调用此系统的接口，此时最适合使用门面模式来封装这些子系统的接口，然后统一对客户端提供方法进行调用，大大降低客户端与子系统间的耦合性。 本文为了简化示例，就不未引入 Dubbo，而是直接 Mock 了两个远程服务类。示例提供一个针对获取用户相关信息的门面类。 示例代码定义两个 Mock 的远程服务类123456789101112131415161718192021@Servicepublic class MockUserRemoteService &#123; public boolean isNewUser(Long userId) &#123; // TODO 调用远程接口 return true; &#125; public UserInfo getUserInfo(Long userId) &#123; // TODO 调用远程接口 return new UserInfo(); &#125;&#125;@Servicepublic class MockDeptRemoteService &#123; public DeptInfo getDeptInfo(Long userId) &#123; // TODO 调用远程接口 return new DeptInfo(); &#125;&#125; 定义用户门面类1234567891011121314151617181920212223242526272829@Componentpublic class UserFacade &#123; @Resource private MockUserRemoteService userRemoteService; @Resource private MockDeptRemoteService deptRemoteService; /** * 判断是否是新用户 */ public boolean isNewUser(Long userId) &#123; return userRemoteService.isNewUser(userId); &#125; /** * 获取用户信息 */ public UserInfo getUserInfo(Long userId) &#123; return userRemoteService.getUserInfo(userId); &#125; /** * 获取用户部门信息 */ public DeptInfo getDeptInfo(Long userId) &#123; return deptRemoteService.getDeptInfo(userId); &#125;&#125; 测试客户端调用12345678910111213@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)public class FacadeTest &#123; @Resource protected UserFacade userFacade; @Test public void test1() &#123; long userId = 11L; UserInfo userInfo = userFacade.getUserInfo(userId); DeptInfo deptInfo = userFacade.getDeptInfo(userId); &#125;&#125; 总结门面模式还是很好理解的，封装了子系统的复杂性，通过门面对象只暴露客户端需要的接口，一些内部的接口可以做到对客户端隐藏。 门面模式的优点 门面模式的优点： ● 松散耦合 门面模式松散了客户端与子系统的耦合关系，让子系统内部的模块能更容易扩展和维护。 ● 简单易用 门面模式让子系统更加易用，客户端不再需要了解子系统内部的实现，也不需要跟众多子系统内部的模块进行交互，只需要跟门面类交互就可以了。 ● 更好的划分访问层次 通过合理使用 Facade，可以帮助我们更好地划分访问的层次。有些方法是对系统外的，有些方法是系统内部使用的。把需要暴露给外部的功能集中到门面中，这样既方便客户端使用，也很好地隐藏了内部的细节。 源码下载 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns 参考 门面模式的注意事项 《JAVA 与模式》之门面模式","tags":[]},{"title":"设计模式之状态模式实战","date":"2020-02-29T06:06:32.000Z","path":"design-patterns-state.html","text":"本文以运营活动状态转换为例，结合 Spring 演示状态模式的实践应用。 类型：行为型模式 意图：允许对象在内部状态发生改变时改变它的行为，对象看起来好像修改了它的类。 主要解决：一个对象存在多个状态，每个状态行为不同，状态可以相互转换。 使用场景：1、行为随状态改变而改变的场景。 2、减少 switch..case 以及 if…else 设计模式系列文章目录 角色 State：抽象状态角色，负责对象状态定义，并且封装环境角色以实现状态切换。 Context：环境角色，定义客户端需要的接口，并且负责具体状态的切换。 ConcreteState：具体状态角色，当前状态要做的事情，以及当前状态如何转换其他状态。 UML 实战本文以运营活动状态为例，结合 Spring 演示状态模式的实践应用。 运营活动创建初始状态为草稿状态，编辑好活动之后，运营会后台启用活动，此时活动状态为已启用；当达到活动开始时间时，定时任务会将活动状态置为进行中；当达到活动结束时间时，定时任务会将活动状态置为已结束。进行中的活动也可能会因为某些原因需要手动停用，此时活动状态置为已停用。 状态之间有着严格的前置校验，比如草稿状态可以继续保存为草稿，也可以进行启动，但不能直接切换为进行中，可以直接编辑切换回草稿箱状态；比如已停用的状态只有在启用之后才能被置为进行中。 活动状态的切换约束如下图： 新状态→ 当前状态↓ 草稿箱 已启用 进行中 已停用 已结束 草稿箱 ✅ ✅ ❌ ❌ ❌ 已启用 ✅ ❌ ✅ ✅ ❌ 进行中 ❌ ❌ ❌ ✅ ✅ 已停用 ❌ ✅ ❌ ❌ ❌ 已结束 ❌ ❌ ❌ ❌ ❌ 如果不采取状态模式，可能写出的代码就是不断使用 if 判断前置状态是否符合规则，当增加了新的状态，需要改动判断的地方，从而可能引入了 Bug。 本文示例 UML 图 示例代码定义抽象状态角色123456789101112131415161718192021222324252627282930313233343536373839404142public abstract class ActivityState &#123; // 抽象状态角色需要持有环境上下文对象 protected ActivityContext activityContext; public void setActivityContext(ActivityContext activityContext) &#123; this.activityContext = activityContext; &#125; public abstract Integer type(); /** * 判断是否是当前状态 */ protected boolean isSameStatus(Activity activity) &#123; return type().equals(activity.getStatus()); &#125; /** * 保存草稿 */ public abstract boolean saveDraft(Activity activity); /** * 启用 */ public abstract boolean enable(Activity activity); /** * 开始 */ public abstract boolean start(Activity activity); /** * 停用 */ public abstract boolean disable(Activity activity); /** * 停止 */ public abstract boolean finish(Activity activity);&#125; 定义环境角色12345678910111213141516171819202122232425262728293031public class ActivityContext &#123; // 持有抽象状态角色引用 private ActivityState activityState; public void setActivityState(ActivityState activityState) &#123; this.activityState = activityState; this.activityState.setActivityContext(this); &#125; public boolean saveDraft(Activity activity) &#123; // 委托具体的状态角色 return this.activityState.saveDraft(activity); &#125; public boolean enable(Activity activity) &#123; return this.activityState.enable(activity); &#125; public boolean start(Activity activity) &#123; return this.activityState.start(activity); &#125; public boolean disable(Activity activity) &#123; return this.activityState.disable(activity); &#125; public boolean finish(Activity activity) &#123; return this.activityState.finish(activity); &#125;&#125; 定义具体状态角色因为本文示例具体状态角色有很多，因此只列举一个开启状态角色举例参考，更多代码可以参考本文对应的 GitHub 示例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Componentpublic class ActivityEnableState extends ActivityState &#123; @Resource private ActivityDraftState activityDraftState; @Resource private ActivityStartState activityStartState; @Resource private ActivityDisableState activityDisableState; @Override public Integer type() &#123; return ActivityStateEnum.ENABLE.getCode(); &#125; @Override public boolean saveDraft(Activity activity) &#123; super.activityContext.setActivityState(activityDraftState); return activityContext.saveDraft(activity); &#125; @Override public boolean enable(Activity activity) &#123; // 如果当前状态已经是 enable 了，则无法再次 enable if (isSameStatus(activity)) &#123; return false; &#125; activity.setStatus(type()); //TODO 更新数据库 return true; &#125; @Override public boolean start(Activity activity) &#123; super.activityContext.setActivityState(activityStartState); return activityContext.start(activity); &#125; @Override public boolean disable(Activity activity) &#123; super.activityContext.setActivityState(activityDisableState); return activityContext.disable(activity); &#125; @Override public boolean finish(Activity activity) &#123; // 非进行中的活动状态，不允许直接进行 finish return false; &#125;&#125; 封装具体状态实例工厂状态角色应该是单例的，结合 Spring 与工厂模式对实例进行封装，方便根据数据库的 status 值获取对应的状态角色实例。12345678910@Componentpublic class ActivityStateFactory implements ApplicationContextAware &#123; public static final Map&lt;Integer, ActivityState&gt; STATE_MAP = new HashMap&lt;&gt;(ActivityStateEnum.values().length); @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; Map&lt;String, ActivityState&gt; beans = applicationContext.getBeansOfType(ActivityState.class); beans.values().forEach(item -&gt; STATE_MAP.put(item.type(), item)); &#125;&#125; 测试12345678910111213141516171819202122232425262728@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)public class BaseTest &#123; @Test public void test1() &#123; // 一般活动都是从数据库查询出来了，此处为方便测试直接 new Activity activity = new Activity() .setId(1L) .setName(\"测试活动\") .setStatus(ActivityStateEnum.DRAFT.getCode()) .setCreateTime(LocalDateTime.now()); ActivityState activityState = ActivityStateFactory.STATE_MAP.get(activity.getStatus()); ActivityContext context = new ActivityContext(); context.setActivityState(activityState); System.out.println(\"保存草稿: \" + (context.saveDraft(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为已启用: \" + (context.enable(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为进行中: \" + (context.start(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为已停用: \" + (context.disable(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为已启用: \" + (context.enable(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为进行中: \" + (context.start(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为已结束: \" + (context.finish(activity) ? \"成功\" : \"失败\")); System.out.println(\"更新活动状态为进行中: \" + (context.start(activity) ? \"成功\" : \"失败\")); &#125;&#125; 结果输出：12345678保存草稿: 成功更新活动状态为已启用: 成功更新活动状态为进行中: 成功更新活动状态为已停用: 成功更新活动状态为已启用: 成功更新活动状态为进行中: 成功更新活动状态为已结束: 成功更新活动状态为进行中: 失败 可以看到状态切换路径：草稿-&gt; 草稿-&gt; 已启用-&gt; 进行中-&gt; 已停用-&gt; 已启用-&gt; 进行中-&gt; 已结束-&gt; 进行中，前面都是正确切换，但是已结束无法切换为进行中状态，从而验证了状态模式的应用。 总结看上一篇策略模式的文章中的 UML，和本文的 UML 是相同的。那么他们的区别是什么呢？策略模式是提供了可相互替换的算法，根据客户端选择一种算法指定一种行为；状态模式则包含了对象状态，根据对象状态不同，行为也不一样，即状态决定行为，将行为对应的逻辑封装到具体状态类中，在环境类中消除逻辑判断，且具体实现不可相互替换。 状态模式中，客户端角色与状态对象不需要进行交互，所有的交互都委托给环境角色进行。 源码下载 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns 参考 https://www.runoob.com/design-pattern/state-pattern.html https://www.cnblogs.com/kubixuesheng/p/5180509.html https://www.kancloud.cn/sstd521/design/193606","tags":[{"name":"Spring 状态模式实战","slug":"Spring-状态模式实战","permalink":"http://nullpointer.pw/tags/Spring-状态模式实战/"}]},{"title":"设计模式之策略模式实战","date":"2020-02-19T12:38:23.000Z","path":"design-patterns-strategy.html","text":"类型：行为型模式 意图：定义一系列算法，不同算法策略可以相互替换，并且互不影响。 主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。 使用场景：一个系统需要动态地在几种算法中选择一种。 设计模式系列文章目录 角色 策略上下文角色：持有抽象策略角色的引用，访问策略的入口 抽象策略角色 具体策略角色 UML 实战以抽奖活动发奖为例，奖品多种多样，可能是现金奖，话费奖品，实物奖等等，每种奖品的发放方式都不一样，比如现金是直接转账，话费奖品是调用运营商提供接口发放，实物奖需要人工快递寄送。在未采用策略模式之前，少不了使用 if…else…来判断发放，当增加一种奖品类型时，就需要增加 if 判断。而采取策略模式之后，只需实现一个策略类即可，对原来的逻辑无需做任何改动，也不会影响其他策略的正常逻辑。 本文示例UML图 定义抽象策略接口1234567891011121314/** * @author WeJan * @since 2020-02-06 */public interface PrizeSendStrategy &#123; String DEFAULT = \"default\"; String MONEY = \"money\"; String IN_KIND = \"in_kind\"; String CALL_CHARGE = \"call_charge\"; String type(); void doSend();&#125; 实现具体的策略123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Componentpublic class CallChargePrizeSendStrategy implements PrizeSendStrategy &#123; @Override public String type() &#123; return CALL_CHARGE; &#125; @Override public void doSend() &#123; System.out.println(\"发放话费奖品\"); &#125;&#125;@Componentpublic class MoneyPrizeSendStrategy implements PrizeSendStrategy &#123; @Override public String type() &#123; return MONEY; &#125; @Override public void doSend() &#123; System.out.println(\"发放现金奖品\"); &#125;&#125;@Componentpublic class InKindPrizeSendStrategy implements PrizeSendStrategy &#123; @Override public String type() &#123; return IN_KIND; &#125; @Override public void doSend() &#123; System.out.println(\"发放实物奖品\"); &#125;&#125;@Componentpublic class EmptyPrizeSendStrategy implements PrizeSendStrategy &#123; @Override public String type() &#123; return DEFAULT; &#125; @Override public void doSend() &#123; System.out.println(\"不发奖\"); &#125;&#125; 实现对抽象策略封装的上下文对象1234567891011121314151617/** * @author WeJan * @since 2020-02-06 */public class PrizeSendContext &#123; private PrizeSendStrategy prizeSendStrategy; public PrizeSendContext() &#123; &#125; public void setPrizeSendStrategy(PrizeSendStrategy prizeSendStrategy) &#123; this.prizeSendStrategy = prizeSendStrategy; &#125; public void executePrizeSendStrategy() &#123; prizeSendStrategy.doSend(); &#125;&#125; 抽取策略工厂客户端需要判断要使用哪一个具体的策略类，若还是按照传统的方法 if…else…来判断策略模式就没有意义了，因此策略模式一般都是结合其他模式共同使用。本文中策略类使用 String 来标识，也可以在策略类中增加抽象方法，返回值为枚举类型。 1234567891011121314@Componentpublic class PrizeSendStrategyFactory implements ApplicationContextAware &#123; private static final Map&lt;String, PrizeSendStrategy&gt; PRIZE_SEND_STRATEGY_MAP = new HashMap&lt;&gt;(); public static PrizeSendStrategy getPrizeSendStrategy(String strategyKey) &#123; return PRIZE_SEND_STRATEGY_MAP.getOrDefault(strategyKey, PRIZE_SEND_STRATEGY_MAP.get(DEFAULT)); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; Map&lt;String, PrizeSendStrategy&gt; beans = applicationContext.getBeansOfType(PrizeSendStrategy.class); beans.values().forEach(bean -&gt; PRIZE_SEND_STRATEGY_MAP.put(bean.type(), bean)); &#125;&#125; 测试12345678910111213141516/** * @author WeJan * @since 2020-02-06 */@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)public class StrategyTest &#123; @Test public void test() &#123; PrizeSendContext sendContext = new PrizeSendContext(); sendContext.setPrizeSendStrategy(PrizeSendStrategyFactory.getPrizeSendStrategy(\"money\")); sendContext.executePrizeSendStrategy(); &#125;&#125; 输出结果为： 1发放现金奖品 示例代码 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns 参考 https://www.runoob.com/design-pattern/strategy-pattern.html https://juejin.im/post/5c25b8bcf265da61117a5ea1","tags":[]},{"title":"设计模式实战目录","date":"2020-02-19T12:30:22.000Z","path":"design-patterns.html","text":"重新学习一下设计模式，掌握好设计模式对代码质量的提高还是很有效果的。 有时间就会更新。有些模式不太好想示例，所以目前的示例可能在想到更好的示例之后，会被后期替换掉。 目录 设计模式之策略模式实战 设计模式之状态模式实战 设计模式之门面模式实战 设计模式之责任链模式实战 设计模式之观察者模式实战 设计模式之命令模式实战 设计模式之模板方法模式实战 源码 https://github.com/Mosiki/learning-modules/tree/master/learning-designpatterns","tags":[]},{"title":"Hibernate Validator 校验最佳实践","date":"2020-02-04T09:13:39.000Z","path":"hibernate-validator-best-practice.html","text":"几年前刚学习 SpringBoot 的时候，就接触并开始使用 HibernateValidator 校验框架，注解校验结合统一异常处理，对代码的整洁性提高特别有帮助。但是目前发现公司里比较新的项目中对参数进行校验还是使用以前传统的方式，需要逐一对请求对象中的属性使用 if 来判断合法性，当需要校验的属性很多时，一大坨的 if 判断校验代码就不可避免。本文介绍 HibernateValidator 校验框架的日常使用，不涉及自定义约束注解。 没有对比就没有伤害首先来看一下使用校验框架前后的代码对比 使用校验框架后，代码简洁很多有木有~ 环境配置首先需要引入依赖，需要注意是否依赖有冲突，如果有一定要解决掉冲突12345&lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.1.0.Final&lt;/version&gt;&lt;/dependency&gt; 依赖引入后，Spring 环境需要配置一下校验器。12345678910111213@Configurationpublic class ValidatorConfig &#123; @Bean public Validator validator() &#123; ValidatorFactory validatorFactory = Validation.byProvider(HibernateValidator.class) .configure() // 快速失败 .failFast(true) .buildValidatorFactory(); return validatorFactory.getValidator(); &#125;&#125; 这里设置 failFast 为 true，代表着如果有多个参数的情况下，只要校验出一个参数有误就返回错误而不再继续校验。 为对象添加校验注解123456789101112131415@Datapublic class StudentVo &#123; @NotNull(message = \"学号不能为空\") private Integer id; @NotNull(message = \"姓名不能为空\") private String name; @NotNull(message = \"邮箱地址不能为空\") @Email(message = \"邮箱地址不正确\") private String email; private Integer age;&#125; Hibernate Validator 是对 JSR 349 验证规范的具体实现，相关的常用注解 我贴在文末以供参考。 请求接口处理1234567891011@PostMapping(\"/create\")public Result&lt;StudentVo&gt; create(@Validated StudentVo student) &#123; System.out.println(student.toString()); return Result.success(student);&#125;@PostMapping(\"/update\")public Result&lt;StudentVo&gt; update(@Validated StudentVo student) &#123; System.out.println(student.toString()); return Result.success(student);&#125; 注意 @Validated 是 org.springframework.validation.annotation.Validated，不要引入错了。加了这个注解之后，就会自动会参数进行校验。如果校验不通过，会抛出 BindException 或者 MethodArgumentNotValidException这两个异常中的一个异常，一般项目中为了规范接口返回值，都会进行统一异常处理。 校验异常统一异常处理1234567891011121314151617181920212223@RestControllerAdvicepublic class GlobalExceptionHandler &#123; @ExceptionHandler(BindException.class) @ResponseBody public Result validateErrorHandler(BindException e) &#123; ObjectError error = e.getAllErrors().get(0); return Result.fail(error.getDefaultMessage()); &#125; @ExceptionHandler(MethodArgumentNotValidException.class) @ResponseBody public Result&lt;?&gt; validateErrorHandler(MethodArgumentNotValidException e) &#123; ObjectError error = e.getBindingResult().getAllErrors().get(0); return Result.fail(error.getDefaultMessage()); &#125; @ExceptionHandler(Throwable.class) @ResponseBody public Result&lt;?&gt; handleException(HttpServletRequest request, Throwable ex) &#123; return Result.fail(ex.getMessage()); &#125;&#125; 因为配置校验为 failFast，因此错误信息中只会有一条记录。 第一次测试1curl -i -X POST -d \"name=张三\" 'http://localhost:8082/learning-validator/create' 只填写了 name 参数，而 id 与 email 未填写的情况下进行请求，返回结果 1&#123;\"success\":false,\"msg\":\"学号不能为空\"&#125; 按照一般开发逻辑而言，create 接口是不需要传递 id 参数的，但是 update 接口一般必须要传 id 参数，难不成用这个这个校验框架后需要写两个对象么？其实不是的。这就引出了 校验分组 的概念，即可以选择校验某些组的属性进行校验。 校验分组首先需要创建两个分组，CreateGroup 与 UpdateGroup，分别代表着新增时需要校验与更新时需要校验。两个组都是空的接口，只是作为一个标记类使用。 1234public interface CreateGroup &#123;&#125;public interface UpdateGroup &#123;&#125; 接着修改请求对象，分别指定哪些对象是哪些组需要校验的，如果不指定组，默认都会进行校验。 1234567891011121314@Datapublic class StudentVo &#123; @NotNull(groups = UpdateGroup.class, message = \"学号不能为空\") private Integer id; @NotNull(message = \"姓名不能为空\") private String name; @NotNull(groups = CreateGroup.class, message = \"邮箱地址不能为空\") @Email(groups = CreateGroup.class, message = \"邮箱地址不正确\") private String email; private Integer age;&#125; 本文示例中请求对象，id 属性只在更新时校验；name 属性无论是新增还是更新都要校验；email 属性只在新增时进行校验，而 age 属性因为未指定校验注解，因此不进行校验，这里的 groups 可以是多个分组。 指定属性的分组之后，控制器接口也需要指定使用哪一个组来进行校验。 1234567891011@PostMapping(\"/create\")public Result&lt;StudentVo&gt; create(@Validated(CreateGroup.class) StudentVo student) &#123; System.out.println(student.toString()); return Result.success(student);&#125;@PostMapping(\"/update\")public Result&lt;StudentVo&gt; update(@Validated(UpdateGroup.class) StudentVo student) &#123; System.out.println(student.toString()); return Result.success(student);&#125; 第二次测试1curl -i -X POST -d \"name=张三\" 'http://localhost:8082/learning-validator/create' 只填写了 name 参数，而 id 与 email 未填写的情况下进行请求，返回结果：1&#123;\"success\":false,\"msg\":\"邮箱地址不能为空\"&#125; 填写 name 参数以及 email 参数进行请求1234curl -i -X POST \\ -d \"name=张三\" \\ -d \"email=vcmq@foxmail.com\" \\ 'http://localhost:8082/learning-validator/create' 返回结果：1&#123;\"data\":&#123;\"name\":\"张三\",\"email\":\"vcmq@foxmail.com\"&#125;,\"success\":true,\"msg\":\"success\"&#125; 可以看到 id 这个字段在 create 的时候，并没有进行校验。修改为 update 接口进行测试。1234curl -i -X POST \\ -d \"name=张三\" \\ -d \"email=vcmq@foxmail.com\" \\ 'http://localhost:8082/learning-validator/update' 返回结果：1&#123;\"success\":false,\"msg\":\"学号不能为空\"&#125; 手动校验除了使用@Validated 注解方式校验，也可以进行手动校验，手动校验同样也支持分组校验。 123456@PostMapping(\"/create2\")public Result&lt;StudentVo&gt; create2(StudentVo student) &#123; ValidatorUtils.validateEntity(student, CreateGroup.class); System.out.println(student.toString()); return Result.success(student);&#125; 校验工具类： 1234567891011121314151617181920212223242526272829public class ValidatorUtils &#123; private static Validator validator; static &#123; validator = Validation.byProvider(HibernateValidator.class) .configure() // 快速失败 .failFast(true) .buildValidatorFactory().getValidator(); &#125; /** * 校验对象 * * @param object 待校验对象 * @param groups 待校验的组 * @throws ApiException 校验不通过，则报 ApiException 异常 */ public static void validateEntity(Object object, Class&lt;?&gt;... groups) throws ApiException &#123; Set&lt;ConstraintViolation&lt;Object&gt;&gt; constraintViolations = validator.validate(object, groups); if (!constraintViolations.isEmpty()) &#123; constraintViolations.stream().findFirst() .map(ConstraintViolation::getMessage) .ifPresent(v1 -&gt; &#123; throw new ApiException(v1); &#125;); &#125; &#125; 常用注解空与非空检查 注解 支持 Java 类型 说明 @Null Object 为 null @NotNull Object 不为 null @NotBlank CharSequence 不为 null，且必须有一个非空格字符 @NotEmpty CharSequence、Collection、Map、Array 不为 null，且不为空（length/size&gt;0） Boolean 值检查 注解 支持 Java 类型 说明 备注 @AssertTrue boolean、Boolean 为 true 为 null 有效 @AssertFalse boolean、Boolean 为 false 为 null 有效 日期检查 注解 支持 Java 类型 说明 备注 @Future Date、Calendar、Instant、LocalDate、LocalDateTime、LocalTime、MonthDay、OffsetDateTime、OffsetTime、Year、YearMonth、ZonedDateTime、HijrahDate、JapaneseDate、MinguoDate、ThaiBuddhistDate 验证日期为当前时间之后 为 null 有效 @FutureOrPresent Date、Calendar、Instant、LocalDate、LocalDateTime、LocalTime、MonthDay、OffsetDateTime、OffsetTime、Year、YearMonth、ZonedDateTime、HijrahDate、JapaneseDate、MinguoDate、ThaiBuddhistDate 验证日期为当前时间或之后 为 null 有效 @Past Date、Calendar、Instant、LocalDate、LocalDateTime、LocalTime、MonthDay、OffsetDateTime、OffsetTime、Year、YearMonth、ZonedDateTime、HijrahDate、JapaneseDate、MinguoDate、ThaiBuddhistDate 验证日期为当前时间之前 为 null 有效 @PastOrPresent Date、Calendar、Instant、LocalDate、LocalDateTime、LocalTime、MonthDay、OffsetDateTime、OffsetTime、Year、YearMonth、ZonedDateTime、HijrahDate、JapaneseDate、MinguoDate、ThaiBuddhistDate 验证日期为当前时间或之前 为 null 有效 数值检查 注解 支持 Java 类型 说明 备注 @Max BigDecimal、BigInteger，byte、short、int、long 以及包装类 小于或等于 为 null 有效 @Min BigDecimal、BigInteger，byte、short、int、long 以及包装类 大于或等于 为 null 有效 @DecimalMax BigDecimal、BigInteger、CharSequence，byte、short、int、long 以及包装类 小于或等于 为 null 有效 @DecimalMin BigDecimal、BigInteger、CharSequence，byte、short、int、long 以及包装类 大于或等于 为 null 有效 @Negative BigDecimal、BigInteger，byte、short、int、long、float、double 以及包装类 负数 为 null 有效，0 无效 @NegativeOrZero BigDecimal、BigInteger，byte、short、int、long、float、double 以及包装类 负数或零 为 null 有效 @Positive BigDecimal、BigInteger，byte、short、int、long、float、double 以及包装类 正数 为 null 有效，0 无效 @PositiveOrZero BigDecimal、BigInteger，byte、short、int、long、float、double 以及包装类 正数或零 为 null 有效 @Digits(integer = 3, fraction = 2) BigDecimal、BigInteger、CharSequence，byte、short、int、long 以及包装类 整数位数和小数位数上限 为 null 有效 其他 注解 支持 Java 类型 说明 备注 @Pattern CharSequence 匹配指定的正则表达式 为 null 有效 @Email CharSequence 邮箱地址 为 null 有效，默认正则 &#39;.*&#39; @Size CharSequence、Collection、Map、Array 大小范围（length/size&gt;0） 为 null 有效 hibernate-validator 扩展约束（部分） 注解 支持 Java 类型 说明 @Length String 字符串长度范围 @Range 数值类型和 String 指定范围 @URL URL 地址验证 示例代码 Hibernate Validator 校验最佳实践示例代码 参考 如何优雅的做数据校验-Hibernate Validator 详细使用说明","tags":[{"name":"Hibernate Validator 校验","slug":"Hibernate-Validator-校验","permalink":"http://nullpointer.pw/tags/Hibernate-Validator-校验/"},{"name":"校验统一异常处理","slug":"校验统一异常处理","permalink":"http://nullpointer.pw/tags/校验统一异常处理/"}]},{"title":"hexo 自定义文章 url 地址","date":"2020-01-26T08:32:17.000Z","path":"hexo-custom-article-url.html","text":"笔者初次通过 hexo 搭建博客时，对 hexo 不太了解，后来发现中文标题的文章 URL 会转义成http://nullpointer.pw/2018/09/17/%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%E6%89%93%E9%80%A0%E4%B8%AA%E7%AE%80%E5%8D%95%E6%90%9C%E7%B4%A2%E4%B9%8BSpringDataElasticSearch%E5%85%B3%E9%94%AE%E8%AF%8D%E9%AB%98%E4%BA%AE.html 类似模样，，，当标题比较长时，URL 就会很长，hexo 默认的 URL 是:year/:month/:title，为了减小 URL 长度，于是笔者做了一件比较傻缺的事情，将 URL 修改成了只有标题，这也造成如今 repo 列表文件过多，同时也并未解决中文标题转义的问题，因此需要自定义文章 URL。 自定义文章 URL先说说中文转义 URL 的方式，首先需要修改 hexo 根目录下的_config.yml 文件， 1234#permalink: :year/:month/:title/permalink: :year/:month/:urlname.htmlpermalink_defaults: urlname: index 将原永久链接 permalink 中的:title 替换为为 urlname.html ，同时在 permalink_defaults 增加 urlname 属性，这样就可以通过在文章内容的 Formatter 配置文章的 URL 链接地址，笔者之前修改过一次永久链接格式，只保留了标题即：permalink: :title.html，现在修改为了 permalink: :urlname.html ,本文文章配置如下图， 本文最终的 URL 地址就是：http://nullpointer.pw/hexo-custom-article-url.html 修改文章模板修改 post.md 123456789---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;urlname: tags:- 1categories:- 2--- 这样通过命令 hexo new xxxxxx命令新建的博客文章中就自动带有这个属性。 兼容旧文章 URL因为旧的文章可能已经被搜索引擎收录，如果直接粗暴修改就会造成收录的结果出现 404 的情况，还有阅读数统计一般也是根据文章 URL 来做的，笔者之前也有过因为修改文章 URL 造成所有文章阅读数归零的遭遇。 总之，这显然不是我们想要看到的情况； 所以为了保持旧的文章 URL 不变，需要对所有旧文章同样增加这个 urlname 属性，属性的值设置为当前文章的文件名即可。比如 http://nullpointer.pw/和我一起打造个简单搜索之 SpringDataElasticSearch 关键词高亮.html这篇文章为了不改变原 URL 地址，在 md 文件中修改为： 1234567891011---title: 和我一起打造个简单搜索之 SpringDataElasticSearch 关键词高亮tags: - SpringDataElasticSearch 高亮 - SpringDataElasticSearch 关键词高亮 - ElasticsearchTemplate 使用 categories: - ElasticSearchdate: 2018-09-17 21:27:03urlname: 和我一起打造个简单搜索之 SpringDataElasticSearch 关键词高亮--- 笔者改了好一会儿才改完所有文章，还好文章不是很多。文章特别多的博主，恐怕只能写个脚本批量修改了。 后记可以自定义文章 URL 的好处多多，第一个是减小 URL 长度，便于搜索引擎收录，也方便自己在其他地方直接引用，不会出现类似一串乱码 URL 的尴尬情况； 另外由于文章 URL 与文章标题关系不是特别紧密，不会造成修改标题导致 URL 改变的窘况，可以后期修改不适当的标题。 好处多多，希望对你有用~ 参考 https://printempw.github.io/hexo-posts-in-subfolder/ https://blog.csdn.net/jingbin_/article/details/80617210","tags":[{"name":"hexo-修改永久链接","slug":"hexo-修改永久链接","permalink":"http://nullpointer.pw/tags/hexo-修改永久链接/"}]},{"title":"必知必会之流程图","date":"2020-01-22T14:18:46.000Z","path":"flow-chart-draw.html","text":"流程图在各行各业都有所应用，尤其广泛应用与技术设计，产品设计。画好流程图，可以让别人能够很轻松地理解业务，特别是遇到比较复杂的业务时，用言语解释可能需要花费一上午甚至一天，但是用图去表示可能只需要 1 个小时。不多说，直奔主题。 预备知识看了网上的一些关于流程图绘制的资料，组件太多，组件含义的解释如同机翻，没有具体实例，让人无从下手，看得令人头大。其实大可不必学习所有组件，因为常用的就只有那几个组件，掌握基本的组件使用，以后可以看情况逐渐扩展。 首先眼熟一下几个常用的流程图基本组件： 再看一下流程图的结构构成： 常用的流程图就是顺序结构与选择结构二者结合的结构。在绘制流程图的过程中还需要注意如下几点，此处引用参考内容： （1）绘制流程图时，为了提高流程图的逻辑性，应遵循从左到右、从上到下的顺序排列。 （2）绘制流程图时，为了提高流程图的逻辑性，应遵循从左到右、从上到下的顺序排列。一个流程从开始符开始，以结束符结束。开始符号只能出现一次，而结束符号可出现多次。若流程足够清晰，可省略开始、结束符号。 （3）菱形为判断符号，必须要有“是和否（或Y和N）”两种处理结果，意思是说，菱形判断框一定需要有两条箭头流出；且判断符号的上下端流入流出一般用“是（或Y）”，左右端流入流出用“否（或Y）”。 （4）同一流程图内，符号大小需要保持一致，同时连接线不能交叉，连接线不能无故弯曲。 （5）流程处理关系为并行关系的，需要将流程放在同一高度。 （6）必要时应采用标注，以此来清晰地说明流程，标注要用专门的标注符号。 （7）处理流程须以单一入口和单一出口绘制，同一路径的指示箭头应只有一个。 （8）同一路径的指示箭头应只有一个。 （9）流程图中，如果有参考其他已经定义的流程，不需重复绘制，直接用已定义流程符号即可。 以上为需要储备的基础流程图知识，参考地址：画了多年的流程图，你真的画规范了吗？ 工具准备所谓工欲善其事必先利其器，绘制流程图需要趁手的兵器，我推荐两个在线绘制的网站。 ProcessOn Draw.io draw.io 提供了桌面版，目前我使用的就是这个， processOn 免费版有数量限制，不过一般也够用了。 业务说明以下笔者就以本人前段时间开发的抽奖系统实践绘制技术流程图，以供参考。 需求就是开发一个春节用的抽奖系统，代码实现上还是不难，但是步骤还是比较多的，画流程图一个原因是帮助自己理清业务设计业务，另一个原因就是可以直接将流程图提供给对应的测试，从而达到针对每个步骤进行测试的目的。 流程图实践绘制流程图的绘制规则如上文参考中的规则进行，处理组件只有一个输入一个输出，菱形判定必定有两个输出，左右端流出代表否定，不同流程使用子流程组件进行引用，连接线无交叉等。 笔者使用Draw.io绘制本图，但是常用的组件分散于不同的分类中，为了方便使用，笔者将常用的几个基本组件放到便笺本分类。这个分类可以进行导入导出，这里分享我的导出内容，可以直接拷贝出来粘贴保存到一个文件中，然后在便笺本中导入即可。 1&lt;mxlibrary&gt;[&#123;\"xml\":\"jVFBDsIgEHwN14bCxbOtevIRRNZCAkJgtfT3QkEbD008kOzM7IQZIHyw6RKEV1cnwRB+InwIzmGdbBrAGMKoloSPhDGaD2HnHbVfVepFgAf+Y2DV8BLmCZWpRMTFNEKhzbHGnvCjFFFBcdIMZqURohe3sjfnBpmLSvgCbZpKpU7eZRdRBGzXQEBIu1FXquW8gLOAYckrs5ao6sahtqEK9KSaizdOxIqnr3PrnYdW/QO3J161nx94Aw==\",\"w\":80,\"h\":30,\"aspect\":\"fixed\",\"title\":\"开始/结束\"&#125;,&#123;\"xml\":\"jVBLDsIgED3N7CnEC9hqVx6CyKSQgDQw2vb2TgvauGjigmTmfch7A6oNc5/0aG/RoAd1AdWmGKlMYW7Re5DCGVAdSCn4gbwesM3GilEnfNA/BlkML+2fWJACZFp8BSwFjtU1oM5GZ4urU/AyWUeYR31fdRM3YKz+holwPky0QTVOjzEgpYUlkzNkq0KU1MKiG2y1nSqmc9mHr3Xvx0Ot+Fn3U27cz6Xf\",\"w\":100,\"h\":50,\"aspect\":\"fixed\",\"title\":\"处理\"&#125;,&#123;\"xml\":\"jZA9DsIwDIVP4z1NFmZa6MTECQK16kgJqVL37/aEJlB1qMQQ6fl7dvRsUKWb66A7uvkGLagLqDJ4z0m5uURrQQrTgKpAShEfyOuBW6yu6HTAF/8zINPAqO2AiSTQ82IzCOTdY+hBnScyjPdOPz/OFDNHRuxi6KqIMn+FgXE+jLOinKVG75DDElsm0zCljlNKLAhNS7xnuk91+5vcdosir/cttzOu3u7Kbw==\",\"w\":80,\"h\":80,\"aspect\":\"fixed\",\"title\":\"判定\"&#125;,&#123;\"xml\":\"jZJRbsMgDIZPwzsF7QKla/cy7aEnoIkXUJ0QgdMkO/0coKs2qdIiRbK/3z8xdoQ2/XKKdnTvoQUU+lVoE0OgEvWLAUShpG+FPgilJL9CHZ+ou6zK0UYY6D8GVQw3ixMUUkCiFStIzo5bOMbQQEpC72fnCc6jbTY8c+/MHPXc/GHH4cU21y6GaWg/JkI/QOWtTQ6270pOPsNAR9t7XBm8Ad6AfGOrcPZf2aRqbgKGmLvRMj/MLfpuYNbwTYHFfaIYrvCn1JjtCI9450PY+tnXa0MkWJ6OLqM6txOEHiiuXDL7llytkGW80oHvXLW9VGZTybsf62MRHNRd3NPHzrP265f4Bg==\",\"w\":100,\"h\":50,\"aspect\":\"fixed\",\"title\":\"子流程\"&#125;,&#123;\"xml\":\"jVJRasMwDD2N/12bHWDLln4NdgUzi9jMiYqstsntp0VJm7AVZjBIT3pPkmXjm348Ujild4xQjH8zviFEVqsfGyjFOJuj8a/GOSvXuPZB9DBH7SkQDPwfglPCJZQzKKJA5aksAAzxmQiv4g04CPgSQ03wI3AQJ3FfFrMy4Rc0WJBmqm9bK0ciKgqxg4d9ztDS5BGwB6ZJUq45ctKMJx3FJshd4j1GUALny14+VHW7m9ytwgdmKezstJdZGRXP9AlL0vb5Vt74N48DdcC/eGJshrpD8zJW9750Td/+iW8=\",\"w\":50,\"h\":50,\"aspect\":\"fixed\",\"title\":\"虚线\"&#125;]&lt;/mxlibrary&gt; 流程图导出本图通过桌面版客户端 Draw.io 绘制，绘制通过导出功能导出，导出方法：先 ctrl + a 选中所有组件，选择文件-导出为文件-png，设置边框宽度，勾选 仅所选内容，即可导出PNG图片。 参考文章 画了多年的流程图，你真的画规范了吗？ 深入浅出为你解读四类「流程图」，附摩拜/ofo案例分析","tags":[{"name":"流程图绘制","slug":"流程图绘制","permalink":"http://nullpointer.pw/tags/流程图绘制/"}]},{"title":"Java8 时间 API 全集合","date":"2020-01-21T22:56:38.000Z","path":"Java8时间API全集合.html","text":"前言Java 8 已经面世很长时间，目前连 Java 13、Java 14 都已经出来了，但自己对于 Java8 中的一些 API 用法还不是很熟悉，今年才算全面切换到 Java8。在日常开发过程中，不可避免地会使用到 Java 8 的新时间 API，比如 LocalDateTime、LocalDate 等，放弃使用难用的 Date；但是每次需要进行转换操作时，总是需要重复去 Google 搜索，索性一次性过一遍所有的时间 API，总结一些常用的转换方法，比如时间戳与 LocalDateTime 互转，Date 与 LocalDateTime 互转。 Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164public class Test1 &#123; public static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"); public static final DateTimeFormatter TIME_FORMATTER = DateTimeFormatter.ofPattern(\"HH:mm:ss\"); public static final DateTimeFormatter MONTH_DAY_FORMATTER = DateTimeFormatter.ofPattern(\"MM-dd\"); public static final DateTimeFormatter YEAR_FORMATTER = DateTimeFormatter.ofPattern(\"yyyy\"); public static final DateTimeFormatter YEAR_MONTH_FORMATTER = DateTimeFormatter.ofPattern(\"yyyy-MM\"); public static void main(String[] args) &#123; // 通用方法 // plus/plusXXX 当前日期或时间加上一段 (年/月/周/日/小时/分钟) 日期或时间 // minus/minusXXX 当前日期或时间减去一段 (年/月/周/日/小时/分钟) 日期或时间 // isBefore 要比较的日期或时间是否在当前对象之前 // isAfter 要比较的日期或时间是否在当前对象之后 // parse 通过字符串创建日期或时间 // format 将日期或时间格式化为字符串 System.out.println(\"--------LocalDate---------\"); // 日期 LocalDate today = LocalDate.now(); System.out.println(\"当前日期：\" + today); System.out.println(\"当前日期加上 3 天：\" + today.plusDays(3)); System.out.println(\"当前日期减去 3 天：\" + today.minusDays(3)); System.out.println(\"年：\" + today.getYear()); System.out.println(\"月 (EN)：\" + today.getMonth()); System.out.println(\"月 (1-12)：\" + today.getMonthValue()); System.out.println(\"当周第几天：\" + today.getDayOfWeek()); System.out.println(\"当月第几天：\" + today.getDayOfMonth()); System.out.println(\"当年第几天：\" + today.getDayOfYear()); System.out.println(\"是否闰年：\" + today.isLeapYear()); System.out.println(\"当月共多少天：\" + today.lengthOfMonth()); System.out.println(\"当年共多少天：\" + today.lengthOfYear()); LocalDate nextDay = today.plusDays(1); Period until = today.until(nextDay); System.out.println(\"today-nextDay 间隔天数: \" + until.getDays()); System.out.println(\"isBefore：\" + today.isBefore(nextDay)); System.out.println(\"isAfter： \" + today.isAfter(nextDay)); LocalDate parseDate = LocalDate.parse(\"2020-01-24\"); System.out.println(\"parseDate：\" + parseDate); System.out.println(\"--------LocalDateTime---------\"); // 日期时间 LocalDateTime nowLocalDateTime = LocalDateTime.now(); System.out.println(\"当前时间：\" + nowLocalDateTime.format(DATE_TIME_FORMATTER)); System.out.println(\"当前时间加上 3 小时：\" + nowLocalDateTime.plusHours(3)); System.out.println(\"当前时间减去 3 小时：\" + nowLocalDateTime.minusHours(3)); System.out.println(\"年：\" + nowLocalDateTime.getYear()); System.out.println(\"月 (EN)：\" + nowLocalDateTime.getMonth()); System.out.println(\"月 (1-12)：\" + nowLocalDateTime.getMonthValue()); System.out.println(\"月 (1-12)：\" + nowLocalDateTime.getMonthValue()); LocalDateTime nextDayNow = nowLocalDateTime.plusDays(1); System.out.println(\"isBefore：\" + nowLocalDateTime.isBefore(nextDayNow)); System.out.println(\"isAfter：\" + nowLocalDateTime.isAfter(nextDayNow)); LocalDateTime parseDateTime = LocalDateTime.parse(\"2020-01-21 18:05:35\", DATE_TIME_FORMATTER); System.out.println(\"parseDateTime：\" + parseDateTime); System.out.println(\"--------LocalTime---------\"); // 时间 LocalTime localTime = LocalTime.now(Clock.systemDefaultZone()); System.out.println(\"当前时间：\" + localTime.format(TIME_FORMATTER)); System.out.println(\"--------MonthDay---------\"); // 月天 MonthDay monthDay = MonthDay.now(); System.out.println(\"当前月天：\" + monthDay.format(MONTH_DAY_FORMATTER)); System.out.println(\"--------Year---------\"); // 年 Year year = Year.now(); System.out.println(\"当前年：\" + year.format(YEAR_FORMATTER)); System.out.println(\"--------YearMonth---------\"); // 年月 YearMonth yearMonth = YearMonth.now(); System.out.println(\"当前年月：\" + yearMonth.format(YEAR_MONTH_FORMATTER)); System.out.println(\"--------DayOfWeek---------\"); // 星期 DayOfWeek dayOfWeek = DayOfWeek.from(LocalDateTime.now()); System.out.println(\"当前星期第几天：\" + dayOfWeek.getValue()); System.out.println(\"--------Month---------\"); // 月份 Month month = Month.from(LocalDate.now()); System.out.println(\"当年第几月\" + month.getValue()); System.out.println(\"当月天数：\" + month.length(Year.now().isLeap())); System.out.println(\"--------clock---------\"); // 时钟对象 Clock clock = Clock.systemDefaultZone(); System.out.println(\"当前时间时间戳：\" + clock.millis()); System.out.println(\"--------duration---------\"); // 基于时间的时间区间（单位：小时、分钟、毫秒、秒、纳秒等） Duration duration = Duration.between(nowLocalDateTime, nowLocalDateTime.plusHours(2)); System.out.println(\"秒：\" + duration.getSeconds()); System.out.println(\"毫秒：\" + duration.toMillis()); System.out.println(\"分钟：\" + duration.toMinutes()); System.out.println(\"小时：\" + duration.toHours()); System.out.println(\"天：\" + duration.toDays()); System.out.println(\"--------period---------\"); // 基于日期的时间区间（单位：年、月、日） Period period = Period.between(LocalDate.now(), LocalDate.now() .plusYears(1) .plusMonths(2) .plusDays(3)); System.out.println(\"天：\" + period.getDays()); System.out.println(\"月：\" + period.getMonths()); System.out.println(\"年：\" + period.getYears()); System.out.println(\"总月数：\" + period.toTotalMonths()); System.out.println(\"--------Instant---------\"); // 在时间线上的瞬间点 Instant instant = Instant.now(); System.out.println(\"毫秒：\" + instant.toEpochMilli()); System.out.println(\"--------实用方法---------\"); // 指定日期的凌晨 00:00:00 LocalDateTime todayStartTime = today.atStartOfDay(); System.out.println(\"指定日期的开始时间：\" + todayStartTime.format(DATE_TIME_FORMATTER)); // 或者 LocalDateTime todayStart = LocalDateTime.of(today, LocalTime.MIN); System.out.println(\"指定日期的开始时间: \" + todayStart.format(DATE_TIME_FORMATTER)); // 指定日期的凌晨 23:59:59 LocalDateTime todayEnd = LocalDateTime.of(today, LocalTime.MAX); System.out.println(\"指定日期的结束时间：\" + todayEnd.format(DATE_TIME_FORMATTER)); // 毫秒时间戳转 LocalDateTime System.out.println(\"毫秒时间戳转 LocalDateTime：\" + LocalDateTime.ofInstant(Instant.ofEpochMilli(1579646281991L), ZoneOffset.ofHours(8)).toString()); // LocalDateTime 转时间戳 System.out.println(\"LocalDateTime 转时间戳：\" + nowLocalDateTime.toInstant(ZoneOffset.ofHours(8)).toEpochMilli()); // LocalDateTime 转 yyyy-MM-dd HH:mm:ss 格式 String System.out.println(\"LocalDateTime 转 yyyy-MM-dd HH:mm:ss 格式 String：\" + LocalDateTime.now().format(DATE_TIME_FORMATTER)); // yyyy-MM-dd HH:mm:ss 格式 String 转 LocalDateTime LocalDateTime parseLocalDateTime = LocalDateTime.parse(\"2020-01-22 23:59:59\", DATE_TIME_FORMATTER); System.out.println(\"yyyy-MM-dd HH:mm:ss 格式 String 转 LocalDateTime：\" + parseLocalDateTime); // Date 转 LocalDateTime Date nowDate = new Date(); LocalDateTime date2LocalTime = LocalDateTime.ofInstant(nowDate.toInstant(), ZoneOffset.ofHours(8)); System.out.println(\"Date 转 LocalDateTime：\" + date2LocalTime); // LocalDateTime 转 Date Date localDateTime2Date = Date.from(nowLocalDateTime.toInstant(ZoneOffset.ofHours(8))); System.out.println(\"LocalDateTime 转 Date：\" + localDateTime2Date); // LocalDate转LocalDateTime LocalDateTime localDate2LocalDateTime = LocalDateTime.of(today, LocalTime.parse(\"00:00:00\")); // LocalDateTime转LocalDate LocalDate localDateTime2LocalDate = nowLocalDateTime.toLocalDate(); &#125;&#125; 输出结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566--------LocalDate---------当前日期：2020-01-22当前日期加上 3 天：2020-01-25当前日期减去 3 天：2020-01-19年：2020月 (EN)：JANUARY月 (1-12)：1当周第几天：WEDNESDAY当月第几天：22当年第几天：22是否闰年：true当月共多少天：31当年共多少天：366today-nextDay 间隔天数: 1isBefore：trueisAfter： falseparseDate：2020-01-24--------LocalDateTime---------当前时间：2020-01-22 07:06:13当前时间加上 3 小时：2020-01-22T10:06:13.979当前时间减去 3 小时：2020-01-22T04:06:13.979年：2020月 (EN)：JANUARY月 (1-12)：1月 (1-12)：1isBefore：trueisAfter：falseparseDateTime：2020-01-21T18:05:35--------LocalTime---------当前时间：07:06:13--------MonthDay---------当前月天：01-22--------Year---------当前年：2020--------YearMonth---------当前年月：2020-01--------DayOfWeek---------当前星期第几天：3--------Month---------当年第几月 1当月天数：31--------clock---------当前时间时间戳：1579647973982--------duration---------秒：7200毫秒：7200000分钟：120小时：2天：0--------period---------天：3月：2年：1总月数：14--------Instant---------毫秒：1579647973983--------实用方法---------指定日期的开始时间：2020-01-22 00:00:00指定日期的开始时间: 2020-01-22 00:00:00指定日期的结束时间：2020-01-22 23:59:59毫秒时间戳转 LocalDateTime：2020-01-22T06:38:01.991LocalDateTime 转时间戳：1579647973979LocalDateTime 转 yyyy-MM-dd HH:mm:ss 格式 String：2020-01-22 07:06:13yyyy-MM-dd HH:mm:ss 格式 String 转 LocalDateTime：2020-01-22T23:59:59Date 转 LocalDateTime：2020-01-22T07:06:13.984LocalDateTime 转 Date：Wed Jan 22 07:06:13 CST 2020 参考 在线 JDK8 中文文档","tags":[{"name":"LocalDateTime转时间戳","slug":"LocalDateTime转时间戳","permalink":"http://nullpointer.pw/tags/LocalDateTime转时间戳/"},{"name":"时间戳转LocalDateTime","slug":"时间戳转LocalDateTime","permalink":"http://nullpointer.pw/tags/时间戳转LocalDateTime/"},{"name":"Date转LocalDateTime","slug":"Date转LocalDateTime","permalink":"http://nullpointer.pw/tags/Date转LocalDateTime/"},{"name":"LocalDateTime转Date","slug":"LocalDateTime转Date","permalink":"http://nullpointer.pw/tags/LocalDateTime转Date/"},{"name":"LocalDateTime转LocalDate","slug":"LocalDateTime转LocalDate","permalink":"http://nullpointer.pw/tags/LocalDateTime转LocalDate/"},{"name":"LocalDate转LocalDateTime","slug":"LocalDate转LocalDateTime","permalink":"http://nullpointer.pw/tags/LocalDate转LocalDateTime/"},{"name":"LocalDateTime转String","slug":"LocalDateTime转String","permalink":"http://nullpointer.pw/tags/LocalDateTime转String/"},{"name":"String转LocalDateTime","slug":"String转LocalDateTime","permalink":"http://nullpointer.pw/tags/String转LocalDateTime/"},{"name":"Java8获取凌晨时间","slug":"Java8获取凌晨时间","permalink":"http://nullpointer.pw/tags/Java8获取凌晨时间/"}]},{"title":"mapstruct最佳实践","date":"2020-01-17T14:21:58.000Z","path":"mapstruct最佳实践.html","text":"前言按照日常开发习惯，对于不同领域层使用不同JavaBean对象传输数据，避免相互影响，因此基于数据库实体对象User衍生出比如UserDto、UserVo等对象，于是在不同层之间进行数据传输时，不可避免地需要将这些对象进行互相转换操作。 常见的转换方式有： 调用getter/setter方法进行属性赋值 调用BeanUtil.copyPropertie进行反射属性赋值 第一种方式不必说，属性多了就需要写一大坨getter/setter代码。第二种方式比第一种方式要简便很多，但是坑巨多，比如sources与target写反，难以定位某个字段在哪里进行的赋值，同时因为用到反射，导致性能也不佳。 鉴于此，今天写一写第三种对象转换方式，本文使用的是 MapStruct 工具进行转换，MapStruct 原理也很简单，就是在代码编译阶段生成对应的赋值代码，底层原理还是调用getter/setter方法，但是这是由工具替我们完成，MapStruct在不影响性能的情况下，解决了前面两种方式弊端，很赞~ 准备工作为了讲解 MapStruct 工具的使用，本文使用常见的 User 类以及对应 UserDto 对象来演示。 123456789101112131415161718192021222324252627282930@Data@Accessors(chain = true)public class User &#123; private Long id; private String username; private String password; // 密码 private Integer sex; // 性别 private LocalDate birthday; // 生日 private LocalDateTime createTime; // 创建时间 private String config; // 其他扩展信息，以JSON格式存储 private String test; // 测试字段&#125;@Data@Accessors(chain = true)public class UserVo &#123; private Long id; private String username; private String password; private Integer gender; private LocalDate birthday; private String createTime; private List&lt;UserConfig&gt; config; private String test; // 测试字段 @Data public static class UserConfig &#123; private String field1; private Integer field2; &#125;&#125; 注意观察这两个类的区别。 一、MapStruct 配置以及基础使用项目中引入 MapStruct 的依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;version&gt;1.3.1.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;1.3.1.Final&lt;/version&gt;&lt;/dependency&gt; 因为项目中的对象转换操作基本都一样，因此抽取除了一个转换基类，不同对象如果只是简单转换可以直接继承该基类，而无需覆写基类任何方法，即只需要一个空类即可。如果子类覆写了基类的方法，则基类上的 @Mapping 会失效。 12345678910111213141516171819202122232425262728293031323334353637@MapperConfigpublic interface BaseMapping&lt;SOURCE, TARGET&gt; &#123; /** * 映射同名属性 */ @Mapping(target = \"createTime\", dateFormat = \"yyyy-MM-dd HH:mm:ss\") TARGET sourceToTarget(SOURCE var1); /** * 反向，映射同名属性 */ @InheritInverseConfiguration(name = \"sourceToTarget\") SOURCE targetToSource(TARGET var1); /** * 映射同名属性，集合形式 */ @InheritConfiguration(name = \"sourceToTarget\") List&lt;TARGET&gt; sourceToTarget(List&lt;SOURCE&gt; var1); /** * 反向，映射同名属性，集合形式 */ @InheritConfiguration(name = \"targetToSource\") List&lt;SOURCE&gt; targetToSource(List&lt;TARGET&gt; var1); /** * 映射同名属性，集合流形式 */ List&lt;TARGET&gt; sourceToTarget(Stream&lt;SOURCE&gt; stream); /** * 反向，映射同名属性，集合流形式 */ List&lt;SOURCE&gt; targetToSource(Stream&lt;TARGET&gt; stream);&#125; 实现 User 与 UserVo 对象的转换器 12345678910111213141516171819202122232425import org.mapstruct.Mapper;import org.mapstruct.Mapping;@Mapper(componentModel = \"spring\")public interface UserMapping extends BaseMapping&lt;User, UserVo&gt; &#123; @Mapping(target = \"gender\", source = \"sex\") @Mapping(target = \"createTime\", dateFormat = \"yyyy-MM-dd HH:mm:ss\") @Override UserVo sourceToTarget(User var1); @Mapping(target = \"sex\", source = \"gender\") @Mapping(target = \"password\", ignore = true) @Mapping(target = \"createTime\", dateFormat = \"yyyy-MM-dd HH:mm:ss\") @Override User targetToSource(UserVo var1); default List&lt;UserConfig&gt; strConfigToListUserConfig(String config) &#123; return JSON.parseArray(config, UserConfig.class); &#125; default String listUserConfigToStrConfig(List&lt;UserConfig&gt; list) &#123; return JSON.toJSONString(list); &#125;&#125; 本文示例使用的是 Spring 的方式，@Mapper 注解的 componentModel 属性值为 spring，不过应该大多数都用的此模式进行开发。 @Mapping用于配置对象的映射关系，示例中 User 对象性别属性名为 sex，而UserVo对象性别属性名为gender，因此需要配置 target 与 source 属性。 password 字段不应该返回到前台，可以采取两种方式不进行转换，第一种就是在vo对象中不出现password字段，第二种就是在@Mapping中设置该字段 ignore = true。 MapStruct 提供了时间格式化的属性 dataFormat，支持Date、LocalDate、LocalDateTime等时间类型与String的转换。示例中birthday 属性为 LocalDate 类型，可以无需指定dataFormat自动完成转换，而LocalDateTime类型默认使用的是ISO格式时间，在国内往往不符合需求，因此需要手动指定一下 dataFormat。 二、自定义属性类型转换方法一般常用的类型字段转换 MapStruct都能替我们完成，但是有一些是我们自定义的对象类型，MapStruct就不能进行字段转换，这就需要我们编写对应的类型转换方法，笔者使用的是JDK8，支持接口中的默认方法，可以直接在转换器中添加自定义类型转换方法。 示例中User对象的config属性是一个JSON字符串，UserVo对象中是List类型的，这需要实现JSON字符串与对象的互转。 1234567default List&lt;UserConfig&gt; strConfigToListUserConfig(String config) &#123; return JSON.parseArray(config, UserConfig.class);&#125;default String listUserConfigToStrConfig(List&lt;UserConfig&gt; list) &#123; return JSON.toJSONString(list);&#125; 如果是 JDK8以下的，不支持默认方法，可以另外定义一个 转换器，然后再当前转换器的 @Mapper 中通过 uses = XXX.class 进行引用。 定义好方法之后，MapStruct当匹配到合适类型的字段时，会调用我们自定义的转换方法进行转换。 三、单元测试1234567891011121314151617181920212223242526272829303132333435363738394041424344@Slf4j@RunWith(SpringRunner.class)@SpringBootTestpublic class MapStructTest &#123; @Resource private UserMapping userMapping; @Test public void tetDomain2DTO() &#123; User user = new User() .setId(1L) .setUsername(\"zhangsan\") .setSex(1) .setPassword(\"abc123\") .setCreateTime(LocalDateTime.now()) .setBirthday(LocalDate.of(1999, 9, 27)) .setConfig(\"[&#123;\\\"field1\\\":\\\"Test Field1\\\",\\\"field2\\\":500&#125;]\"); UserVo userVo = userMapping.sourceToTarget(user); log.info(\"User: &#123;&#125;\", user); // User: User(id=1, username=zhangsan, password=abc123, sex=1, birthday=1999-09-27, createTime=2020-01-17T17:46:20.316, config=[&#123;\"field1\":\"Test Field1\",\"field2\":500&#125;]) log.info(\"UserVo: &#123;&#125;\", userVo); // UserVo: UserVo(id=1, username=zhangsan, gender=1, birthday=1999-09-27, createTime=2020-01-17 17:46:20, config=[UserVo.UserConfig(field1=Test Field1, field2=500)]) &#125; @Test public void testDTO2Domain() &#123; UserConfig userConfig = new UserConfig(); userConfig.setField1(\"Test Field1\"); userConfig.setField2(500); UserVo userVo = new UserVo() .setId(1L) .setUsername(\"zhangsan\") .setGender(2) .setCreateTime(\"2020-01-18 15:32:54\") .setBirthday(LocalDate.of(1999, 9, 27)) .setConfig(Collections.singletonList(userConfig)); User user = userMapping.targetToSource(userVo); log.info(\"UserVo: &#123;&#125;\", userVo); // UserVo: UserVo(id=1, username=zhangsan, gender=2, birthday=1999-09-27, createTime=2020-01-18 15:32:54, config=[UserVo.UserConfig(field1=Test Field1, field2=500)]) log.info(\"User: &#123;&#125;\", user); // User: User(id=1, username=zhangsan, password=null, sex=2, birthday=1999-09-27, createTime=2020-01-18T15:32:54, config=[&#123;\"field1\":\"Test Field1\",\"field2\":500&#125;]) &#125; 四、常见问题 当两个对象属性不一致时，比如User对象中某个字段不存在与UserVo当中时，在编译时会有警告提示，可以在@Mapping中配置 ignore = true，当字段较多时，可以直接在@Mapper中设置unmappedTargetPolicy属性或者unmappedSourcePolicy属性为 ReportingPolicy.IGNORE即可。 如果项目中也同时使用到了 Lombok，一定要注意 Lombok的版本要等于或者高于1.18.10，否则会有编译不通过的情况发生，笔者掉进这个坑很久才爬了出来，希望各位不要重复踩坑。 代码下载本文涉及代码已上传到 Github，以供参考。 mapstruct最佳实践示例代码 参考 官方文档：https://mapstruct.org/documentation/stable/reference/html/ 官方FAQ：https://mapstruct.org/faq/ 官方Example：https://github.com/mapstruct/mapstruct-examples 机翻中文版文档：http://www.kailing.pub/MapStruct1.3/index.html 5种常见Bean映射工具的性能比对：https://www.cnblogs.com/javaguide/p/11861749.html","tags":[{"name":"mapstruct自定义类型转换","slug":"mapstruct自定义类型转换","permalink":"http://nullpointer.pw/tags/mapstruct自定义类型转换/"}]},{"title":"数据库与应用服务器时间不同步踩坑记","date":"2020-01-12T06:53:47.000Z","path":"数据库与应用服务器时间不同步踩坑记.html","text":"业务说明这样一个业务场景，有一个抽奖活动，抽奖活动中如果完成一次分享的任务，就会增加一次抽奖机会。前端判断用户分享成功后，会调用一次保存分享记录的接口，将分享记录入库。完成后，前端会调用获取调用查询【活动任务可完成次数】接口，如果任务可完成次数为0，则置灰分享按钮。 本文原文链接地址: http://nullpointer.pw/数据库与应用服务器时间不同步踩坑记.html 如图所示： 定位问题测试给我提了一个 BUG，在测试环境，完成分享任务后，重新调用【活动任务可完成次数】后，发现接口返回可完成次数仍然是 1，但是过了好几分钟后，重新进入活动页，接口返回了可完成次数为 0，分享按钮也已经置灰不可点击。 这就很奇怪了，一开始怀疑是缓存的问题，结果并不是，在测试环境远程 Debug 后发现，每次分享成功后，查询数据库分享任务的完成次数竟然没有查询到数据，而数据中确实存在了分享的记录，将查询 SQL 和查询参数取出来放到 Navicat 中一查，结果是因为时间区间筛选掉了分享的那条记录。SQL 如下： 12SELECT * from t_share_record where uid = \"1234567\" and activity_id = 38867247 and create_time between '2020-01-08 00:00:00' and '2020-01-08 19:18:29' 而数据库中的那条分享记录的create_time 是 2020-01-08 19:19:21， 这条记录不在筛选的时间区间内，因而被过滤掉了。 问题原因时序图如下： 可以看到，分享保存记录为第 8 步，但是重新查询任务可完成接口是第 10 步，但目前的情况则是，第10步中查询时的时间竟然要早于第 8 步数据入库的时间。难道插入数据竟然要这么久？也不至于耗费近一分钟吧！ 于是怀疑是数据库时间问题，在MySQL执行查询命令 1select now(); 然后，在部署应用的服务器上执行命令 1date 查看结果，果然二者时间相差了近一分钟。最终的原因就是：因为数据库和应用服务器不在同一台服务器上，而其他测试人员修改了应用服务器的时间，导致两个服务器时间不一致从而出现这个问题。想到这里，马上向运维确认线上服务器时间与数据库时间是否同步，还好，回答我的是同步的，心里的石头算放下了。 解决问题查看插入分享记录的 Mybatis 中XML如下 1insert into t_share_record(uid, activity_id, create_time) values(#&#123;uid&#125;, #&#123;activityId&#125;, now()) 这样写是没问题的，但是根据时间段筛选数据时，时间区间却又是从应用服务器生成传递参数传入SQL的，Mybatis 中XML如下： 12SELECT * from t_share_record where uid = #&#123;uid&#125; and activity_id = #&#123;activityId&#125; and create_time between #&#123;beginTime&#125; and #&#123;endTime&#125; 为了避免以上这种因为应用服务器与数据库时间不一致导致的问题，这里的SQL筛选时间的结束时间也通过数据库服务器获取。 12SELECT * from t_share_record where uid = #&#123;uid&#125; and activity_id = #&#123;activityId&#125; and create_time between #&#123;beginTime&#125; and now() 当然，为了防止出现更多类似的问题，应用服务器与数据库服务器的时间一定要通过时间同步设置保持完全一致才是最合理的。 希望对你有所帮助！","tags":[{"name":"解决数据库与应用服务器时间不同步导致的问题","slug":"解决数据库与应用服务器时间不同步导致的问题","permalink":"http://nullpointer.pw/tags/解决数据库与应用服务器时间不同步导致的问题/"}]},{"title":"2019年终总结","date":"2020-01-04T01:39:31.000Z","path":"2019年终总结.html","text":"来杭州不足一年，我就拥有了 M 型发型，不知道该喜还是忧，今天为这苦逼的一年画上句号，希望 2020 年能不如此苦逼吧！本来计划着 2019 年 12 月 29 日写完的年终总结，自己拖拖拉拉搞到今天才算写完。 三月份从北京离开，本意是跳出自己的舒适区，杭州今年却还没有下过雪，倒是最近总是淅淅沥沥地下小雨，给人湿冷湿冷的感觉，来到杭州发现这边的环境果然让我很不舒适。 不仅仅是气候环境。。。 不过，来了杭州确实让自己身上的缺点暴露出来，在新的公司工作时，也发现了自己某些能力的欠缺，果然最让自己害怕的就是自己最薄弱的地方，找到自己的不足并去努力提升改进，或许是我来做出来杭州这个选择，唯一值得欢喜之处吧！ 技术就我个人本身而言，学习新的东西能学下去的基本上是我不得不学的，还有就是我真的特别感兴趣的。这看着好像问题不大？不得不学的大部分都是工作中需要用到的，还有需要教导别人的，学了就能派上用场，效果最好。但是事实是，能马上学会并且能应用的技术，太过于浮于表面，而这些技术更深层面的东西才是精髓所在，而自己却往往总是忽略了这一点， 在某些时刻，虚弱的内功让我不堪一击。 来年需沉下心来，学习更深层次，相信刻意练习，没有什么是学不会的。 时间管理从今年 9 月份开始，形成了每日总结的习惯，从一开始的流水账到逐渐添加自己的思考，目前也连续了一百多天。在做总结的过程中发现自己在很多时候，无法想起今天究竟做了哪些工作、今天发生了哪些有趣的事情，今天学到了哪些知识，只能为自己又一天的碌碌无为地荒废而自责 无时无刻，丰富的互联网总是在消费着注意力，一不小心半个小时，一个小时就过去了，而手头很急的事情却丝毫没有进展，这也是造成自己每日荒废的罪魁祸首。 于是，自己便开始尝试上班或者学习开启番茄闹钟，被动打断总是不可避免，但确实有效地控制了自己的注意力，能让自己专注做事； 以前上班自己总是戴着耳机听随机算法推荐的歌曲，尝试番茄闹钟之后，发现歌曲也会让自己的注意力漂移，于是便不再听歌曲，而是改听大自然白噪声，加上女朋友送的 SONY 降噪耳机主动降噪效果，对外界抗干扰能力也变强很多。 通过番茄闹钟，找到了自己最高效的时间段，也是意外的收获。 注意力得到了一定的控制，但是还是不够，因为还是会有大把的空闲时间被白白浪费掉。没有产出，是一件让我感觉很痛苦的事情，特别是每周写周报的时候，痛感尤甚。 于是，我踏上了学习 GTD 的道路，安排好下一日的任务，尽量利用好每一块时间，不过还没实践太久，现在便不多言。 习惯坚持两年多写博客的习惯，可能因为从北京离开，习惯也被遗落在了北京吧！今年只更新了 6 篇博客，还水了几篇非技术类文章，和去年 20 几篇真的差远了。其实自己博客选题的 list 已经被拉得很长了，自己却总是找接口没时间懒癌发作拖延症发作不去写，来年 GTD 要实践起来，好习惯应该一直保持下去。 收获来杭州不满一年，发际线后移了一公分，真的好悲伤，甚至有点不太敢照镜子！求推荐护发养发洗发露~ 其他抢票真的挺难的，我现在才感觉到。离家太远机票又贵，害怕最后一天不能赶回来上班，提前抢票也是秒没，这是在在北京从未有过的烦恼。 立 FlagFlag 还是一定要立的，万一都实现了呢！😎ི 恢复每月写 2 篇博客的习惯 早睡早起 240 天，番茄数量 25 个/周 读完《重构》《Java8 实战》《Spring 实战》《Spring 源码深度解析》《领域驱动设计》 读完《搞定》《思考，快与慢》《稀缺》《万历十五年》《原则》 每月在博客单独页面同步 flag 进度","tags":[{"name":"年终总结","slug":"年终总结","permalink":"http://nullpointer.pw/tags/年终总结/"}]},{"title":"easypoi导入Excel最佳实践","date":"2019-12-28T07:50:55.000Z","path":"easypoi导入Excel最佳实践.html","text":"前言一直以来，使用EasyPOI做了不少导入导出的需求，但是每次做完都是临时去看官方文档现学现用，正巧最近朋友遇到这么个需求，用到了EasyPOI来完成导入，我也正好整理整理EasyPOI的导入用法。 需求是这样的：现在要在后台导入用户的简历，简历的格式是这样子的： 一个人有多个属性，某些属性如申请职位、薪资是单一属性，即只会有一个值；某些属性如工作经历、教育经历、获奖情况是一组属性，可能会有多组值。现在要将这批简历数据导入到库中。 零、文件准备： 示例Excel以及示例Excel2 加入 EasyPOI 的依赖 12345&lt;dependency&gt; &lt;groupId&gt;cn.afterturn&lt;/groupId&gt; &lt;artifactId&gt;easypoi-web&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; 一、定义EasyPOI导入实体类首先定义一个 EasyPOI 实体类，单个属性使用 @Excel注解声明，name 属性需要与Excel中的表头保持一致，比如 姓名*中的 * 号就不能省略掉。一对多关系使用 @Collection 注解声明，name 是最上方的表头，对应的集合元素类型需要另外定义一个对象，这里因为篇幅问题，只展示一个工作经历以及教育经历对应的集合对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495@Datapublic class TalentUserInputEntity&#123; @Excel(name = \"姓名*\") private String name; @Excel(name = \"性别*\") private Integer gender; @Excel(name = \"手机号*\") private String phone; @Excel(name = \"开始工作时间*\") private Date workTime; @Excel(name = \"民族*\") private String national; @Excel(name = \"语言水平*\") private String languageProficiency; @Excel(name = \"出生日期*\") private Date birth; @Excel(name = \"职位*\") private String jobsName; @Excel(name = \"职位类型*\") private String categoryName; @Excel(name = \"薪资*\") private Integer salary; @Excel(name = \"工作地点*\") private String workArea; @ExcelCollection(name = \"工作经历*\") private List&lt;ExperienceInputEntity&gt; experienceList; @ExcelCollection(name = \"教育经历*\") private List&lt;EducationInputEntity&gt; educationList; @ExcelCollection(name = \"获奖情况\") private List&lt;AwardsInputEntity&gt; awardList; @ExcelCollection(name = \"技能证书\") private List&lt;PunishmentInputEntity&gt; punishmentList; @Excel(name = \"特长\") private String specialty;&#125;// 工作经历对象@Datapublic class ExperienceInputEntity &#123; @Excel(name = \"公司名称*\") private String companyName; @Excel(name = \"所在行业*\") private String industry; @Excel(name = \"开始时间*\") private Date beginTime; @Excel(name = \"结束时间*\") private Date finishTime; @Excel(name = \"职位名称*\") private String jobTitle; @Excel(name = \"所属部门*\") private String department; @Excel(name = \"工作内容*\") private String description;&#125;// 教育经历对象@Datapublic class EducationInputEntity &#123; @Excel(name = \"学校*\") private String schoolName; @Excel(name = \"学历*\") private Integer record; @Excel(name = \"开始年份*\") private Date beginTime; @Excel(name = \"毕业年份*\") private Date finishTime; @Excel(name = \"专业*\") private String profession;&#125;// 省略其他 二、EasyPOI基础导入这里为方便演示，直接将导入结果转成JSON打印输出。 12345678910@PostMapping(\"/upload\")public Boolean upload(@RequestParam(\"file\") MultipartFile multipartFile) throws Exception &#123; ImportParams params = new ImportParams(); params.setHeadRows(2); // params.setTitleRows(0); List&lt;TalentUserInputEntity&gt; result = ExcelImportUtil.importExcel(multipartFile.getInputStream(), TalentUserInputEntity.class, params); System.out.println(JSONUtil.toJsonStr(result)); return true;&#125; 这里需要注意表头的行数设置一定要正确！否则集合数据将无法读取，可以通过WPS或者office查看实际表头所占用的行数，一定要区分表头与标题的区别，表头是列名称，标题是表头上面的文字，本文示例文件中没有标题，所以setTitleRows为0 三、值替换使用 postman 或者 Talend API Tester 等工具进行上传 示例文件.xlsx，结果控制台输出了异常，异常如下： 123java.lang.NumberFormatException: For input string: &quot;男&quot;...cn.afterturn.easypoi.exception.excel.ExcelImportException: Excel 值获取失败 原因是因为数据库中性别字段类型为 Integer 类型的，所以导入对象也设置成了 Integer，而 Excel 中填写的是男/女汉字，自然就出错了，这里就需要使用 easypoi 的 replace 方式来替换最终值，修改 gender 字段上的注解为： 123// replace格式为 \"替换前的值_替换后的值\"@Excel(name = \"性别*\", replace = &#123;\"男_0\", \"女_1\"&#125;)private Integer gender; 同理，薪资类型和教育经历中的学历需要替换需要的值 12345@Excel(name = \"薪资*\", replace = &#123;\"3K以下_1\", \"3K-5K_2\", \"5K-10K_3\", \"10K-20K_4\", \"20K-50K_5\", \"50K以上_6\"&#125;)private Integer salary;@Excel(name = \"学历*\", replace =&#123;\"初中及以下_1\",\"中专_2\",\"高中_3\",\"大专_4\",\"本科_5\",\"硕士_6\",\"博士_7\"&#125;)private Integer record; 再重新尝试导入，控制台输出了导入的JSON内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111[ &#123; \"experienceList\": [ &#123; \"finishTime\": 1571673600000, \"companyName\": \"科技公司1\", \"jobTitle\": \"运营\", \"description\": \"运营方案处理\", \"industry\": \"互联网\", \"beginTime\": 1546358400000, \"department\": \"运营部\" &#125;, &#123; \"finishTime\": 1571673600000, \"companyName\": \"财务公司\", \"jobTitle\": \"会计\", \"description\": \"审计\", \"industry\": \"财务\", \"beginTime\": 1574179200000, \"department\": \"会计部\" &#125; ], \"gender\": 0, \"languageProficiency\": \"英语四级\", \"jobsName\": \"销售\", \"educationList\": [ &#123; \"profession\": \"计算机\", \"finishTime\": 1530374400000, \"record\": 5, \"beginTime\": 1409500800000, \"schoolName\": \"山东大学\" &#125;, &#123; \"profession\": \"计算机\", \"finishTime\": 1593532800000, \"record\": 6, \"beginTime\": 1409500800000, \"schoolName\": \"山东大学\" &#125; ], \"birth\": 851097600000, \"salary\": 4, \"workTime\": 1549900800000, \"categoryName\": \"销售\", \"phone\": \"13122223333\", \"workArea\": \"浙江省金华市义乌市\", \"name\": \"张无忌\", \"national\": \"汉族\", \"punishmentList\": [ &#123;&#125;, &#123;&#125; ], \"awardList\": [ &#123; \"date\": 1530374400000, \"name\": \"国家奖学金\", \"description\": \"国家一级奖学金\" &#125;, &#123;&#125; ] &#125;, &#123; \"specialty\": \"特长就是太多特长\", \"experienceList\": [ &#123; \"finishTime\": 1571673600000, \"companyName\": \"科技公司1\", \"jobTitle\": \"java开发\", \"description\": \"code\", \"industry\": \"互联网\", \"beginTime\": 1546358400000, \"department\": \"开发部门\" &#125; ], \"gender\": 0, \"languageProficiency\": \"英语八级\", \"jobsName\": \"java\", \"educationList\": [ &#123; \"profession\": \"计算机\", \"finishTime\": 1530374400000, \"record\": 5, \"beginTime\": 1409500800000, \"schoolName\": \"安徽大学\" &#125; ], \"birth\": 851097600000, \"salary\": 4, \"workTime\": 1549900800000, \"categoryName\": \"开发\", \"phone\": \"18311111111\", \"workArea\": \"浙江省金华市义乌市\", \"name\": \"张小凡\", \"national\": \"汉族\", \"punishmentList\": [ &#123; \"date\": 1530374400000, \"description\": \"技能没有\" &#125; ], \"awardList\": [ &#123; \"date\": 1530374400000, \"name\": \"国家奖学金\", \"description\": \"国家一级奖学金\" &#125; ] &#125;, // 省略其他...] 四、导入之基础校验现在产品需要对导入的Excel进行校验，不合法的Excel不允许入库，需要返回具体的错误信息给前端，提示给用户，错误信息中需要包含行号以及对应的错误。 因为 EasyPOI 支持 Hibernate Validator ，所以直接使用就可以了，因为要将错误信息以及错误行号返回，所以需要用到 EasyPOI 的高级用法，实现 IExcelDataModel与 IExcelModel接口，IExcelDataModel负责设置行号，IExcelModel 负责设置错误信息。 修改导入实体类，增加字段 rowNum 与 errorMsg以及增加校验注解。 如果使用到了 @Pattern 注解，则字段类型必须是 String 类型，否则会抛出异常，本文中的原 Integer 类型的 gender 修改成为 String 类型的 genderStr，record 字段也修改为了 String 类型的 recordStr等等。同理如果校验 Date 类型字段，先将类型改成String，正则表达式参考下文写法。 这里需要注意，如果@Excel注解中设置了 replace 属性，则Hibernate Validator 校验的是替换后的值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124@Datapublic class TalentUserInputEntity implements IExcelDataModel, IExcelModel &#123; // 时间格式校验正则 public static final String DATE_REGEXP = \"(Mon|Tue|Wed|Thu|Fri|Sat|Sun)( )(Dec|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov)( )\\\\d&#123;2&#125;( )(00:00:00)( )(CST)( )\\\\d&#123;4&#125;\"; /** * 行号 */ private int rowNum; /** * 错误消息 */ private String errorMsg; @Excel(name = \"姓名*\") @NotBlank(message = \"[姓名]不能为空\") private String name; @Excel(name = \"性别*\", replace = &#123;\"男_0\", \"女_1\"&#125;) @Pattern(regexp = \"[01]\", message = \"性别错误\") private String genderStr; @Excel(name = \"手机号*\") private String phone; @Excel(name = \"开始工作时间*\") @Pattern(regexp = DATE_REGEXP, message = \"[开始工作时间]时间格式错误\") private String workTimeStr; @Excel(name = \"民族*\") @NotBlank(message = \"[民族]不能为空\") private String national; @Excel(name = \"语言水平*\") @NotBlank(message = \"[语言水平]不能为空\") private String languageProficiency; @Excel(name = \"出生日期*\") @Pattern(regexp = DATE_REGEXP, message = \"[出生日期]时间格式错误\") private String birthStr; @Excel(name = \"职位*\") @NotBlank(message = \"[职位]不能为空\") private String jobsName; @Excel(name = \"职位类型*\") @NotBlank(message = \"[职位类型]不能为空\") private String categoryName; @Excel(name = \"薪资*\", replace = &#123;\"3K以下_1\", \"3K-5K_2\", \"5K-10K_3\", \"10K-20K_4\", \"20K-50K_5\", \"50K以上_6\"&#125;) @Pattern(regexp = \"[123456]\", message = \"薪资信息错误\") private String salaryStr; @Excel(name = \"工作地点*\") @NotBlank(message = \"[工作地点]不能为空\") private String workArea; @ExcelCollection(name = \"工作经历*\") private List&lt;ExperienceInputEntity&gt; experienceList; @ExcelCollection(name = \"教育经历*\") private List&lt;EducationInputEntity&gt; educationList; @ExcelCollection(name = \"获奖情况\") private List&lt;AwardsInputEntity&gt; awardList; @ExcelCollection(name = \"技能证书\") private List&lt;PunishmentInputEntity&gt; punishmentList; @Excel(name = \"特长\") private String specialty;&#125;// 工作经历@Datapublic class ExperienceInputEntity &#123; @Excel(name = \"公司名称*\") private String companyName; @Excel(name = \"所在行业*\") private String industry; @Excel(name = \"开始时间*\") @Pattern(regexp = DATE_REGEXP, message = \"[工作经历][开始时间]时间格式错误\") private String beginTimeStr; @Excel(name = \"结束时间*\") @Pattern(regexp = DATE_REGEXP, message = \"[工作经历][结束时间]时间格式错误\") private String finishTimeStr; @Excel(name = \"职位名称*\") private String jobTitle; @Excel(name = \"所属部门*\") private String department; @Excel(name = \"工作内容*\") private String description;&#125;// 教育经历@Datapublic class EducationInputEntity &#123; @Excel(name = \"学校*\") private String schoolName; @Excel(name = \"学历*\", replace = &#123;\"初中及以下_1\", \"中专_2\", \"高中_3\", \"大专_4\", \"本科_5\", \"硕士_6\", \"博士_7\"&#125;) @Pattern(regexp = \"[1234567]\", message = \"学历信息错误\") private String recordStr; @Excel(name = \"开始年份*\") @Pattern(regexp = DATE_REGEXP, message = \"[教育经历][开始年份]时间格式错误\") private String beginTimeStr; @Excel(name = \"毕业年份*\") @Pattern(regexp = DATE_REGEXP, message = \"[教育经历][毕业年份]时间格式错误\") private String finishTimeStr; @Excel(name = \"专业*\") private String profession;&#125;// 其他省略... 修改完实体类后，修改导入处的代码 1234567891011121314151617181920@PostMapping(\"/upload\")public Boolean upload(@RequestParam(\"file\") MultipartFile multipartFile) throws Exception &#123; ImportParams params = new ImportParams(); // 表头设置为2行 params.setHeadRows(2); // 标题行设置为0行，默认是0，可以不设置 params.setTitleRows(0); // 开启Excel校验 params.setNeedVerfiy(true); ExcelImportResult&lt;TalentUserInputEntity&gt; result = ExcelImportUtil.importExcelMore(multipartFile.getInputStream(), TalentUserInputEntity.class, params); System.out.println(\"是否校验失败: \" + result.isVerfiyFail()); System.out.println(\"校验失败的集合:\" + JSONObject.toJSONString(result.getFailList())); System.out.println(\"校验通过的集合:\" + JSONObject.toJSONString(result.getList())); for (TalentUserInputEntity entity : result.getFailList()) &#123; String msg = \"第\" + entity.getRowNum() + \"行的错误是：\" + entity.getErrorMsg(); System.out.println(msg); &#125; return true;&#125; 为了方便测试，我基于正确的Excel另存一份个别字段有误的 示例Excel2 并上传，解析结果为： 这里贴出部分校验失败集合数据，可以看到第50行和61行是原Excel中错误的数据，也已经打印了出来，但是教育经历中的教育水平也是错误的，却没被打印出来，查看源码发现，EasyPOI 对 Collection 中的对象并没有进行校验，我们在下文中解决。 五、导入值自定义校验之重复值校验上文所作的校验只是一些基本的校验，可能会有诸如Excel中重复行校验，Excel中数据与数据库重复校验等等。这种校验就无法通过 Hibernate Validator 来完成，只能写代码来实现校验逻辑了。 首先从简单的Excel数据与数据库值重复校验开始。为了便于演示，就不引入数据库了，直接Mock一些数据用来判断是否重复。 1234567891011121314151617181920@Servicepublic class MockTalentDataService &#123; private static List&lt;TalentUser&gt; talentUsers = new ArrayList&lt;&gt;(); static &#123; TalentUser u1 = new TalentUser(1L, \"凌风\", \"18311342567\"); TalentUser u2 = new TalentUser(2L, \"张三\", \"18512343567\"); TalentUser u3 = new TalentUser(3L, \"李四\", \"18902343267\"); talentUsers.add(u1); talentUsers.add(u2); talentUsers.add(u3); &#125; /** * 校验是否重复 */ public boolean checkForDuplicates(String name, String phone) &#123; // 姓名与手机号相等个数不等于0则为重复 return talentUsers.stream().anyMatch(e -&gt; e.getName().equals(name) &amp;&amp; e.getPhone().equals(phone)); &#125;&#125; 其中Mock数据中 ID 为 1 的数据与示例Excel2 中的数据是重复的。EasyPOI 提供了校验的接口，这需要我们自己写一个用于校验的类。在这个类中，可以对导入时的每一行数据进行校验，框架通过 ExcelVerifyHandlerResult 对象来判断是否校验通过，校验不通过需要传递 ErrorMsg。 1234567891011121314151617181920212223@Componentpublic class TalentImportVerifyHandler implements IExcelVerifyHandler&lt;TalentUserInputEntity&gt; &#123; @Resource private MockTalentDataService mockTalentDataService; @Override public ExcelVerifyHandlerResult verifyHandler(TalentUserInputEntity inputEntity) &#123; StringJoiner joiner = new StringJoiner(\",\"); // 根据姓名与手机号判断数据是否重复 String name = inputEntity.getName(); String phone = inputEntity.getPhone(); // mock 数据库 boolean duplicates = mockTalentDataService.checkForDuplicates(name, phone); if (duplicates) &#123; joiner.add(\"数据与数据库数据重复\"); &#125; if (joiner.length() != 0) &#123; return new ExcelVerifyHandlerResult(false, joiner.toString()); &#125; return new ExcelVerifyHandlerResult(true); &#125;&#125; 修改校验处代码，设置校验类对象。 12345678910111213141516171819202122232425@Resourceprivate TalentImportVerifyHandler talentImportVerifyHandler;@PostMapping(\"/upload\")public Boolean upload(@RequestParam(\"file\") MultipartFile multipartFile) throws Exception &#123; ImportParams params = new ImportParams(); // 表头设置为2行 params.setHeadRows(2); // 标题行设置为0行，默认是0，可以不设置 params.setTitleRows(0); // 开启Excel校验 params.setNeedVerfiy(true); params.setVerifyHandler(talentImportVerifyHandler); ExcelImportResult&lt;TalentUserInputEntity&gt; result = ExcelImportUtil.importExcelMore(multipartFile.getInputStream(), TalentUserInputEntity.class, params); System.out.println(\"是否校验失败: \" + result.isVerfiyFail()); System.out.println(\"校验失败的集合:\" + JSONObject.toJSONString(result.getFailList())); System.out.println(\"校验通过的集合:\" + JSONObject.toJSONString(result.getList())); for (TalentUserInputEntity entity : result.getFailList()) &#123; int line = entity.getRowNum() + 1; String msg = \"第\" + line + \"行的错误是：\" + entity.getErrorMsg(); System.out.println(msg); &#125; return true;&#125; 上传 示例Excel2 文件测试，结果输出： 而第七行的数据正是与Mock中的数据相重复的。 六、导入值自定义校验之Collection对象校验上文中还有一个待解决的问题，就是Collection中的对象添加了Hibernate Validator 注解校验但是并未生效的问题，现在就来解决一下。上一步中实现了导入对象的校验类，校验类会校验Excel中的每一条数据， 那我是不是可以直接在校验类中校验Collection中对象了呢？实践证明行不通，因为这个校验类的verifyHandler方法只会被调用一次，所以Collection中只有一条记录。既然这里行不通的话，就只能对导入结果再进行校验了。 因为Collection中的数据EasyPOI校验不到，所以有问题的数据也可能会被框架放到result.getList()中而不是result.getFailList() 中，为了校验需要将两个集合合并为一个集合，使用 EasyPOI 自带的工具类 PoiValidationUtil 进行校验 Collection 中的对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Resourceprivate TalentImportVerifyHandler talentImportVerifyHandler;@PostMapping(\"/upload\")public Boolean upload(@RequestParam(\"file\") MultipartFile multipartFile) throws Exception &#123; ImportParams params = new ImportParams(); // 表头设置为2行 params.setHeadRows(2); // 标题行设置为0行，默认是0，可以不设置 params.setTitleRows(0); // 开启Excel校验 params.setNeedVerfiy(true); params.setVerifyHandler(talentImportVerifyHandler); ExcelImportResult&lt;TalentUserInputEntity&gt; result = ExcelImportUtil.importExcelMore(multipartFile.getInputStream(), TalentUserInputEntity.class, params); System.out.println(\"是否校验失败: \" + result.isVerfiyFail()); System.out.println(\"校验失败的集合:\" + JSONObject.toJSONString(result.getFailList())); System.out.println(\"校验通过的集合:\" + JSONObject.toJSONString(result.getList())); // 合并结果集 List&lt;TalentUserInputEntity&gt; resultList = new ArrayList&lt;&gt;(); resultList.addAll(result.getFailList()); resultList.addAll(result.getList()); for (TalentUserInputEntity inputEntity : resultList) &#123; StringJoiner joiner = new StringJoiner(\",\"); joiner.add(inputEntity.getErrorMsg()); // 校验Collection的元素 inputEntity.getExperienceList().forEach(e -&gt; verify(joiner, e)); inputEntity.getEducationList().forEach(e -&gt; verify(joiner, e)); inputEntity.getAwardList().forEach(e -&gt; verify(joiner, e)); inputEntity.getPunishmentList().forEach(e -&gt; verify(joiner, e)); inputEntity.setErrorMsg(joiner.toString()); &#125; for (TalentUserInputEntity entity : result.getFailList()) &#123; int line = entity.getRowNum() + 1; String msg = \"第\" + line + \"行的错误是：\" + entity.getErrorMsg(); System.out.println(msg); &#125; return true;&#125;private void verify(StringJoiner joiner, Object object) &#123; String validationMsg = PoiValidationUtil.validation(object, null); if (StringUtils.isNotEmpty(validationMsg)) &#123; joiner.add(validationMsg); &#125;&#125; 上传 示例Excel2 ，结果如下： 七、导入值自定义校验之Excel重复行校验上文中对Excel中数据与数据库数据进行重复校验，可有些需求是要求数据库在入库前需要对Excel的的重复行进行校验。这需要在校验类中完成，但校验类中并没有全部行的数据，该如何实现呢？博主的做法是将导入的数据放到 ThreadLocal 中进行暂存，从而达到在校验类中校验Excel重复行的目的。ThreadLocal使用注意完之后一定要及时清理！ 首先定义什么叫重复行，完全相同的两行是重复行，本文中设定name 与 phone 相同的行为重复行，由于只需要比较这两个字段，所以我们需要重写导入对象的equals与hashCode方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788@Datapublic class TalentUserInputEntity implements IExcelDataModel, IExcelModel &#123; // 时间格式校验正则 public static final String DATE_REGEXP = \"(Mon|Tue|Wed|Thu|Fri|Sat|Sun)( )(Dec|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov)( )\\\\d&#123;2&#125;( )(00:00:00)( )(CST)( )\\\\d&#123;4&#125;\"; /** * 行号 */ private int rowNum; /** * 错误消息 */ private String errorMsg; @Excel(name = \"姓名*\") @NotBlank(message = \"[姓名]不能为空\") private String name; @Excel(name = \"性别*\", replace = &#123;\"男_0\", \"女_1\"&#125;) @Pattern(regexp = \"[01]\", message = \"性别错误\") private String genderStr; @Excel(name = \"手机号*\") @Pattern(regexp = \"[0-9]&#123;11&#125;\", message = \"手机号不正确\") private String phone; @Excel(name = \"开始工作时间*\") @Pattern(regexp = DATE_REGEXP, message = \"[开始工作时间]时间格式错误\") private String workTimeStr; @Excel(name = \"民族*\") @NotBlank(message = \"[民族]不能为空\") private String national; @Excel(name = \"语言水平*\") @NotBlank(message = \"[语言水平]不能为空\") private String languageProficiency; @Excel(name = \"出生日期*\") @Pattern(regexp = DATE_REGEXP, message = \"[出生日期]时间格式错误\") private String birthStr; @Excel(name = \"职位*\") @NotBlank(message = \"[职位]不能为空\") private String jobsName; @Excel(name = \"职位类型*\") @NotBlank(message = \"[职位类型]不能为空\") private String categoryName; @Excel(name = \"薪资*\", replace = &#123;\"3K以下_1\", \"3K-5K_2\", \"5K-10K_3\", \"10K-20K_4\", \"20K-50K_5\", \"50K以上_6\"&#125;) @Pattern(regexp = \"[123456]\", message = \"薪资信息错误\") private String salaryStr; @Excel(name = \"工作地点*\") @NotBlank(message = \"[工作地点]不能为空\") private String workArea; @ExcelCollection(name = \"工作经历*\") private List&lt;ExperienceInputEntity&gt; experienceList; @ExcelCollection(name = \"教育经历*\") private List&lt;EducationInputEntity&gt; educationList; @ExcelCollection(name = \"获奖情况\") private List&lt;AwardsInputEntity&gt; awardList; @ExcelCollection(name = \"技能证书\") private List&lt;PunishmentInputEntity&gt; punishmentList; @Excel(name = \"特长\") private String specialty; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; TalentUserInputEntity that = (TalentUserInputEntity) o; return Objects.equals(name, that.name) &amp;&amp; Objects.equals(phone, that.phone); &#125; @Override public int hashCode() &#123; return Objects.hash(name, phone); &#125;&#125; 修改校验类代码，实现重复行的校验逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Componentpublic class TalentImportVerifyHandler implements IExcelVerifyHandler&lt;TalentUserInputEntity&gt; &#123; private final ThreadLocal&lt;List&lt;TalentUserInputEntity&gt;&gt; threadLocal = new ThreadLocal&lt;&gt;(); @Resource private MockTalentDataService mockTalentDataService; @Override public ExcelVerifyHandlerResult verifyHandler(TalentUserInputEntity inputEntity) &#123; StringJoiner joiner = new StringJoiner(\",\"); // 根据姓名与手机号判断数据是否重复 String name = inputEntity.getName(); String phone = inputEntity.getPhone(); // mock 数据库 boolean duplicates = mockTalentDataService.checkForDuplicates(name, phone); if (duplicates) &#123; joiner.add(\"数据与数据库数据重复\"); &#125; List&lt;TalentUserInputEntity&gt; threadLocalVal = threadLocal.get(); if (threadLocalVal == null) &#123; threadLocalVal = new ArrayList&lt;&gt;(); &#125; threadLocalVal.forEach(e -&gt; &#123; if (e.equals(inputEntity)) &#123; int lineNumber = e.getRowNum() + 1; joiner.add(\"数据与第\" + lineNumber + \"行重复\"); &#125; &#125;); // 添加本行数据对象到ThreadLocal中 threadLocalVal.add(inputEntity); threadLocal.set(threadLocalVal); if (joiner.length() != 0) &#123; return new ExcelVerifyHandlerResult(false, joiner.toString()); &#125; return new ExcelVerifyHandlerResult(true); &#125; public ThreadLocal&lt;List&lt;TalentUserInputEntity&gt;&gt; getThreadLocal() &#123; return threadLocal; &#125;&#125; 由于校验类中使用了ThreadLocal，因此需要及时释放，修改导入处的代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Resourceprivate TalentImportVerifyHandler talentImportVerifyHandler;@PostMapping(\"/upload\")public Boolean upload(@RequestParam(\"file\") MultipartFile multipartFile) throws Exception &#123; ExcelImportResult&lt;TalentUserInputEntity&gt; result; try &#123; ImportParams params = new ImportParams(); // 表头设置为2行 params.setHeadRows(2); // 标题行设置为0行，默认是0，可以不设置 params.setTitleRows(0); // 开启Excel校验 params.setNeedVerfiy(true); params.setVerifyHandler(talentImportVerifyHandler); result = ExcelImportUtil.importExcelMore(multipartFile.getInputStream(), TalentUserInputEntity.class, params); &#125; finally &#123; // 清除threadLocal 防止内存泄漏 ThreadLocal&lt;List&lt;TalentUserInputEntity&gt;&gt; threadLocal = talentImportVerifyHandler.getThreadLocal(); if (threadLocal != null) &#123; threadLocal.remove(); &#125; &#125; System.out.println(\"是否校验失败: \" + result.isVerfiyFail()); System.out.println(\"校验失败的集合:\" + JSONObject.toJSONString(result.getFailList())); System.out.println(\"校验通过的集合:\" + JSONObject.toJSONString(result.getList())); // 合并结果集 List&lt;TalentUserInputEntity&gt; resultList = new ArrayList&lt;&gt;(); resultList.addAll(result.getFailList()); resultList.addAll(result.getList()); for (TalentUserInputEntity inputEntity : resultList) &#123; StringJoiner joiner = new StringJoiner(\",\"); joiner.add(inputEntity.getErrorMsg()); // 校验Collection的元素 inputEntity.getExperienceList().forEach(e -&gt; verify(joiner, e)); inputEntity.getEducationList().forEach(e -&gt; verify(joiner, e)); inputEntity.getAwardList().forEach(e -&gt; verify(joiner, e)); inputEntity.getPunishmentList().forEach(e -&gt; verify(joiner, e)); inputEntity.setErrorMsg(joiner.toString()); &#125; for (TalentUserInputEntity entity : result.getFailList()) &#123; int line = entity.getRowNum() + 1; String msg = \"第\" + line + \"行的错误是：\" + entity.getErrorMsg(); System.out.println(msg); &#125; return true;&#125;private void verify(StringJoiner joiner, Object object) &#123; String validationMsg = PoiValidationUtil.validation(object, null); if (StringUtils.isNotEmpty(validationMsg)) &#123; joiner.add(validationMsg); &#125;&#125; 导入示例Excel2，结果如下： 至此，我们就完成了导入的大部分需求。 结语 示例代码 这篇博客花费了我不少时间来写，文中的代码也上传到了GitHub上，并且在写文章的同时，学习了一下Git tag 的使用，读者可以在 GitHub 的分支切换可以切换到不同tag看代码。","tags":[{"name":"easypoi导入Excel","slug":"easypoi导入Excel","permalink":"http://nullpointer.pw/tags/easypoi导入Excel/"},{"name":"easypoi导入Excel时间格式校验","slug":"easypoi导入Excel时间格式校验","permalink":"http://nullpointer.pw/tags/easypoi导入Excel时间格式校验/"},{"name":"easypoi导入Collection校验","slug":"easypoi导入Collection校验","permalink":"http://nullpointer.pw/tags/easypoi导入Collection校验/"},{"name":"easypoi导入重复行校验","slug":"easypoi导入重复行校验","permalink":"http://nullpointer.pw/tags/easypoi导入重复行校验/"},{"name":"easypoi导入重复值校验","slug":"easypoi导入重复值校验","permalink":"http://nullpointer.pw/tags/easypoi导入重复值校验/"}]},{"title":"ParallelsDesktop安装精简版系统","date":"2019-09-12T14:36:34.000Z","path":"ParallelsDesktop安装精简版系统.html","text":"分享一个虚拟机安装精简版 Windows10 系统的方法，只占用 5G 存储空间大小的完整版 win10 系统哦，不要安装那些垃圾阉割过的win7或者xp系统了，win10 也可以占用这么小的空间哦~ 用了 Mac 这么久，感觉开发确实用 Mac 更方便更友好一些，但是有时候就会用到 Windows 系统，比如需要用到 XShell 之类 win 上独有的软件。这时候在 Mac 上安一个 Windows 虚拟机就很有用处了。即使是 Windows 电脑，做一些敏感操作容易中毒时候（比如作死玩玩病毒之类的），也最好隔离系统在虚拟机里鼓捣。 在 Mac 上 Parallels Desktop 安装的虚拟机全屏后，和Mac系统无缝切换，就是舒服~ 啥？你还问我有什么用？如果你买个 128 GB 存储的 Mac 就知道有没有用了。（我的 256 也不够用，因此鼓捣出这么个方法来） Windows 系统很占空间，新安装的系统动辄占用 40G - 60G 存储空间，如果安装到虚拟机中，就显得不是那么划算了。（在虚拟机中安过原版镜像的同学，对于虚拟机占用空间的大小，肯定深有体会） 原版镜像的 ISO 文件一般都要 13G 以上，于是我在 远景论坛 找到一个大神精简过的 Win10 ESD 镜像，只精简了无用的一些服务，无捆绑，镜像包仅 1G 大小，安装后也仅占用 4G 空间，真香！ 如果你不知道我叨叨的是什么，就不用继续往下看了，以下以 Parallels Desktop 虚拟机举例，其他虚拟机比如 Vmware Fusion 之类的请触类旁通。 安装准备几乎所有的虚拟机软件安装时，只支持 ISO 文件，但是一般封装的系统都是 ESD WIM 之类的镜像格式，So，需要转换成 ISO 格式。 我这里给出转换好的大神精简的镜像 ISO 文件，用了一年多了，很稳定，和原版镜像没有任何差别，占用空间相对原版镜像少的不是一点半点。若想要自己封装 ISO，见文末参考链接。 Win10精简版 ISO 镜像以及引导镜像下载 提取码: vcmq 网盘内有两个 ISO 文件，WePE_64_V2.0.iso 为引导镜像，win10.iso 为系统镜像。 下载文件到本地。 安装引导镜像 新建虚拟机，选择 WePE_64_V2.0.iso 这个引导镜像，可能会有如下之类的提示，无视跳过即可。 选择系统类型为 win10 这里随便选一个就行 选择保存位置以及设置虚拟机名称 关闭设定，继续安装虚拟机，直到开机进入 PE 引导系统 安装 win10 配置虚拟机连接的镜像为 win10.iso 打开引导系统内左下角的分区工具 DiskGenius，右键硬盘选择建立新分区（这里的大小是不准确的，不用理会），确定之后，选择保存更改，然后格式化选是（放心不会影响你的宿主机环境） 操作完如图所示（我的是 256G，这里也显示的是 256GB） 打开 Windows 安装器，选择 win10 镜像（在DVD 驱动器内） 选择开始安装，然后点确定，等待安装，安装完需要手动在开始菜单中手动重启。 重启后，系统自动进行 win10系统 的加载，稍等一会儿即可 收尾工作 选择桌面上的系统激活工具，直接选择激活即可。 这时候全屏虚拟机，会发现分辨率不是最佳的，因为你需要安装一下虚拟机工具，退出全屏 选择安装 Parallels Tools 进入我的电脑，DVD驱动器，运行 Autorun 文件进行安装，安装完选择重启系统 重启后，选择最大化（而不是融合模式） 稍等一会，虚拟机会自动调整分辨率到自适应。系统也会成功激活，桌面激活软件会自动消失。 完成！ 亮点看一下虚拟机占用吧嘿嘿~ 希望对你有所帮助吧~ 参考 https://jingyan.baidu.com/article/1876c85261a7f5890b137683.html","tags":[{"name":"ParallelsDesktop安装精简版系统","slug":"ParallelsDesktop安装精简版系统","permalink":"http://nullpointer.pw/tags/ParallelsDesktop安装精简版系统/"},{"name":"Vmvare安装精简版系统","slug":"Vmvare安装精简版系统","permalink":"http://nullpointer.pw/tags/Vmvare安装精简版系统/"},{"name":"精简版win10系统","slug":"精简版win10系统","permalink":"http://nullpointer.pw/tags/精简版win10系统/"},{"name":"虚拟机安装esd格式系统镜像","slug":"虚拟机安装esd格式系统镜像","permalink":"http://nullpointer.pw/tags/虚拟机安装esd格式系统镜像/"},{"name":"虚拟机安装wim格式系统镜像","slug":"虚拟机安装wim格式系统镜像","permalink":"http://nullpointer.pw/tags/虚拟机安装wim格式系统镜像/"}]},{"title":"Java内存分页工具类","date":"2019-07-23T14:34:01.000Z","path":"Java内存分页工具类.html","text":"前言工作过程中，经常会遇到基于内存数据进行分页处理的情况，例如批量更新数据库时，集合过大需要分批更新的情况，还有例如对缓存中的集合数据进行分页获取这种情况。本文提供了通用的内存分页工具，参考了网络上的一些代码，主要基于 subList() 方法实现，希望对你有所帮助！工具类源码在本文底部。 优化前首先来看一下正常如果要实现这些需求的话，代码的实现是怎么样的 1234567891011121314151617181920212223242526272829303132333435// 分页进行批量更新private void batchUpdateStudent(List&lt;Student&gt; students) &#123; int limit = 100; int size = students.size(); int m = size / limit; int n = size % limit; for (int i = 1; i &lt;= m; i++) &#123; List&lt;Student&gt; list = students.subList((i - 1) * limit, i * limit); studentDao.batchUpdate(list); &#125; if (n != 0) &#123; List&lt;Student&gt; list = students.subList(m * limit, students.size()); studentDao.batchUpdate(list); &#125;&#125;// 分页获取数据public List&lt;Student&gt; pageStudents(Integer page, Integer pageSize) &#123; if (page &lt; 1) &#123; page = 1; &#125; int start = (page - 1) * pageSize; int limit = page * pageSize; // 从缓存中获取全量数据 List&lt;Student&gt; students = studentCache.getStudents(); if (CollectionUtils.isEmpty(students)) &#123; return new ArrayList&lt;&gt;(); &#125; if (limit &gt; students.size()) &#123; limit = students.size(); &#125; return students.subList(start, limit), students.size();&#125; 可以看出方法的代码比较冗余，如果多处需要内存分页，重复代码不可避免会有很多！ 优化后12345678910111213141516171819202122// 分页进行批量更新private void batchUpdateStudent(List&lt;Student&gt; students) &#123; RAMPager&lt;Student&gt; pager = new RAMPager&lt;&gt;(students, 100); // 方式一：使用迭代器 Iterator&lt;List&lt;Student&gt;&gt; iterator = pager.iterator(); while (iterator.hasNext()) &#123; studentDao.batchUpdate(iterator.next()); &#125; // 方式二：使用索引 //for (int i = 1; i &lt;= pager.getPageCount(); i++) &#123; // studentDao.batchUpdate(pager.page(i)); //&#125;&#125;// 分页获取数据public List&lt;Student&gt; pageStudents(Integer page, Integer pageSize) &#123; // 从缓存中获取全量数据 List&lt;Student&gt; students = studentCache.getStudents(); RAMPager&lt;Student&gt; pager = new RAMPager&lt;&gt;(students, pageSize); return pager.page(page);&#125; 注：如果只是分页，而不需要关注页码，使用迭代器即可； 工具类源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import java.util.ArrayList;import java.util.Arrays;import java.util.Iterator;import java.util.List;/** * 类名称：RAMPager * 类描述：内存分页工具 * 创建人：WeJan * 创建时间：2019年07月22日 13:43 * Version: 1.1 */public class RAMPager&lt;T&gt; &#123; private List&lt;T&gt; data; private int pageSize; /** * @param data 原始数据 * @param pageSize 每页条数 */ public RAMPager(List&lt;T&gt; data, int pageSize) &#123; this.data = data; this.pageSize = pageSize; &#125; /** * 获取某页数据，从第1页开始 * * @param pageNum 第几页 * @return 分页数据 */ public List&lt;T&gt; page(int pageNum) &#123; if (pageNum &lt; 1) &#123; pageNum = 1; &#125; int from = (pageNum - 1) * pageSize; int to = Math.min(pageNum * pageSize, data.size()); if (from &gt; to) &#123; from = to; &#125; return data.subList(from, to); &#125; /** * 获取总页数 */ public int getPageCount() &#123; if (pageSize == 0) &#123; return 0; &#125; return data.size() % pageSize == 0 ? (data.size() / pageSize) : (data.size() / pageSize + 1); &#125; /** * 元素迭代器 */ public Iterator&lt;List&lt;T&gt;&gt; iterator() &#123; return new Itr(); &#125; private class Itr implements Iterator&lt;List&lt;T&gt;&gt; &#123; int page = 1; Itr() &#123; &#125; public boolean hasNext() &#123; return page &lt;= getPageCount(); &#125; public List&lt;T&gt; next() &#123; int i = page; if (i &gt; getPageCount()) return new ArrayList&lt;&gt;(); page = i + 1; return RAMPager.this.page(i); &#125; &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8); System.out.println(\"原始数据是：\" + list); int pageSize = 2; System.out.println(\"每页大小是：\" + pageSize); RAMPager&lt;Integer&gt; pager = new RAMPager&lt;&gt;(list, pageSize); System.out.println(\"总页数是: \" + pager.getPageCount()); System.out.println(\"&lt;- - - - - - - - - - - - - -&gt;\"); // 无需感知页码情况下使用 Iterator&lt;List&lt;Integer&gt;&gt; iterator = pager.iterator(); while (iterator.hasNext()) &#123; List&lt;Integer&gt; next = iterator.next(); System.out.println(\"next: \" + next); &#125; System.out.println(\"&lt;- - - - - - - - - - - - - -&gt;\"); // 需要指定页码情况使用，页码从第一页开始，且小于等于总页数！ for (int i = 1; i &lt;= pager.getPageCount(); i++) &#123; List&lt;Integer&gt; page = pager.page(i); System.out.println(\"第 \" + i + \" 页数据是:\" + page); &#125; &#125;&#125; 结语希望对你有用~","tags":[{"name":"java内存分页工具类","slug":"java内存分页工具类","permalink":"http://nullpointer.pw/tags/java内存分页工具类/"},{"name":"java内存分页","slug":"java内存分页","permalink":"http://nullpointer.pw/tags/java内存分页/"}]},{"title":"Mysql日期分组无数据查询填充0","date":"2019-05-09T14:30:30.000Z","path":"Mysql日期分组无数据查询填充0.html","text":"前言这篇文章标题不好取。。。(主要是生成连续的日期)，本文关键点有：Mysql 获取指定时间段内的所有日期列表，Mysql 按照日期分组查询没有数据的日期也一并查询出来。 问题产品提出一个需求，需要展示这样的一张折线图，用来反映指定时间段内网站注册用户的增加趋势，于是需要后端的 JSON 工程师给出对应的接口。 疏忽大意具体的表结构和数据是这样的 JSON 工程师不加思索，展开了 CRUD 大法，顺手写下了一个 SQL，不到5分钟，接口完活，测都没测试直接给到了前端开发。 1234select date(t.create_time) as `date`, count(t.id) as numfrom t_user tgroup by `date`; 前端拿到数据后开始绘图，结果画的图完全不对啊，因为时间不是连续的。于是反馈到了 JSON 工程师这里。 JSON 工程师一想，哎哟，没考虑掉某天没有数据的情况，分组查询的话，肯定缺少这一天的数据的。 123456789+------------+-----+| date | num |+------------+-----+| 2019-05-06 | 2 |+------------+-----+| 2019-05-08 | 2 |+------------+-----+| 2019-05-09 | 9 |+------------+-----+ 正确的做法应该是即使某天没有数据，也填充一个 0 作为记录值。 修正查询数据找到了问题就好办了，捋了一下逻辑分成了两步 一、拿到所有日期2019年09月12日更新 感谢评论区中 @一个小可爱（感觉是个有趣的人哈哈）提出的方法，于是我就来更新一下本文了。最好的方式是在 Java 代码中处理生成连续的日期，然后创建一个 map 对象，初始化所有的键值对的值为 0，然后遍历查询出来的按照日期作为 key，之后进行 put 覆盖默认值即可。 提供一个生成连续日期的方法 12345678910111213141516171819public static Set&lt;String&gt; getBetweenDate(String start, String end) &#123; LocalDate startDate = LocalDate.parse(start); LocalDate endDate = LocalDate.parse(end); long between = ChronoUnit.DAYS.between(startDate, endDate); if (between &lt; 1) &#123; return Stream.of(start, end).collect(Collectors.toSet()); &#125; return Stream.iterate(startDate, e -&gt; e.plusDays(1)) .limit(between + 1) .map(LocalDate::toString) .collect(Collectors.toSet());&#125;public static void main(String[] args) &#123; Set&lt;String&gt; days = BaseTest.getBetweenDate(\"2019-08-29\", \"2019-09-02\"); log.info(\"&#123;&#125;\", days);&#125; 二、按照日期分组查询原来的文章可能会造成误解，已更新掉按照日期分组查询，遍历查询结果，覆盖第一步里的时间map的值即可。 结果如下： 1234567891011121314151617+------------+-----+| date | num |+------------+-----+| 2019-05-03 | 0 |+------------+-----+| 2019-05-04 | 0 |+------------+-----+| 2019-05-05 | 0 |+------------+-----+| 2019-05-06 | 2 |+------------+-----+| 2019-05-07 | 0 |+------------+-----+| 2019-05-08 | 2 |+------------+-----+| 2019-05-09 | 9 |+------------+-----+ 顺利达到了目的，收工！ 参考 https://www.cnblogs.com/zhengguangpan/p/10308886.html https://blog.csdn.net/Dai_Aixy/article/details/83144619 https://www.cnblogs.com/dennyzhangdd/p/8073181.html","tags":[{"name":"分组查询无数据填充0","slug":"分组查询无数据填充0","permalink":"http://nullpointer.pw/tags/分组查询无数据填充0/"},{"name":"获取指定时间段内的时间日期列表","slug":"获取指定时间段内的时间日期列表","permalink":"http://nullpointer.pw/tags/获取指定时间段内的时间日期列表/"},{"name":"java生成连续日期","slug":"java生成连续日期","permalink":"http://nullpointer.pw/tags/java生成连续日期/"}]},{"title":"WebMagic源码阅读之Scheduler","date":"2019-01-28T21:26:25.000Z","path":"WebMagic源码阅读之Scheduler.html","text":"本文是 WebMagic 源码阅读第三篇，版本依然是 version-0.0.1 的 tag。作者对于 Scheduler 的描述是如下： 包含url管理和调度的接口。包括url抓取队列，url去重等功能。 阅读Scheduler源码Scheduler 是一个接口类，源码中实现有三个，分别是 默认 Spider 使用 QueueScheduler12// 默认使用基于内存的队列private Scheduler scheduler = new QueueScheduler(); Scheduler 接口只有两个方法12345678910111213/** * 加入一个待抓取的链接 * @param request 待抓取的链接 * @param task 定义的任务，以满足单Scheduler多Task的情况 */public void push(Request request,Task task);/** * 返回下一个要抓取的链接 * @param task 定义的任务，以满足单Scheduler多Task的情况 * @return 下一个要抓取的链接 */public Request poll(Task task); 接下来，分析常用的的两个实现类 QueueScheduler 和 RedisScheduler QueueScheduler 线程安全的队列实现请求的生产消费关系，push 和 poll 都是同步方法，同时因为 LinkedBlockingQueue 的长度是 Integer.MAX_VALUE，如果队列过长，会出现 OOM 的情况。 RedisScheduler 源码中使用 Redis 的 list 结构做 URL 队列，通过 rpush 方法，将新的 URL 放入队列右边，通过 lpop 从队列左侧弹出一个待抓取的 URL 。 使用了 Redis 的有序集合进行 URL 去重，原理是：如果 url 是有序集和的成员，返回 url 的排名。 如果 url 不是有序集和的成员，返回 null。zrank 方法参数如下：zrank(有序集合的key, 成员名称) 当成功添加 URL 到队列，调用 zadd 方法将 URL 放入有序集合当中，便于后续 URL 判重。 总结功力不足，尚且看不出这部分源码的不足之处，只看到 RedisScheduler 中 jedis 释放资源的操作未放到 final 中，有所隐患。 联系独行的路总是孤独的，希望能找到一些小伙伴共同进步~ QQ 群号:967808880","tags":[{"name":"源码解析","slug":"源码解析","permalink":"http://nullpointer.pw/tags/源码解析/"}]},{"title":"WebMagic源码阅读之Spider","date":"2019-01-15T13:28:37.000Z","path":"WebMagic源码阅读之Spider.html","text":"总是觉得自己平时总是在写逻辑代码，于是学习设计模式，但是又觉得无处可施展，对于如何提高自己的编码能力总是困惑，也曾看了一些源码，但总因为难度太大而不了了之。因为大多数的开源框架源码随着版本的更迭，难度越发上升源码愈发晦涩难懂，但源码还是不能不读，所以觉得从最简单的源码开始读起吧，因为工作中，用到了 WebMagic 框架，于是决定从它开始，为了降低阅读难度，我选择了第一个 tag 版本的源码开始阅读，从易入难，由浅入深，希望自己 2019 年代码能力能有所提升! 下载源码首先在 IDEA 上把 WebMagic 的源码 clone 到本地 然后 checkout 第一个版本的 tag 在对话框中，使用智能提示快捷键展示出所有的 tag，默认快捷键是 ctrl(command) + 空格，因为可能会和输入法冲突，我修改成了 command + ; 我选择了 version-0.1.0 版本，然后基于这个 tag，创建了自己的本地分支，便于在上面进行添加注释等操作。 阅读 Spider 源码参见官网架构图，Spider 这个组件是 WebMagic 框架的核心所在，同时也是爬虫的入口类，Spider 将各大组件串联起来，共同工作。 Spider 实现了 Task 和 Runnable 接口，Task 提供了两个接口方法，一个是 getSite()，另外一个是 getUUID()，爬虫运行时，将爬虫自身引用传递到各个组件进行处理。 一般使用 WebMagic 时，通过 Spider.create() 方法创建爬虫，创建时可以指定 Downloader、Scheduler、PageProcessor、Pipeline 这几个组件，作者将这四个组件抽象成了接口，方便扩展，其中 PageProcessor 是必须指定的组件，其他的组件 WebMagic 提供了对应的默认值。 Spider 类主要核心是 run 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Overridepublic void run() &#123; // 通过 AtomicInteger 的 CAS 乐观锁保证多线程下，爬虫只会启动一次 // 如果爬虫状态当前状态是 STAT_INIT，则更新成 STAT_RUNNING，否则抛出异常 if (!stat.compareAndSet(STAT_INIT, STAT_RUNNING)) &#123; throw new IllegalStateException(\"Spider is already running!\"); &#125; // 添加启动 URL if (startUrls != null) &#123; for (String startUrl : startUrls) &#123; scheduler.push(new Request(startUrl), this); &#125; &#125; // 从队列中取出一个请求 Request request = scheduler.poll(this); // 添加管道处理，如果为空添加，输出到控制台 if (pipelines.isEmpty()) &#123; pipelines.add(new ConsolePipeline()); &#125; //single thread if (executorService==null)&#123; // 默认单线程 // 循环调度所有的 Request while (request != null) &#123; processRequest(request); request = scheduler.poll(this); &#125; &#125; else &#123; // 如果已经调用 Spider.thread(n) 方法则走这个分支 //multi thread // 初始化活跃线程数量为 0，当爬虫线程启动后，如果活跃线程变成 0 时，跳出死循环 final AtomicInteger threadAlive = new AtomicInteger(0); while (true) &#123; if (request == null) &#123; //when no request found but some thread is alive, sleep a while. try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; &#125; &#125; else &#123; final Request requestFinal = request; // 执行线程之前对活跃线程数进行原子性自增操作 threadAlive.incrementAndGet(); executorService.execute(new Runnable() &#123; @Override public void run() &#123; processRequest(requestFinal); // 处理完请求之后对活跃线程数进行原子性自减操作 threadAlive.decrementAndGet(); &#125; &#125;); &#125; // 循环从队列中取新的待抓取 URL request = scheduler.poll(this); // 当活跃线程数为 0，说明队列已经为空了 if (threadAlive.get() == 0) &#123; // 为了保险起见，再重试获取一下请求 request = scheduler.poll(this); // 如果依然为 null，则跳出 while 循环 if (request == null) &#123; break; &#125; &#125; &#125; // 关闭线程池 executorService.shutdown(); &#125; // 当所有请求抓取完毕，即活跃线程数量变成 0 时，修改爬虫状态为已停止 stat.compareAndSet(STAT_RUNNING, STAT_STOPPED);&#125; 总结从源码中看到，对于多线程共享资源活跃线程数的处理，使用到了 Integer 的原子类 AtomicInteger，保证活跃线程数量在多线程情况下统计结果的正确性。 作者将四大组件抽象成了接口，面向接口编程，方便扩展，很值得学习。 因为是初始版本，不可避免的会有一些明显的 Bug 存在，比如在 page 类的 addTargetRequests 方法中应该用 continue，但是错用了 break的情况，所以初始版本源码阅读完成后，需要读读新一些版本的源码。 联系独行的路总是孤独的，希望能找到一些小伙伴共同进步哈哈~ QQ 群号:967808880","tags":[{"name":"源码解析","slug":"源码解析","permalink":"http://nullpointer.pw/tags/源码解析/"}]},{"title":"2018年终总结","date":"2018-12-31T02:00:25.000Z","path":"2018年终总结.html","text":"与去年一样，已然 2018 年尾，全国多数城市几乎都大大小小地落过几场雪；北京，却无半点下雪的征兆，初雪应该也要拖延到明年开春之后了吧！与去年一样，立下的目标除了继续坚持写博客之外，其他全部被抛诸脑后；如果不是把写博客这个目标进行了量化，强迫着自己去执行，怕是所有的目标都只是有始无终罢了！ 体验·突破从 2017 年始，一直以来，记录每天时间支出的习惯都在保持着，但是目前对于自己的作用并不大；也仅仅是年底这个时候，需要写年终总结的时候，看一眼全年时间统计的分布情况，让自己后悔未能珍惜光阴而已。借助于 APP 记录时间的便利性，虽然一天的时间能在 5 分钟内全部记录完毕，但是有时候也会想，记这个对自己到底有什么用呢？ 也许量变或许能产生质变吧！柳比歇夫几十年如一日的记录，世人也不是没有弄清他记录时间到底有什么用吗？其中的妙处可能也只有柳比歇夫自己知道吧！ 知道自己生命流逝的所有时间，能让自己变得更加自律，其实这样就足够了！ 年中，和女朋友一起去看了一次房子，当时就决定在那里买，做什么决定都很快，买房这件事也不例外。买之前觉得自己是刚需，买了之后，其实买不买也并非那么重要。但是结婚的话，房子还是要有的，就这样上车了，2 年后才能交房，目前的体验就是如果不是每个月银行要从卡里扣除房贷，我都快忘了自己也算是买过房的人了😂。 家庭·生活和朋友出去吃饭的时候，总是会感叹我们活的有点像行尸走肉，除了上班，回家，几乎没有其他的事情去做，不像那些在广场跳街舞，玩滑板，玩摄影的人活的有趣，总是会感叹这才是生活！ 去年也总是被女朋友拉着去北京的各大景点公园玩，但也只是走马观花地游览，今年索性就没有再去过那些地方了，人对于找不到乐趣的事情一直是抗拒的。于是国庆跑到上海玩了几日，除了迪士尼，也并未觉得上海更有趣。 也该去找到并且去做一件有意义有意思的事情了。 健康·身体想起今年印象最深的一件事，如何看待年仅 25 岁的大疆 DJI 员工或因过劳而猝死家中？，仅仅 25 岁而已！这种情况发生在了同龄人的身上，真的是太可怕了！这件事情也给自己敲响了警钟，保护好自己的身体很重要。 保证健康的作息，规律的饮食，合理的运动，很有必要。 虽然不加班，但是我坐在电脑前时间并不比加班的人少，健康的作息，规律的饮食，这两点都有做到，早睡早起，一日三餐，只是缺乏一些运动。 学习·成长一直找不到学习与生活之间的平衡点，学习的时候，抵抗学习之外的任何事情；不学习的时候，怀有深深的负罪感，但是学习效率总没有期望的那样高。 合理安排好学习与休闲的关系，又是一项需要自我学习提升之处。 自然码双拼打字法，是看到知乎上一名大 V 推荐学习的技能，花了不到 3 个小时就能基本掌握。比如要打出“中国”这两个字，如果使用全拼的话，需要输入 zhonggo， 至少需要敲 8 下键盘，而用双拼，只需要输入 vsgo，加上空格选择只需要 5 下。双拼也没有五笔打字那样难以学习，对自己的好处还是显而易见的，少敲击键盘也是保护了自己的手指头。 之前一直纠结看书的数量，囫囵吞枣地看了一些书，看完的时候几乎已经忘了这本书到底讲了些什么，买的 kindle 现在已经放在角落默默吃灰中…… 今年没有用 kindle 看书，而是买了一些经典的技术实体书来看，看书的效果也比用 kindle 看的效果要好，kindle 还是更适合看小说鸡汤励志书这类书吧！ 工作·事业最近这两年，逐渐觉得自己的技术提升速度变得相当慢了，今年一年，在脑海里能马上想起来的就是学习了 ES，搭了个公司公用搜索工程提供网站的搜索，其他的几乎就想不起来了；学习了 Spring Cloud 相关技术，但是没有真正地去实践应用过，都快遗忘了😂，希望能有机会实践吧；最近做旧单体项目的重构为分布式应用的工作，考虑到需要平衡迁移与团队技术，保守地使用了 Dubbo 来实现，做之前觉得很简单啊，就是把之前代码迁移过来，改改就行了，做的时候才发现，如果要按照我期望中的项目规范/代码质量去做，几乎就又是重写了一半代码，又给自己埋了个坑。目前也只有自己在做，希望来年能有一个并肩战斗的伙伴，毕竟只有自己在摸索着行进，还是很慢的。 2019要做的事情量化自己的目标，处理好生活与学习的平衡关系。","tags":[{"name":"年终总结","slug":"年终总结","permalink":"http://nullpointer.pw/tags/年终总结/"}]},{"title":"dubbo自定义异常传递信息丢失问题解决","date":"2018-12-10T22:39:07.000Z","path":"dubbo自定义异常传递信息丢失问题解决.html","text":"目前计划对已有的单体项目进行组织架构拆分，调研了分布式系统中常用中间件 Dubbo 和 Spring Cloud，选择了 Dubbo，可以对当前现有项目进行平滑升级改造。但是一开始就遇到了麻烦，自定义异常在传递的过程中变成了 RuntimeException，统一异常处理 GlobalExceptionHandler 无法获取异常信息。 问题重现项目进行统一异常处理，抽取了一个通用异常 ServiceException，此异常是非受检异常，即继承于 RuntimeException。调研时发现如果服务提供方即 provider 抛出了 ServiceException 异常，consumer 服务消费方就会收到一个 RuntimeException 异常，而 ServiceException 异常的内容被包含在了 RuntimeException 的异常堆栈中 1234[Request processing failed; nested exception is java.lang.RuntimeException: io.github.mosiki.common.exception.ServiceException: missing_required_parametersio.github.mosiki.common.exception.ServiceException: missing_required_parameters at io.github.mosiki.provider.HelloService.sayHello(HelloService.java:20) at com.alibaba.dubbo.common.bytecode.Wrapper1.invokeMethod(Wrapper1.java) 而我的统一异常处理是这样的，只处理 ServiceException 以及 Exception，因此就无法获取到原始异常的信息了。123456789101112131415@Slf4j@RestControllerAdvicepublic class GlobalExceptionHandler &#123; @ExceptionHandler(ServiceException.class) public Result handlerServiceException(ServiceException ex) &#123; return Result.failure(ex.getCode(), ex.getMessage()); &#125; @ExceptionHandler(&#123;Exception.class&#125;) public Result handlerException(Exception ex) &#123; log.error(\"发生未知异常：&#123;&#125;\", ex); return Result.failure(HttpStatus.INTERNAL_SERVER_ERROR.value(), \"服务器打了个小盹儿~请稍候再试\"); &#125;&#125; 访问接口将返回如下，异常中原有信息丢失。 上网搜索发现，这是因为 dubbo 的异常处理类 com.alibaba.dubbo.rpc.filter.ExceptionFilter 进行处理后的结果，Debug 之后确实如此，dubbo 在此进行了转换。 问题解决之道现在我想要 provider 把自定义的异常原封不动的抛给 consumer 进行处理，于是有了如下思路： 禁用 provider 的 ExceptionFilter 让 GlobalExceptionHandler 处理 consumer 的异常 按照此思路做就很简单了，网上大多文章的办法都比较麻烦，有用 AOP 处理的，甚至还有让自己修改编译源码上传私服的-_-||，本文给出比较简便的方法，提供参考。 禁用provider的ExceptionFilter修改 provider 的配置，我这里使用 yml 配置文件，其他类型如 xml/properties 也同理，设置 provider 的 filter 为 -exception，这样异常就不会被处理而是直接抛出了。1234567891011dubbo: application: name: provider protocol: name: dubbo port: 20100 registry: address: 127.0.0.1:2181 protocol: zookeeper provider: filter: -exception GlobalExceptionHandler捕获ServiceException只是禁用了 provider 的 ExceptionHandler 还不能完全达到我们的目的，访问接口，provider 抛出异常 consumer 正确接收为 ServiceException。 12345[Request processing failed; nested exception is io.github.mosiki.common.exception.ServiceException: missing_required_parameters] with root causeio.github.mosiki.common.exception.ServiceException: missing_required_parameters at io.github.mosiki.provider.HelloService.sayHello(HelloService.java:20) ~[na:na] at com.alibaba.dubbo.common.bytecode.Wrapper1.invokeMethod(Wrapper1.java) ~[na:na] 我们处理一下 GlobalExceptionHandler。 SpringBoot 主要这个启动类的位置和全局异常处理器的位置，一定要保证异常处理器在启动类的同级包或者在启动类的子包当中，否则异常处理器将不生效！ 效果展示以上两步完成后，重启服务，访问接口测试。 拿到了 provider 抛出的原始自定义异常，如此问题就解决了。 代码下载 https://github.com/Mosiki/dubbo-exception-example-parent 参考 https://blog.csdn.net/yangzaizi/article/details/80638306","tags":[{"name":"dubbo自定义异常","slug":"dubbo自定义异常","permalink":"http://nullpointer.pw/tags/dubbo自定义异常/"},{"name":"dubbo异常堆栈被吃","slug":"dubbo异常堆栈被吃","permalink":"http://nullpointer.pw/tags/dubbo异常堆栈被吃/"},{"name":"dubbo处理自定义异常问题","slug":"dubbo处理自定义异常问题","permalink":"http://nullpointer.pw/tags/dubbo处理自定义异常问题/"}]},{"title":"IDEA远程仓库版本回滚","date":"2018-12-02T11:10:42.000Z","path":"IDEA远程仓库版本回滚.html","text":"使用 git 进行项目的版本控制时，肯定会遇到回滚版本的情况，回滚有两种，一种是本地仓库回滚，另外一种是远程仓库回滚。以下详细讲解两种回滚方式，本文主要讲解远程回滚，以及常见使用误区。 本地仓库回滚本地回滚就很简单，只是要撤销 commit 信息即可。可以直接使用 Reset HEAD进行回滚。 HEAD^ 是还原到上一个版本，HEAD^^ 是还原到上上一个版本。Reset Type 有三种： mixed 默认方式，只保留源码，回退commit和index信息soft 回退到某个版本，只回退了commit的信息，不会恢复到index file一级。如果还要提交，直接commithard 彻底回退，本地源码也会变成上一个版本内容 错误的远程仓库回滚方式远程仓库代码回滚，如果上 Google 上一搜，基本出来的答案都是使用 Reset HEAD 加上 git push -f 强制提交的方式。 我本地测试了一下，如果有两个人同时在这条分支上开发，其中一个人 小 A 如果使用这种方式进行回滚了远程仓库代码，另外一个人 小 B 此时本地还是回滚之前的代码，小 B 那里就会出现本地代码版本高于远程分支版本的情况，此时若小 B 进行了 push 操作，之前小 A 回滚的代码就又会被推送到远程的仓库了。除非小 B 在回滚后，将其本地仓库的版本也进行回滚。才能保证不会把回滚的代码重新 push 回远程仓库，但是如果开发人员多，就无法保证所有开发人员的本地分支都回滚！ 因此，不要使用 git push -f 这种方式来强制回滚远程仓库代码！ 正确的远程仓库回滚方式 如图所示，现在需要将远程仓库回滚到 dev-103 这次提交点。 选中 dev-103 这一行，右键选择 Revert 这里提示需要 commit commit 之后，push 到远程仓库中。最终结果如下： 新增了一条提交记录，查看代码已经被回滚了 其他开发人员如果进行了 pull，其本地仓库也被正确回滚到了 dev-103 提交点，就无需再担心回滚的代码又被重新 push 到远程仓库啦。 搞定，收工！","tags":[{"name":"Git远程仓库回滚","slug":"Git远程仓库回滚","permalink":"http://nullpointer.pw/tags/Git远程仓库回滚/"},{"name":"Git本地仓库回滚","slug":"Git本地仓库回滚","permalink":"http://nullpointer.pw/tags/Git本地仓库回滚/"}]},{"title":"ES搜索结果调优","date":"2018-11-29T14:13:58.000Z","path":"ES搜索结果调优.html","text":"自从使用 ElasticSearch 重构了主站的搜索项目之后，之后又陆续接入了其他两个项目，目前使用 SpringBoot 方式跑了一个伪集群，主站使用的时候，比较稳定，没有人反馈说有问题。 但新接入的一个站点商务反馈说，搜索不够准确，完全匹配的关键词不是排在搜索结果列表首位，跑到搜索上去看了一眼，确实完全匹配的结果分数不是最高的，导致没有排在结果首位，今天就来解决这个问题。 默认匹配查询先看看我之前写的查询代码片段， 12MultiMatchQueryBuilder matchQuery = QueryBuilders.multiMatchQuery(query.getQueryString(), \"name\", \"author\");boolQuery.must(matchQuery); 这种写法，完全没有对搜索结果的平分进行干扰，只是按照 ES 的默认分词计算匹配度的结果。 权重查询我尝试了使用权重查询，即提升某些字段的权重，但是设置之后，结果反而更加不尽如人意。 12boolQuery.should(QueryBuilders.matchQuery(\"name\", queryString).boost(3.0f));boolQuery.should(QueryBuilders.matchQuery(\"author\", queryString).boost(1.f)); 这样进行查询之后，如果想要查询作者，但是作品名称的权重更高些，所以完全匹配的作者也被排在了后面。 最佳字段查询看了官方博客和一篇博客文章，发现 multi-match-query 的高级查询方式。 multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即： best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。 这里我们想要搜索时，完全匹配的关键字排名更靠前，所以这里使用最佳字段 best_fields 进行查询 12345MultiMatchQueryBuilder multiMatchQuery = QueryBuilders .multiMatchQuery(queryString, \"name\", \"author\") .type(MultiMatchQueryBuilder.Type.BEST_FIELDS) .tieBreaker(0.1f); boolQuery.must(multiMatchQuery); 首先设置 type 为 BEST_FIELDS，其次，我们想要完全匹配的分数高点，那么就让没有完全匹配的分文档评分低即可，我这里乘以了 0.1 的系数，系数的范围是 0-1 之间。 加上了系数之后，不完全匹配的文档评分就被拉开了，就达到了我的最终目的。 最佳字段优化2019年01月19日08:01:31 更新通过上一步优化，已经提升了完全匹配文档的评分，但是还不足以拉开评分 现在是作品名称/作者两个字段都存在相同的值，但是想让作品名称字段的权重更高点，即搜索相同值的时候，优先搜索出作品名为该值的数据最终优化结果： 代码实现： 12345678910111213141516171819202122232425262728293031public void testHighlightQuery() &#123; BookQuery query = new BookQuery(); query.setQueryString(\"穿越\"); // 复合查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); PageRequest pageRequest = PageRequest.of(query.getPage() - 1, query.getSize()); NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder() .withQuery(boolQuery) .withHighlightFields( new HighlightBuilder.Field(\"name\").preTags(\"&lt;span style=\\\"color:red\\\"&gt;\").postTags(\"&lt;/span&gt;\"), new HighlightBuilder.Field(\"author\").preTags(\"&lt;span style=\\\"color:red\\\"&gt;\").postTags(\"&lt;/span&gt;\")) .withPageable(pageRequest); // 以下为查询条件, 使用 must query 进行查询组合// MultiMatchQueryBuilder matchQuery = QueryBuilders.multiMatchQuery(query.getQueryString(), \"name\", \"intro\", \"author\");// boolQuery.must(matchQuery); String queryString = query.getQueryString(); // 最佳字段 + 降低除了name之外字段的权重系数 MatchQueryBuilder nameQuery = QueryBuilders.matchQuery(\"name\", queryString); MatchQueryBuilder authorQuery = QueryBuilders.matchQuery(\"author\", queryString).boost(0.8f); DisMaxQueryBuilder disMaxQueryBuilder = QueryBuilders.disMaxQuery().add(nameQuery).add(authorQuery); queryBuilder.withQuery(disMaxQueryBuilder); NativeSearchQuery searchQuery = queryBuilder.build(); Page&lt;Book&gt; books = elasticsearchTemplate.queryForPage(searchQuery, Book.class, extResultMapper); books.forEach(e -&gt; log.info(\"&#123;&#125;\", e)); // &lt;span style=\"color:red\"&gt;穿越&lt;/span&gt;小道人&#125; 主要通过 QueryBuilders.disMaxQuery 结合 boost 对非核心字段降低权重来完成 资源下载 https://github.com/Mosiki/SpringDataElasticSearchQuickStartExample 参考 https://www.cnblogs.com/yjf512/p/4897294.html dis_max 查询","tags":[{"name":"ElasticSearch完全匹配结果调优","slug":"ElasticSearch完全匹配结果调优","permalink":"http://nullpointer.pw/tags/ElasticSearch完全匹配结果调优/"},{"name":"es查询调优","slug":"es查询调优","permalink":"http://nullpointer.pw/tags/es查询调优/"}]},{"title":"github代码clone加速","date":"2018-11-24T08:52:01.000Z","path":"github代码clone加速.html","text":"这阵子想看看开源项目 MyBatis 的源码，结果使用 git 的 clone 命令怎么也 clone 不下来，我以为是网速慢，上 Google 一搜，原来 Github 的域名被 DNS 污染了，我说呢怎么访问 Github 一直都很慢，知道了问题原因就解决掉这个恶心的问题 更新一下2019年09月03日更新： 最近发现用 hosts 方式不太好使了，如果改 hosts 的方式也不起作用的话，可以使用这个绝招了。 步骤一：fork 你想要 clone 的库 步骤二：登录码云，登录成功后，点击网页右上角加号，选择 从 GitHub 导入仓库 这个过程码云会让你去 GitHub 授权拉取你的仓库，选择同意就行了。 步骤三：找到你在 GitHub 中 fork 的仓库选择导入，等待一会儿就行了，如果仓库较大，可能导入会花一些时间。 步骤四：使用码云的地址进行 clone 即可，国内的速度还是非常快的！ 备注：此方法只适用于临时下载仓库，如果要是提 PR 什么的，还是使用改 hosts 方法或者走代理的方式吧。 2019年06月16日更新一下： 看评论有人说不好使，我在 mac 上本地测试了一下（未开启全局代理），红框之上是未指定 hosts 的结果，速度很慢，平均不到 50k/s 的速度，红框内是切换到最新 host 之后的结果，速度一直在增加，达到 3.25MB/s，效果显而易见。 Tips: 如果切换到最新 hosts 后还是很慢，可以重新开一个终端尝试，或者断开再重连一下网络，可能是 hosts 还未生效。（switch hosts 在 win 下需要使用管理员身份打开，mac 下需要输入开机密码获得权限） 加速访问在网上搜索一圈，好多文章中提速的方法就是修改 git 的代理配置，前提是有 SS 之类的{代}{理}工具，然而我开了全局{代}{理}，设置了 git 的代理配置，clone 的速度还是很慢，只能到 30kb/s，如果库大点，下个几天几夜都可能😅 设置代理不好使，就继续搜索解决方法，在 github 上找到一个 issues，其中提供了 github 的 host列表。 GitHub中国加速访问 随着 issue 找到了一个生成 hosts 的 repo，但是这个 repo 的 hosts 并不是每日更新的，于是自己 fork 了一份，写了个简单 crontab，每日执行生成更新 hosts 文件。 仓库地址：https://github.com/Mosiki/github 食用方法最麻烦的方式就是去手动修改 host 文件，最简单的方式就是下载 Switch Host 软件进行 host 修改，跨平台，因为 hosts 文件每日都会自动更新，所以需要本地的 hosts 也能自动更新， 好在 SwitchHosts 提供了远程 hosts 的功能。 复制如下 hosts 地址（测试发现GitHub的网址有可能访问不了，因而更换成了码云的镜像仓库地址） 1https://gitee.com/Mosiki/github/raw/master/github_hosts.txt 新增远程 hosts笔者在 mac 下操作，win 是同理。 设置自动更新选择 24 小时即可，第一次添加的时候需要手动点击刷新按钮，刷新获取一下远程的 hosts更新完成之后点击确定保存即可。 打开 hosts 开关 如图设置完，把开关打开就 Ok 了。 测试Clone速度由 30kb/s 变成 4M/s 了 😂 修改后，在 GitHub 的 release 中下载东西也变快了。","tags":[{"name":"github clone慢解决","slug":"github-clone慢解决","permalink":"http://nullpointer.pw/tags/github-clone慢解决/"},{"name":"github 下载慢解决","slug":"github-下载慢解决","permalink":"http://nullpointer.pw/tags/github-下载慢解决/"}]},{"title":"IDEA之Git分支以及Stash使用","date":"2018-10-31T00:31:42.000Z","path":"IDEA之Git分支合并以及Stash使用.html","text":"随着公司开发人员的增加，以及多需求的并行开发，功能上线就会碍手碍脚；害怕自己没写完的代码被别人部署到线上，害怕别人代码没写完被自己部署到线上；总之功能上线之前还要和所有开发沟通，能不能部署代码？如果只是几个人的团队倒也无妨，但是开发人员多了，沟通成本就很高了。于是 Git 的分支就发挥它的作用了，本文讲解工作中使用 IDEA 进行分支的管理以及合并，以及其他 Git 使用技巧。 环境准备为了演示，先用 IDEA 创建一个简单工程，提交到 git 远程仓库当中。 dev-100 分支创建现在接到了一个编号为 100 的需求，我们在 master 基础上，创建 dev-100 分支 创建新分支 dev-100的同时，并切换到 dev-100 分支。 dev-100 分支代码开发在 dev-100 分支编写需求编号为 100 的 功能，代码完成后进行 commit 以及 push（如果这个分支只有你一个人在开发的话，就不用 push 到远程分支了，只需要 commit 即可） 分支合并现在我们要把 dev-100 分支上的代码合并到 master 主分支上先切换到 master 分支 合并 dev-100 分支到 master 分支之前，建议先对 master 代码进行 pull 更新操作，然后再执行 Merge into Current 如果没有冲突，dev-100 中的代码就会被合并到 master 分支上了，合并成功后，需要 push 才能推送到远程仓库 取消分支合并合并完成后，但是由于一些问题，我们想要取消本次合并，右键 git，选择 Reset HEAD HEAD^ 是还原到上一个版本，HEAD^^ 是还原到上上一个版本。Reset Type 有三种： mixed 默认方式，只保留源码，回退commit和index信息 soft 回退到某个版本，只回退了commit的信息，不会恢复到index file一级。如果还要提交，直接commit hard 彻底回退，本地源码也会变成上一个版本内容 一般使用默认的 mixed 或者粗暴的 hard 方式。我们这里是取消合并，所以选择 Hard 方式，并且是HEAD^还原到上一个版本，回退后恢复了原来 master 的代码。 解决合并冲突问题接下来演示合并冲突，此时是在 master 分支，我们修改文件，并 commit 以及 push 到远程仓库。 此时再把 dev-100 分支合并到 master 分支就会提示冲突。 双击冲突文件，处理冲突。处理完成后，点击 apply 即可，如果有多个冲突文件，都按照这种方式处理，这是我们处理完冲突之后的代码。 dev-100 分支已经被成功合并到 master 了，就可以删除了。可以直接删除远程 dev-100 分支，删除时 IDEA 会提示是否同时删除本地的 dev-100 分支，勾选即可。 现在我们把分支合并的结果 push 到远程仓库。 代码暂存之git stash编号 100 的需求完成之后，现在我们又接到一个新的需求，正在 dev-101 分支进行开发，开发还未完成。 突然线上出现 bug，需要我们紧急进行修改，于是我们要基于最新的 master 分支新建一个 bug 分支 bug-12，需要先切换到 master 分支，但是当前分支的代码没有commit， 如果直接切换到 master 分支的话，dev-101 分支上的新增代码就会跑到 master 分支，而代码又不能此时 commit ，于是就轮到 stash 出场了。Stash 会保存当前工作进度，会把暂存区和工作区的改动保存起来。添加备注，选择 CREATE STASH。你会发现当前工作区内的代码被恢复成了原样。 代码暂存还原此刻切换到 master 分支，并创建 bug-12 分支进行修复 bug，修复完成后合并到 master 分支并 push 到远程仓库，上文已经演示如何合并，在此不再赘述。 将 bug-12 与 master 合并完成之后，现在要接着写 dev-101 需求代码，首先先切换到 dev-101 分支；但是之前的代码已经被我们放到了 git 的 stash 当中，我们现在要把代码还原到工作区当中。选择 Unstash Changes 选择之前保存的，同时勾选 Pop stash（还原完成后，会自动删除这个 stash），确定后，工作区之前写的代码就又回来了。 结语Stash 利用好了，就可以自如切换分支，面对突如其来的需求也不必烦恼了~","tags":[{"name":"IDEA Git分支合并","slug":"IDEA-Git分支合并","permalink":"http://nullpointer.pw/tags/IDEA-Git分支合并/"},{"name":"IDEA 使用Stash","slug":"IDEA-使用Stash","permalink":"http://nullpointer.pw/tags/IDEA-使用Stash/"},{"name":"IDEA 处理分支合并冲突","slug":"IDEA-处理分支合并冲突","permalink":"http://nullpointer.pw/tags/IDEA-处理分支合并冲突/"}]},{"title":"MongoDB 极简实践入门","date":"2018-10-27T01:20:46.000Z","path":"MongoDB极简实践入门.html","text":"以前学习 MongoDB 时候参考的一篇文章，写的蛮好的，很适合 MongoDB 的入门学习，遂转载过来。 MongoDB 极简实践入门1. 为什么用MongoDB？传统的计算机应用大多使用关系型数据库来存储数据，比如大家可能熟悉的MySql, Sqlite等等，它的特点是数据以表格(table)的形式储存起来的。数据库由一张张排列整齐的表格构成，就好像一个Excel表单一样，每个表格会有若干列，比如一个学生信息表，可能包含学号、姓名、性别、入学年份、高考成绩、籍贯等等。而表格的每一排，则是一个个学生的具体信息。在企业级应用和前互联网时代，关系型数据库几乎是不二选择。关系型数据库的特点是有整齐划一的组织，很方便对数据进行描述、插入、搜索。 想象有一个传统的网上服装商店吧，它的主要的数据可能是储存在一张叫products的表单里，表单可能包含这些列：商品编号(ID)、名称(Name)、商家(brand)、主目录(cate)、子目录(sub-cat)、零售价(price)、是否促销(promotion)等等。如果有一个用户想要查找所有价格低于300元的正在促销的鞋子的编号和名称，则可以执行类似于以下的SQL语句： 1SELECT ID, name FROM products WHERE cate=&apos;shoes&apos; AND price&lt;300 and AND promotion=true; SQL具备了强大了的深度查询能力，能满足各式各样的查询要求。而如果要对数据进行添加和删除，成本也是非常低的。这些是SQL的优势之一， 但随着互联网的兴起以及数据形式的多样化，四平八稳的SQL表单在一些领域渐渐显现出它的劣势。让我们通过一个例子来说明。考虑一个博客后台系统，如果我们用关系型数据库为每篇博客(article)建一个表单的话，这个表单大概会包括以下这些列： ID Title Description Author Content Likes A_1 Title1 Political Article Joe Content 1 12 A_2 Title2 Humorous Story Sam Content 2 50 这时候用SQL数据库来存储是非常方便的，但假如我们要位每篇文章添加评论功能，会发现每篇文章可能要多篇评论，而且这个数目是动态变化的，而且每篇评论还包括好几项内容：评论的人、评论的时间、以及评论内容。这时候要将这些内容都塞进上述的那个表，就显得很困难。通常的做法是为评论(comment)单独建一个表： ID Author Time Content Article C_1 Anna 2014-12-26 08:23 Really good articles! A_1 C_2 David 2014-12-25 09:30 I like it! A_1 类似地，每篇文章可能会有若干标签(tags)。标签本身又是一个表单： ID Category Tags Content Article T_1 Anna 2014-12-26 08:23 Really good articles! A_1 T_2 David 2014-12-25 09:30 I like it! A_2 而博客的表格则要通过foreign key跟这些相关联的表格联系起来(可能还包括作者、出版社等其它表格)。这样一来，当我们做查询的时候，比如说，“找出评论数不少于3的标签为‘政治评论’的作者为Sam的文章”，就会涉及到复杂的跨表查询，需要大量使用join语句。这种跨表查询不仅降低了查询速度，而且这些语句写起来也不简单。 那么，如果用MongoDB数据库来实现，可以如何设计数据模型呢？很简单，像下面这样[1]： 123456789101112131415161718_id: POST_ID title: TITLE_OF_POST, description: POST_DESCRIPTION, author: POST_BY, tags: [TAG1, TAG2, TAG3], likes: TOTAL_LIKES, comments: [ &#123; user:&apos;COMMENT_BY&apos;, message: TEXT, dateCreated: DATE_TIME, &#125;, &#123; user:&apos;COMMENT_BY&apos;, message: TEXT, dateCreated: DATE_TIME, &#125; ] 在MongoDB里，每篇博客文章以一个文档(document)的形式保存起来，而文档内部包含了很多项目，比如title tags等，每一个项目都是key-value的形式，即有一个项目的名字，比如title，以及它的值TITLE_OF_POST。而重要的是，一个key可以有多个values，他们用[]括起来。 这种“宽松”的数据存储形式非常灵活，MongoDB不限制每个key对应的values的数目。比如有的文章没有评论，则它的值就是一个空集，完全没有问题；有的文章评论很多，也可以无限制地插入。更灵活的是，MongoDB不要求同一个集合(collection，相当于SQL的table)里面的不同document有相同的key，比如除了上述这种文档组织，有的文档所代表的文章可能没有likes这个项目，再比如有的文章可能有更多的项目，比如可能还有dislikes等等。这些不同的文档都可以灵活地存储在同一个集合下，而且查询起来也异常简单，因为都在一个文档里，不用进行各种跨文档查询。而这种MongoDB式的存储也方便了数据的维护，对于一篇博客文章来说，所有的相关数据都在这个document里面，不用去考虑一个数据操作需要involve多少个表格。 当然，除了上述的优点，MongoDB还有不少别的优势，比如MongoDB的数据是用JSON(Javascript Object Notation)存储的(就是上面的这种key-value的形式)，而几乎所有的web应用都是基于Javascript的。因此，存储的数据和应用的数据的格式是高度一致的，不需经过转换。更多的优点可以查看：[2]。 2. 关于这篇文章这个极简教程，或者说笔记，并不是一个覆盖MongoDB方方面面的教程。所谓极简的意思，就是只选取那些最重要、最常用的内容进行基于实例的介绍，从而让读者能够在最短的时间内快速上手，并且能顺利地进行后续的纵深的学习。 具体地说，这个教程的特点是： 不求全面，只求实用。只覆盖最核心的部分； 以大量例子为导向； 一边阅读一边动手操作的话，大约只需要2小时的时间； 阅读这篇文章不需要有特别的基础，但最好知道数据库的基本概念，如果本身熟悉SQL那就更好啦。 3. 安装与环境MongoDB可以在Windows、Linux、Mac OS X等主流平台运行，而且下载和安装非常简单，非常友好。这篇文档的例子采用MongoDB 2.6版本，均在OS X测试过，有充足的理由相信，在其它平台也能顺利运行。 Windows的安装和设置可以参考：http://www.w3cschool.cc/mongodb/mongodb-window-install.html； Linux的安装和设置可以参考：http://www.w3cschool.cc/mongodb/mongodb-linux-install.html； Mac OS X下的安装和设置： 1. 在https://www.mongodb.org/ 下载适合你的Mac的MongoDb; 2. 下载得到的文件是一个zip文件，解压，然后放到你想到的文件夹，比如/Users/Steven/MongoDB; 3. 创建一个你喜欢的文件夹来存储你的数据，比如/User/Steven/myData; 4. 打开Terminal，cd到2里面那个文件夹/Users/Steven/MongoDB，再cd bin; 5. 输入./mongod –dbpath /User/Steven/myData,等到出现类似“waiting for connections on port 27017”，说明MongoDB服务器已架设好，而数据将储存在myData里面； 6. 新打开一个Terminal, cd /Users/Steven/MongoDB/bin,然后运行./mongo;顺利的话它将出现一个interactive shell让你进行各种操作，而你的数据将储存在myData里 如果以上的各个步骤都运行顺利，就可以跳到下一节啦。 4. 创建集合和删除集合在上一节执行完步骤6后，你会看到命令行里显示：connecting to: test，这里的test是默认的数据库。这里我们可以新建一个数据库。在命令行里打入： 1use tutorial 这样就新建了一个叫做tutorial的数据库。你可以执行 1show databases 来显示当前的数据库。不过这时候由于我们的新数据库是空的，所以会显示类似这样的： 12admin (empty)local 0.078GB 我们试着往我们的数据库里添加一个集合(collection)，MongoDB里的集合和SQL里面的表格是类似的： 1db.createCollection(&apos;author&apos;) 顺利的话会显示： 1&#123; &quot;ok&quot; : 1 &#125; 表示创建成功。 你可以再回头执行： 1show databases 这时候我们的tutorial集合已经位列其中。你可以再执行 1show collections 可以看到创建的集合author也在其中。 我们暂时不需要author这个集合，所以我们可以通过执行： 1db.author.drop() 来将其删除。这时候你再执行show collections，就再也看不到我们的author了。 这一节要记住的点主要只有一个：集合(collection)类似于SQL的表格(table)，类似于Excel的一个个表格。 5. 插入想象一个精简版的“豆瓣电影”。我们需要创建一个数据库，来存储每部电影的信息，电影的信息包括： 电影名字 导演 主演(可能多个) 类型标签(可能多个) 上映日期 喜欢人数 不喜欢人数 用户评论(可能多个) 显然我们需要先创建一个叫电影的集合： 1db.createCollection(&apos;movie&apos;) 然后，我们就可以插入数据了： 12345678910111213141516171819202122232425db.movie.insert( &#123; title: &apos;Forrest Gump&apos;, directed_by: &apos;Robert Zemeckis&apos;, stars: [&apos;Tom Hanks&apos;, &apos;Robin Wright&apos;, &apos;Gary Sinise&apos;], tags: [&apos;drama&apos;, &apos;romance&apos;], debut: new Date(1994,7,6,0,0), likes: 864367, dislikes: 30127, comments: [ &#123; user:&apos;user1&apos;, message: &apos;My first comment&apos;, dateCreated: new Date(2013,11,10,2,35), like: 0 &#125;, &#123; user:&apos;user2&apos;, message: &apos;My first comment too!&apos;, dateCreated: new Date(2013,11,11,6,20), like: 0 &#125; ]&#125;) 请注意，这里插入数据之前，我们并不需要先声明movie这个集合里面有哪些项目。我们直接插入就可以了~这一点和SQL不一样，SQL必须先声明一个table里面有哪些列，而MongoDB不需要。 把上面的例子复制进命令行应该可以顺利运行，但我强烈建议你手动打一下，或者输入一部你自己喜欢的电影。insert操作有几点需要注意： 1. 不同key-value需要用逗号隔开，而key:value中间是用冒号； 2. 如果一个key有多个value，value要用[]。哪怕当前只有一个value，也加上[]以备后续的添加； 3. 整个“数据块”要用{}括起来； 如果你在insert之后看到WriteResult({ &quot;nInserted&quot; : 1 })，说明写入成功。 这个时候你可以用查询的方式来返回数据库中的数据： 1db.movie.find().pretty() 这里find()里面是空的，说明我们不做限制和筛选，类似于SQL没有WHERE语句一样。而pretty()输出的是经格式美化后的数据，你可以自己试试没有pretty()会怎么样。 仔细观察find()的结果，你会发现多了一个叫&#39;_id&#39;的东西，这是数据库自动创建的一个ID号，在同一个数据库里，每个文档的ID号都是不同的。 我们也可以同时输入多个数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960db.movie.insert([ &#123; title: &apos;Fight Club&apos;, directed_by: &apos;David Fincher&apos;, stars: [&apos;Brad Pitt&apos;, &apos;Edward Norton&apos;, &apos;Helena Bonham Carter&apos;], tags: &apos;drama&apos;, debut: new Date(1999,10,15,0,0), likes: 224360, dislikes: 40127, comments: [ &#123; user:&apos;user3&apos;, message: &apos;My first comment&apos;, dateCreated: new Date(2008,09,13,2,35), like: 0 &#125;, &#123; user:&apos;user2&apos;, message: &apos;My first comment too!&apos;, dateCreated: new Date(2003,10,11,6,20), like: 14 &#125;, &#123; user:&apos;user7&apos;, message: &apos;Good Movie!&apos;, dateCreated: new Date(2009,10,11,6,20), like: 2 &#125; ]&#125;,&#123; title: &apos;Seven&apos;, directed_by: &apos;David Fincher&apos;, stars: [&apos;Morgan Freeman&apos;, &apos;Brad Pitt&apos;, &apos;Kevin Spacey&apos;], tags: [&apos;drama&apos;,&apos;mystery&apos;,&apos;thiller&apos;], debut: new Date(1995,9,22,0,0), likes: 134370, dislikes: 1037, comments: [ &#123; user:&apos;user3&apos;, message: &apos;Love Kevin Spacey&apos;, dateCreated: new Date(2002,09,13,2,35), like: 0 &#125;, &#123; user:&apos;user2&apos;, message: &apos;Good works!&apos;, dateCreated: new Date(2013,10,21,6,20), like: 14 &#125;, &#123; user:&apos;user7&apos;, message: &apos;Good Movie!&apos;, dateCreated: new Date(2009,10,11,6,20), like: 2 &#125; ]&#125;]) 顺利的话会显示： 123456789BulkWriteResult(&#123; &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 2, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ] 表面我们成功地插入了两个数据。注意批量插入的格式是这样的：db.movie.insert([{ITEM1},{ITEM2}])。几部电影的外面需要用[]括起来。 请注意，虽然collection的插入不需要先声明，但表达相同意思的key，名字要一样，比如，如果我们在一个文档里用directed_by来表示导演，则在其它文档也要保持同样的名字(而不是director之类的)。不同的名字不是不可以，技术上完全可行，但会给查询和更新带来困难。 好了，到这里，我们就有了一个叫tutorial的数据库，里面有一个叫movie的集合，而movie里面有三个记录。接下来我们就可以对其进行查询了。 6. 查询在上一节我们已经接触到最简单的查询db.movie.find().pretty()。MongoDB支持各种各样的深度查询功能。先来一个最简单的例子，找出大卫芬奇(David Fincher)导演的所有电影： 1db.movie.find(&#123;&apos;directed_by&apos;:&apos;David Fincher&apos;&#125;).pretty() 将返回《搏击俱乐部》和《七宗罪》两部电影。这种搜索和SQL的WHERE语句是很相似的。 也可以设置多个条件。比如找出大卫芬奇导演的, 摩根弗里曼主演的电影： 1db.movie.find(&#123;&apos;directed_by&apos;:&apos;David Fincher&apos;, &apos;stars&apos;:&apos;Morgan Freeman&apos;&#125;).pretty() 这里两个条件之间，是AND的关系，只有同时满足两个条件的电影才会被输出。同理，可以设置多个的条件，不赘述。 条件之间也可以是或的关系，比如找出罗宾怀特或摩根弗里曼主演的电影： 1234567db.movie.find(&#123; $or: [ &#123;&apos;stars&apos;:&apos;Robin Wright&apos;&#125;, &#123;&apos;stars&apos;:&apos;Morgan Freeman&apos;&#125; ]&#125;).pretty() 注意这里面稍显复杂的各种括号。 还可以设置一个范围的搜索，比如找出50万人以上赞的电影： 1db.movie.find(&#123;&apos;likes&apos;:&#123;$gt:500000&#125;&#125;).pretty() 同样要注意略复杂的括号。注意，在这些查询里，key的单引号都是可选的，也就是说，上述语句也可以写成： 1db.movie.find(&#123;likes:&#123;$gt:500000&#125;&#125;).pretty() 类似地，少于二十万人赞的电影： 1db.movie.find(&#123;likes:&#123;$lt:200000&#125;&#125;).pretty() 类似的运算符还有：$lte:小于或等于；$gte:大于或等于；$ne:不等于。 注意，对于包含多个值的key，同样可以用find来查询。比如： 1db.movie.find(&#123;&apos;tags&apos;:&apos;romance&apos;&#125;) 将返回《阿甘正传》，虽然其标签既有romance，又有drama，但只要符合一个就可以了。 如果你确切地知道返回的结果只有一个，也可以用findOne: 1db.movie.findOne(&#123;&apos;title&apos;:&apos;Forrest Gump&apos;&#125;) 如果有多个结果，则会按磁盘存储顺序返回第一个。请注意，findOne()自带pretty模式，所以不能再加pretty()，将报错。 如果结果很多而你只想显示其中一部分，可以用limit()和skip()，前者指明输出的个数，后者指明从第二个结果开始数。比如： 1db.movie.find().limit(2).skip(1).pretty() 则跳过第一部，从第二部开始选取两部电影。 7. 局部查询第五节的时候我们讲了find的用法，但对于符合条件的条目，我们都是返回整个JSON文件的。这类似于SQL里面的SELECT *。有的时候，我们需要的，仅仅是部分数据，这个时候，find的局部查询的功能就派上用场了。先来看一个例子，返回tags为drama的电影的名字和首映日期。 1db.movie.find(&#123;&apos;tags&apos;:&apos;drama&apos;&#125;,&#123;&apos;debut&apos;:1,&apos;title&apos;:1&#125;).pretty() 数据库将返回： 123456789101112131415&#123; &quot;_id&quot; : ObjectId(&quot;549cfb42f685c085f1dd47d4&quot;), &quot;title&quot; : &quot;Forrest Gump&quot;, &quot;debut&quot; : ISODate(&quot;1994-08-05T16:00:00Z&quot;)&#125;&#123; &quot;_id&quot; : ObjectId(&quot;549cff96f685c085f1dd47d6&quot;), &quot;title&quot; : &quot;Fight Club&quot;, &quot;debut&quot; : ISODate(&quot;1999-11-14T16:00:00Z&quot;)&#125;&#123; &quot;_id&quot; : ObjectId(&quot;549cff96f685c085f1dd47d7&quot;), &quot;title&quot; : &quot;Seven&quot;, &quot;debut&quot; : ISODate(&quot;1995-10-21T16:00:00Z&quot;)&#125; 这里find的第二个参数是用来控制输出的，1表示要返回，而0则表示不返回。默认值是0，但_id是例外，因此如果你不想输出_id，需要显式地声明： 1db.movie.find(&#123;&apos;tags&apos;:&apos;drama&apos;&#125;,&#123;&apos;debut&apos;:1,&apos;title&apos;:1,&apos;_id&apos;:0&#125;).pretty() 8. 更新很多情况下你需要更新你的数据库，比如有人对某部电影点了个赞，那么你需要更新相应的数据库。比如有人对《七宗罪》点了个赞，而它本来的赞的个数是134370，那么你需要更新到134371。可以这样操作： 1db.movie.update(&#123;title:&apos;Seven&apos;&#125;, &#123;$set:&#123;likes:134371&#125;&#125;) 第一个大括号里表明要选取的对象，第二个表明要改动的数据。请注意上述的操作相当不现实，因为你首先要知道之前的数字是多少，然后加一，但通常你不读取数据库的话，是不会知道这个数(134370)的。MongoDB提供了一种简便的方法，可以对现有条目进行增量操作。假设又有人对《七宗罪》点了两个赞，则可以： 1db.movie.update(&#123;title:&apos;Seven&apos;&#125;, &#123;$inc:&#123;likes:2&#125;&#125;) 如果你查询的话，会发现点赞数变为134373了，这里用的是$inc。除了增量更新，MongoDB还提供了很多灵活的更新选项，具体可以看：http://docs.mongodb.org/manual/reference/operator/update-field/ 。 注意如果有多部符合要求的电影。则默认只会更新第一个。如果要多个同时更新，要设置{multi:true}，像下面这样： 1db.movie.update(&#123;&#125;, &#123;$inc:&#123;likes:10&#125;&#125;,&#123;multi:true&#125;) 所有电影的赞数都多了10. 注意，以上的更新操作会替换掉原来的值，所以如果你是想在原有的值得基础上增加一个值的话，则应该用$push，比如，为《七宗罪》添加一个popular的tags。 1db.movie.update(&#123;&apos;title&apos;:&apos;Seven&apos;&#125;, &#123;$push:&#123;&apos;tags&apos;:&apos;popular&apos;&#125;&#125;) 你会发现《七宗罪》现在有四个标签： 123456&quot;tags&quot; : [ &quot;drama&quot;, &quot;mystery&quot;, &quot;thiller&quot;, &quot;popular&quot;], 9. 删除删除的句法和find很相似，比如，要删除标签为romance的电影，则： 1db.movie.remove(&#123;&apos;tags&apos;:&apos;romance&apos;&#125;) 考虑到我们数据库条目异常稀少，就不建议你执行这条命令了~ 注意，上面的例子会删除所有标签包含romance的电影。如果你只想删除第一个，则 1db.movie.remove(&#123;&apos;tags&apos;:&apos;romance&apos;&#125;,1) 如果不加任何限制： 1db.movie.remove() 会删除movie这个集合下的所有文档。 10. 索引和排序为文档中的一些key加上索引(index)可以加快搜索速度。这一点不难理解，假如没有没有索引，我们要查找名字为Seven的电影，就必须在所有文档里逐个搜索。而如果对名字这个key加上索引值，则电影名这个字符串和数字建立了映射，这样在搜索的时候就会快很多。排序的时候也是如此，不赘述。MongoDB里面为某个key加上索引的方式很简单，比如我们要对导演这个key加索引，则可以： 1db.movie.ensureIndex(&#123;directed_by:1&#125;) 这里的1是升序索引，如果要降序索引，用-1。 MongoDB支持对输出进行排序，比如按名字排序： 1db.movie.find().sort(&#123;&apos;title&apos;:1&#125;).pretty() 同样地，1是升序，-1是降序。默认是1。 1db.movie.getIndexes() 将返回所有索引，包括其名字。 而 1db.movie.dropIndex(&apos;index_name&apos;) 将删除对应的索引。 11. 聚合MongoDB支持类似于SQL里面的GROUP BY操作。比如当有一张学生成绩的明细表时，我们可以找出每个分数段的学生各有多少。为了实现这个操作，我们需要稍加改动我们的数据库。执行以下三条命令： 123db.movie.update(&#123;title:&apos;Seven&apos;&#125;,&#123;$set:&#123;grade:1&#125;&#125;)db.movie.update(&#123;title:&apos;Forrest Gump&apos;&#125;,&#123;$set:&#123;grade:1&#125;&#125;)db.movie.update(&#123;title:&apos;Fight Club&apos;&#125;,&#123;$set:&#123;grade:2&#125;&#125;) 这几条是给每部电影加一个虚拟的分级，前两部是归类是一级，后一部是二级。 这里你也可以看到MongoDB的强大之处：可以动态地后续添加各种新项目。 我们先通过聚合来找出总共有几种级别。 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$grade&apos;&#125;&#125;]) 输出： 12&#123; &quot;_id&quot; : 2 &#125;&#123; &quot;_id&quot; : 1 &#125; 注意这里的2和1是指级别，而不是每个级别的电影数。这个例子看得清楚些： 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$directed_by&apos;&#125;&#125;]) 这里按照导演名字进行聚合。输出： 12&#123; &quot;_id&quot; : &quot;David Fincher&quot; &#125;&#123; &quot;_id&quot; : &quot;Robert Zemeckis&quot; &#125; 接着我们要找出，每个导演的电影数分别有多少： 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$directed_by&apos;,num_movie:&#123;$sum:1&#125;&#125;&#125;]) 将会输出： 12&#123; &quot;_id&quot; : &quot;David Fincher&quot;, &quot;num_movie&quot; : 2 &#125;&#123; &quot;_id&quot; : &quot;Robert Zemeckis&quot;, &quot;num_movie&quot; : 1 &#125; 注意$sum后面的1表示只是把电影数加起来，但我们也可以统计别的数据，比如两位导演谁的赞比较多： 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$directed_by&apos;,num_likes:&#123;$sum:&apos;$likes&apos;&#125;&#125;&#125;]) 输出： 12&#123; &quot;_id&quot; : &quot;David Fincher&quot;, &quot;num_likes&quot; : 358753 &#125;&#123; &quot;_id&quot; : &quot;Robert Zemeckis&quot;, &quot;num_likes&quot; : 864377 &#125; 注意这些数据都纯属虚构啊！ 除了$sum，还有其它一些操作。比如： 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$directed_by&apos;,num_movie:&#123;$avg:&apos;$likes&apos;&#125;&#125;&#125;]) 统计平均的赞。 1db.movie.aggregate([&#123;$group:&#123;_id:&apos;$directed_by&apos;,num_movie:&#123;$first:&apos;$likes&apos;&#125;&#125;&#125;]) 返回每个导演的电影中的第一部的赞数。 其它各种操作可以参考：http://docs.mongodb.org/manual/reference/operator/aggregation/group/ 。 12. All or Nothing?MongoDB支持单个文档内的原子化操作(atomic operation)，这是说，可以将多条关于同一个文档的指令放到一起，他们要么一起执行，要么都不执行。而不会执行到一半。有些场合需要确保多条执行一起顺次执行。比如一个场景：一个电商网站，用户查询某种商品的剩余数量，以及用户购买该种商品，这两个操作，必须放在一起执行。不然的话，假定我们先执行剩余数量的查询，这是假定为1，用户接着购买，但假如这两个操作之间还加入了其它操作，比如另一个用户抢先购买了，那么原先购买用户的购买的行为就会造成数据库的错误，因为实际上这种商品以及没有存货了。但因为查询剩余数量和购买不是在一个“原子化操作”之内，因此会发生这样的错误[2]。 MongoDB提供了findAndModify的方法来确保atomic operation。比如这样的： 123456db.movie.findAndModify( &#123; query:&#123;&apos;title&apos;:&apos;Forrest Gump&apos;&#125;, update:&#123;$inc:&#123;likes:10&#125;&#125; &#125; ) query是查找出匹配的文档，和find是一样的，而update则是更新likes这个项目。注意由于MongoDB只支持单个文档的atomic operation，因此如果query出多于一个文档，则只会对第一个文档进行操作。 findAndModify还支持更多的操作，具体见：http://docs.mongodb.org/manual/reference/command/findAndModify/。 13. 文本搜索除了前面介绍的各种深度查询功能，MongoDB还支持文本搜索。对文本搜索之前，我们需要先对要搜索的key建立一个text索引。假定我们要对标题进行文本搜索，我们可以先这样： 1db.movie.ensureIndex(&#123;title:&apos;text&apos;&#125;) 接着我们就可以对标题进行文本搜索了，比如，查找带有”Gump”的标题： 1db.movie.find(&#123;$text:&#123;$search:&quot;Gump&quot;&#125;&#125;).pretty() 注意text和search前面的$符号。 这个例子里，文本搜索作用不是非常明显。但假设我们要搜索的key是一个长长的文档，这种text search的方便性就显现出来了。MongoDB目前支持15种语言的文本搜索。 14. 正则表达式MongoDB还支持基于正则表达式的查询。如果不知道正则表达式是什么，可以参考Wikipedia。这里简单举几个例子。比如，查找标题以b结尾的电影信息： 1db.movie.find(&#123;title:&#123;$regex:&apos;.*b$&apos;&#125;&#125;).pretty() 也可以写成： 1db.movie.find(&#123;title:/.*b$/&#125;).pretty() 查找含有’Fight’标题的电影： 1db.movie.find(&#123;title:/Fight/&#125;).pretty() 注意以上匹配都是区分大小写的，如果你要让其不区分大小写，则可以： 1db.movie.find(&#123;title:&#123;$regex:&apos;fight.*b&apos;,$options:&apos;$i&apos;&#125;&#125;).pretty() $i是insensitive的意思。这样的话，即使是小写的fight，也能搜到了。 15. 后记至此，MongoDB的最基本的内容就介绍得差不多了。如果有什么遗漏的以后我会补上來。如果你一路看到底完全了这个入门教程，恭喜你，你一定是一个有毅力的人。 把这个文档过一遍，不会让你变成一个MongoDB的专家(如果会那就太奇怪了)。但如果它能或多或少减少你上手的时间，或者让你意识到“咦，MongoDB其实没那么复杂”，那么这个教程的目的也就达到啦。 这个文档是匆忙写就的，出错简直是一定的。如果您发现了任何错误或者有关于本文的任何建议，麻烦发邮件给我（stevenslxie at gmail.com）或者在GitHub上直接交流，不胜感激。 转载声明如果你喜欢这篇文章，可以随意转载。但请 标明原作者StevenSLXie; 标明原链接(https://github.com/StevenSLXie/Tutorials-for-Web-Developers/blob/master/MongoDB%20%E6%9E%81%E7%AE%80%E5%AE%9E%E8%B7%B5%E5%85%A5%E9%97%A8.md); 在可能的情况下请保持文本显示的美观。比如，请不要直接一键复制到博客之类，因为代码的显示效果可能非常糟糕; 请将这个转载声明包含进来；","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://nullpointer.pw/tags/mongodb/"}]},{"title":"Java生成sitemap网站地图","date":"2018-10-21T12:47:31.000Z","path":"Java生成sitemap网站地图.html","text":"sitemap 是什么？对应没有接触过网站 SEO 的同学可能不知道，这里引用一下百度站长的一段解释。 Sitemap（即站点地图）就是您网站上各网页的列表。创建并提交Sitemap有助于百度发现并了解您网站上的所有网页。您还可以使用Sitemap提供有关您网站的其他信息，如上次更新日期、Sitemap文件的更新频率等，供百度Spider参考 简单来说，sitemap 就是搜索引擎爬虫便于爬取到网站内的所有网页。 SEO之初之前特意通过 site 指令查询过公司同行业网站的收录情况，发现搜索引擎对于我司的网站收录数量真的不是差了一星半点-.-，原因嘛，就是一直没有专门的人做 SEO 优化工作。 前几个月我倒是生成了网站的一部分 sitemap 提交到神马搜索站长平台，但是明明是按照官方文档格式生成的，却一直提示格式不对，也不知道什么情况，也没有官方反馈渠道，就不了了之了。最近公司新招了 SEO 专员，于是便有了定时生成 sitemap 文件的需求。这种生成的文件一般都是对外直接可以访问的，可以通过 nginx 配置静态资源文件来完成。 需求探讨sitemap 的格式一般是 XML 格式的，第一反应就是可以使用 DOM4J 来完成，但是做 SEO 的同事说了，因为 sitemap 文件的限制，同一个 sitemap 文件最多有 5 万条 URL，超出 5 万条，就应该放入到下一个 sitemap 文件当中去。通过 DOM4J 来做的话，还需要判断条数感觉有点麻烦(懒)，就上 gayhub 搜了一下相关的轮子 generate sitemap，找到了现成的轮子 sitemapgen4j，项目的 README 中使用方法写的比较完善，这里不再多说。 为什么文档很完善的情况下，我要写这篇博客呢？(10月第一篇，凑数的(逃~) 上文提到了，如果超出了 5 万条需要写入另外一个 sitemap 当中，这个功能 sitemapgen4j 已经替我们实现了，无需担心。如果超出，生成的文件就像是这样的：1234- article1.xml- article2.xml- article3.xml- article4.xml 这样的话，在站长平台这里，如果新增了文件就要新增一条sitemap 网址记录，很麻烦。 好在，搜索引擎考虑到了这种问题，有相应的解决方法。 我们把每次生成的 sitemap 文件的地址，添加到一个 sitemap 索引文件当中去，这样，我们只需要向平台提交一个 sitemap 索引文件的地址即可。 不多说，代码见。 生成 sitemap 文件首先引入 sitemapgen4j 依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.github.dfabulich&lt;/groupId&gt; &lt;artifactId&gt;sitemapgen4j&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; 编写生成 sitemap 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void generateSitemap() &#123; String tempPath = \"/home/seo\"; //String tempPath = System.getProperty(\"java.io.tmpdir\") + File.separator + \"/seo/\"; File file = new File(tempPath); if (!file.exists()) &#123; file.mkdirs(); &#125; String domain = \"https://www.domain.com\"; try &#123; WebSitemapGenerator g1 = WebSitemapGenerator.builder(domain, file) .fileNamePrefix(\"article\").build(); Date date = new Date(); for (int i = 1; i &lt; 21; i++) &#123; WebSitemapUrl url = new WebSitemapUrl.Options(domain + \"/article/\" + i ).lastMod(date).build(); g1.addUrl(url); &#125; WebSitemapGenerator g2 = WebSitemapGenerator.builder(domain, file) .fileNamePrefix(\"issue\").build(); Date date2 = new Date(); for (int i = 1; i &lt; 21; i++) &#123; WebSitemapUrl url = new WebSitemapUrl.Options(domain + \"/issue/\" + i ).lastMod(date2).build(); g2.addUrl(url); &#125; List&lt;String&gt; fileNames = new ArrayList&lt;&gt;(); // 生成 sitemap 文件 List&lt;File&gt; articleFiles = g1.write(); articleFiles.forEach(e -&gt; fileNames.add(e.getName())); List&lt;File&gt; issueFiles = g2.write(); issueFiles.forEach(e -&gt; fileNames.add(e.getName())); // 构造 sitemap_index 生成器 W3CDateFormat dateFormat = new W3CDateFormat(W3CDateFormat.Pattern.DAY); SitemapIndexGenerator sitemapIndexGenerator = new SitemapIndexGenerator .Options(domain, new File(tempPath + \"sitemap_index.xml\")) .dateFormat(dateFormat) .autoValidate(true) .build(); fileNames.forEach(e -&gt; &#123; try &#123; // 组装 sitemap 文件 URL 地址 sitemapIndexGenerator.addUrl(domain + \"/\" + e); &#125; catch (MalformedURLException e1) &#123; e1.printStackTrace(); &#125; &#125;); // 生成 sitemap_index 文件 sitemapIndexGenerator.write(); &#125; catch (MalformedURLException e) &#123; e.printStackTrace(); &#125;&#125; 通过 Nginx 提供给外部访问文件已经生成了，就需要对外提供访问了，这里使用 nginx 来实现，如果用 apache 也是可以的。 上文生成的文件我放在了 /home/seo 目录下，文件有123456- seo - article1.xml - article2.xml - article3.xml - article4.xml - sitemap_index.xml 现在修改 nginx 配置文件，这里说明一下，因为 sitemap 生成一份就可以了，但是要通过域名能直接访问到，所以需要在 nginx 跳转一下到有 sitemap 文件的服务器。 存放 sitemap 文件的 nginx 配置如下123456789101112131415server &#123; listen 8888; server_name 111.112.113.114; charset utf8; access_log logs/access.log main; location / &#123; alias /home/seo/; sendfile on; autoindex on; autoindex_exact_size off; autoindex_localtime on; &#125;&#125; 根域名的 nginx 配置文件123456789101112131415server &#123; listen 80; server_name www.domain.com; charset utf8; access_log logs/access.log main; location /seo/ &#123; proxy_pass http://111.112.113.114:8888; &#125; location / &#123; proxy_pass http://homeServer; &#125;&#125; 注意: 第一个 location块 中的 proxy_pass 行尾有个 / www.domain.com/seo/sitemap_index.xml就会在 www.domain.com 这里直接反代到 111.112.113.114 这台服务器上了。 站长平台只用提交这一个 URL www.domain.com/seo/sitemap_index.xml 即可。 有疑问?欢迎来信，给我写信","tags":[{"name":"java生成sitemap","slug":"java生成sitemap","permalink":"http://nullpointer.pw/tags/java生成sitemap/"},{"name":"sitemapgen4j生成sitemap","slug":"sitemapgen4j生成sitemap","permalink":"http://nullpointer.pw/tags/sitemapgen4j生成sitemap/"},{"name":"seo","slug":"seo","permalink":"http://nullpointer.pw/tags/seo/"}]},{"title":"和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮","date":"2018-09-17T13:27:03.000Z","path":"和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮.html","text":"前面几篇文章详细讲解了 ElasticSearch 的搭建以及使用 SpringDataElasticSearch 来完成搜索查询，但是搜索一般都会有搜索关键字高亮的功能，今天我们把它给加上。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 环境依赖本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 SpringDataElasticSearch 的基本使用可以看我的上一篇文章 和我一起打造个简单搜索之SpringDataElasticSearch入门，本文就不再赘述。 高亮关键字实现前文查询是通过写一个接口来继承 ElasticsearchRepository 来实现的，但是如果要实现高亮，我们就不能这样做了，我们需要使用到 ElasticsearchTemplate来完成。 查看这个类的源码123public class ElasticsearchTemplate implements ElasticsearchOperations, ApplicationContextAware &#123; ...&#125; 可以看到，ElasticsearchTemplate 实现了接口 ApplicationContextAware，所以这个类是被 Spring 管理的，可以在类里面直接注入使用。 代码如下:123456789101112131415161718192021222324252627282930313233343536@Slf4j@Componentpublic class HighlightBookRepositoryTest extends EsSearchApplicationTests &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Resource private ExtResultMapper extResultMapper; @Test public void testHighlightQuery() &#123; BookQuery query = new BookQuery(); query.setQueryString(\"穿越\"); // 复合查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 以下为查询条件, 使用 must query 进行查询组合 MultiMatchQueryBuilder matchQuery = QueryBuilders.multiMatchQuery(query.getQueryString(), \"name\", \"intro\", \"author\"); boolQuery.must(matchQuery); PageRequest pageRequest = PageRequest.of(query.getPage() - 1, query.getSize()); NativeSearchQuery searchQuery = new NativeSearchQueryBuilder() .withQuery(boolQuery) .withHighlightFields( new HighlightBuilder.Field(\"name\").preTags(\"&lt;span style=\\\"color:red\\\"&gt;\").postTags(\"&lt;/span&gt;\"), new HighlightBuilder.Field(\"author\").preTags(\"&lt;span style=\\\"color:red\\\"&gt;\").postTags(\"&lt;/span&gt;\")) .withPageable(pageRequest) .build(); Page&lt;Book&gt; books = elasticsearchTemplate.queryForPage(searchQuery, Book.class, extResultMapper); books.forEach(e -&gt; log.info(\"&#123;&#125;\", e)); // &lt;span style=\"color:red\"&gt;穿越&lt;/span&gt;小道人 &#125;&#125; 注意这里 的1Page&lt;Book&gt; books = elasticsearchTemplate.queryForPage(searchQuery, Book.class, extResultMapper); 这里返回的是分页对象。查询方式和上文的差不多，只不过是是 Repository 变成了 ElasticsearchTemplate，操作方式也大同小异。 这里用到了 ExtResultMapper，请接着看下文。 自定义ResultMapperResultMapper 是用于将 ES 文档转换成 Java 对象的映射类，因为 SpringDataElasticSearch 默认的的映射类 DefaultResultMapper 不支持高亮，因此，我们需要自己定义一个 ResultMapper。 复制 DefaultResultMapper 类，重命名为 ExtResultMapper，对构造方法名称修改为正确的值。 新增一个方法，用于将高亮的内容赋值给需要转换的 Java 对象内。 在 mapResults 方法内调用这个方法。 注意：这个类可以直接拷贝到你的项目中直接使用！我写这么多，只是想说明为什么这个类是这样的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216import com.fasterxml.jackson.core.JsonEncoding;import com.fasterxml.jackson.core.JsonFactory;import com.fasterxml.jackson.core.JsonGenerator;import org.apache.commons.beanutils.PropertyUtils;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.get.MultiGetItemResponse;import org.elasticsearch.action.get.MultiGetResponse;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.common.text.Text;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHitField;import org.elasticsearch.search.fetch.subphase.highlight.HighlightField;import org.springframework.data.domain.Pageable;import org.springframework.data.elasticsearch.ElasticsearchException;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.ScriptedField;import org.springframework.data.elasticsearch.core.AbstractResultMapper;import org.springframework.data.elasticsearch.core.DefaultEntityMapper;import org.springframework.data.elasticsearch.core.EntityMapper;import org.springframework.data.elasticsearch.core.aggregation.AggregatedPage;import org.springframework.data.elasticsearch.core.aggregation.impl.AggregatedPageImpl;import org.springframework.data.elasticsearch.core.mapping.ElasticsearchPersistentEntity;import org.springframework.data.elasticsearch.core.mapping.ElasticsearchPersistentProperty;import org.springframework.data.mapping.context.MappingContext;import org.springframework.stereotype.Component;import org.springframework.util.Assert;import org.springframework.util.StringUtils;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.lang.reflect.InvocationTargetException;import java.nio.charset.Charset;import java.util.*;/** * 类名称：ExtResultMapper * 类描述：自定义结果映射类 * 创建人：WeJan * 创建时间：2018-09-13 20:47 */@Componentpublic class ExtResultMapper extends AbstractResultMapper &#123; private MappingContext&lt;? extends ElasticsearchPersistentEntity&lt;?&gt;, ElasticsearchPersistentProperty&gt; mappingContext; public ExtResultMapper() &#123; super(new DefaultEntityMapper()); &#125; public ExtResultMapper(MappingContext&lt;? extends ElasticsearchPersistentEntity&lt;?&gt;, ElasticsearchPersistentProperty&gt; mappingContext) &#123; super(new DefaultEntityMapper()); this.mappingContext = mappingContext; &#125; public ExtResultMapper(EntityMapper entityMapper) &#123; super(entityMapper); &#125; public ExtResultMapper( MappingContext&lt;? extends ElasticsearchPersistentEntity&lt;?&gt;, ElasticsearchPersistentProperty&gt; mappingContext, EntityMapper entityMapper) &#123; super(entityMapper); this.mappingContext = mappingContext; &#125; @Override public &lt;T&gt; AggregatedPage&lt;T&gt; mapResults(SearchResponse response, Class&lt;T&gt; clazz, Pageable pageable) &#123; long totalHits = response.getHits().totalHits(); List&lt;T&gt; results = new ArrayList&lt;&gt;(); for (SearchHit hit : response.getHits()) &#123; if (hit != null) &#123; T result = null; if (StringUtils.hasText(hit.sourceAsString())) &#123; result = mapEntity(hit.sourceAsString(), clazz); &#125; else &#123; result = mapEntity(hit.getFields().values(), clazz); &#125; setPersistentEntityId(result, hit.getId(), clazz); setPersistentEntityVersion(result, hit.getVersion(), clazz); populateScriptFields(result, hit); // 高亮查询 populateHighLightedFields(result, hit.getHighlightFields()); results.add(result); &#125; &#125; return new AggregatedPageImpl&lt;T&gt;(results, pageable, totalHits, response.getAggregations(), response.getScrollId()); &#125; private &lt;T&gt; void populateHighLightedFields(T result, Map&lt;String, HighlightField&gt; highlightFields) &#123; for (HighlightField field : highlightFields.values()) &#123; try &#123; PropertyUtils.setProperty(result, field.getName(), concat(field.fragments())); &#125; catch (InvocationTargetException | IllegalAccessException | NoSuchMethodException e) &#123; throw new ElasticsearchException(\"failed to set highlighted value for field: \" + field.getName() + \" with value: \" + Arrays.toString(field.getFragments()), e); &#125; &#125; &#125; private String concat(Text[] texts) &#123; StringBuffer sb = new StringBuffer(); for (Text text : texts) &#123; sb.append(text.toString()); &#125; return sb.toString(); &#125; private &lt;T&gt; void populateScriptFields(T result, SearchHit hit) &#123; if (hit.getFields() != null &amp;&amp; !hit.getFields().isEmpty() &amp;&amp; result != null) &#123; for (java.lang.reflect.Field field : result.getClass().getDeclaredFields()) &#123; ScriptedField scriptedField = field.getAnnotation(ScriptedField.class); if (scriptedField != null) &#123; String name = scriptedField.name().isEmpty() ? field.getName() : scriptedField.name(); SearchHitField searchHitField = hit.getFields().get(name); if (searchHitField != null) &#123; field.setAccessible(true); try &#123; field.set(result, searchHitField.getValue()); &#125; catch (IllegalArgumentException e) &#123; throw new ElasticsearchException(\"failed to set scripted field: \" + name + \" with value: \" + searchHitField.getValue(), e); &#125; catch (IllegalAccessException e) &#123; throw new ElasticsearchException(\"failed to access scripted field: \" + name, e); &#125; &#125; &#125; &#125; &#125; &#125; private &lt;T&gt; T mapEntity(Collection&lt;SearchHitField&gt; values, Class&lt;T&gt; clazz) &#123; return mapEntity(buildJSONFromFields(values), clazz); &#125; private String buildJSONFromFields(Collection&lt;SearchHitField&gt; values) &#123; JsonFactory nodeFactory = new JsonFactory(); try &#123; ByteArrayOutputStream stream = new ByteArrayOutputStream(); JsonGenerator generator = nodeFactory.createGenerator(stream, JsonEncoding.UTF8); generator.writeStartObject(); for (SearchHitField value : values) &#123; if (value.getValues().size() &gt; 1) &#123; generator.writeArrayFieldStart(value.getName()); for (Object val : value.getValues()) &#123; generator.writeObject(val); &#125; generator.writeEndArray(); &#125; else &#123; generator.writeObjectField(value.getName(), value.getValue()); &#125; &#125; generator.writeEndObject(); generator.flush(); return new String(stream.toByteArray(), Charset.forName(\"UTF-8\")); &#125; catch (IOException e) &#123; return null; &#125; &#125; @Override public &lt;T&gt; T mapResult(GetResponse response, Class&lt;T&gt; clazz) &#123; T result = mapEntity(response.getSourceAsString(), clazz); if (result != null) &#123; setPersistentEntityId(result, response.getId(), clazz); setPersistentEntityVersion(result, response.getVersion(), clazz); &#125; return result; &#125; @Override public &lt;T&gt; LinkedList&lt;T&gt; mapResults(MultiGetResponse responses, Class&lt;T&gt; clazz) &#123; LinkedList&lt;T&gt; list = new LinkedList&lt;&gt;(); for (MultiGetItemResponse response : responses.getResponses()) &#123; if (!response.isFailed() &amp;&amp; response.getResponse().isExists()) &#123; T result = mapEntity(response.getResponse().getSourceAsString(), clazz); setPersistentEntityId(result, response.getResponse().getId(), clazz); setPersistentEntityVersion(result, response.getResponse().getVersion(), clazz); list.add(result); &#125; &#125; return list; &#125; private &lt;T&gt; void setPersistentEntityId(T result, String id, Class&lt;T&gt; clazz) &#123; if (mappingContext != null &amp;&amp; clazz.isAnnotationPresent(Document.class)) &#123; ElasticsearchPersistentEntity&lt;?&gt; persistentEntity = mappingContext.getRequiredPersistentEntity(clazz); ElasticsearchPersistentProperty idProperty = persistentEntity.getIdProperty(); // Only deal with String because ES generated Ids are strings ! if (idProperty != null &amp;&amp; idProperty.getType().isAssignableFrom(String.class)) &#123; persistentEntity.getPropertyAccessor(result).setProperty(idProperty, id); &#125; &#125; &#125; private &lt;T&gt; void setPersistentEntityVersion(T result, long version, Class&lt;T&gt; clazz) &#123; if (mappingContext != null &amp;&amp; clazz.isAnnotationPresent(Document.class)) &#123; ElasticsearchPersistentEntity&lt;?&gt; persistentEntity = mappingContext.getPersistentEntity(clazz); ElasticsearchPersistentProperty versionProperty = persistentEntity.getVersionProperty(); // Only deal with Long because ES versions are longs ! if (versionProperty != null &amp;&amp; versionProperty.getType().isAssignableFrom(Long.class)) &#123; // check that a version was actually returned in the response, -1 would indicate that // a search didn't request the version ids in the response, which would be an issue Assert.isTrue(version != -1, \"Version in response is -1\"); persistentEntity.getPropertyAccessor(result).setProperty(versionProperty, version); &#125; &#125; &#125;&#125; 注意这里使用到了 PropertyUtils ，需要引入一个 Apache 的依赖。12345&lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils&lt;/artifactId&gt; &lt;version&gt;1.9.3&lt;/version&gt;&lt;/dependency&gt; 自定义 ResultMapper 写好之后，添加 @Component 注解，表示为 Spring 的一个组件，在类中进行注入使用即可。 最后本文示例项目地址：https://github.com/Mosiki/SpringDataElasticSearchQuickStartExample 有疑问?欢迎来信，给我写信","tags":[{"name":"SpringDataElasticSearch高亮","slug":"SpringDataElasticSearch高亮","permalink":"http://nullpointer.pw/tags/SpringDataElasticSearch高亮/"},{"name":"SpringDataElasticSearch关键词高亮","slug":"SpringDataElasticSearch关键词高亮","permalink":"http://nullpointer.pw/tags/SpringDataElasticSearch关键词高亮/"},{"name":"ElasticsearchTemplate使用","slug":"ElasticsearchTemplate使用","permalink":"http://nullpointer.pw/tags/ElasticsearchTemplate使用/"}]},{"title":"和我一起打造个简单搜索之SpringDataElasticSearch入门","date":"2018-09-13T13:51:17.000Z","path":"和我一起打造个简单搜索之SpringDataElasticSearch入门.html","text":"网上大多通过 java 操作 es 使用的都是 TransportClient，而介绍使用 SpringDataElasticSearch 的文章相对比较少，笔者也是摸索了许久，接下来本文介绍 SpringDataElasticSearch 的 api 使用，更加方便的进行查询。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 环境依赖本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 本文项目基于 SpringBoot 2.0.4.RELEASE 进行构建，首先引入 Spring Data ElasticSearch 的依赖。 注意：因为是 Spring Boot 项目，所以引入的依赖是 spring-boot-starter-data-elasticsearch，而不是直接引入 spring-data-elasticsearch。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; Spring Data ElasticSearch 与 ElasticSearch 有对应关系 spring data elasticsearch elasticsearch 3.1.x 6.2.2 3.0.x 5.5.0 2.1.x 2.4.0 2.0.x 2.2.0 1.3.x 1.5.2 而本文使用的 SpringBoot 2.0.4.RELEASE 自动依赖的 Spring Data ElasticSearch 版本是 3.0.9.RELEASE，对应的 elasticsearch 版本是 5.5.x，可知依赖是正确的版本。 创建 Document 类12345678910111213141516171819202122232425262728293031323334353637383940414243@Data@Document(indexName = \"novel\", type = \"book\", createIndex = false)public class Book implements Serializable &#123; private static final long serialVersionUID = 8504604495927552402L; /** * 需要添加 @Id 标识主键 */ @Id private Integer id; private Integer words; private String intro; private String name; private Integer sort; private Boolean vip; private Integer site; private String author; private Integer collection; private Integer click; private Integer popularity; private Integer goods; private Integer status; /** * 需要自定义时间格式化格式，否则会使用默认时间格式化 */ @JsonFormat (shape = JsonFormat.Shape.STRING, pattern =\"yyyy-MM-dd HH:mm:ss\") private Date updatetime;&#125; 这个 Document 类封装了索引的全部字段信息，注意字段名称要与索引类型的字段名称一致。 创建 Repository 接口123public interface BookRepository extends ElasticsearchRepository&lt;Book, Integer&gt; &#123;&#125; 用过 SpringDataJPA 的朋友都应该知道，ElasticsearchRepository 的两个泛型分别为 Documet 以及 Document 的主键类型。 创建测试类12345678910@Slf4j@SpringBootTest@RunWith(SpringRunner.class)public class BookRepositoryTest &#123; @Autowired private BookRepository bookRepository; &#125; 演示简单查询匹配查询(MatchQuery)进行模糊匹配查询，这里演示的是通过 name 这个字段进行查询123456@Testpublic void findBook() &#123; MatchQueryBuilder matchQuery = QueryBuilders.matchQuery(&quot;name&quot;, &quot;火爆娱乐天王&quot;); bookRepository.search(matchQuery) .forEach(e -&gt; log.info(&quot;作品信息：&#123;&#125;&quot;, e));&#125; 项查询(TermQuery)完全匹配查询，这里演示查询 id 为 2 的数据1234567@Testpublic void findBook() &#123; TermQueryBuilder termQuery = QueryBuilders.termQuery(\"id\", 2); bookRepository.search(termQuery) .forEach(e -&gt; log.info(\"作品信息：&#123;&#125;\", e));&#125; 范围查询(Range Query)范围查询，这里演示查询字数在 0-30w 之间的作品123456@Testpublic void findBook() &#123; RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(\"words\").gt(0).lt(300000); bookRepository.search(rangeQuery) .forEach(e -&gt; log.info(\"作品信息：&#123;&#125;\", e));&#125; 注意：如果对时间进行范围查询，注意不能传递 Date 对象或者 毫秒值，只能传递 yyyy-MM-dd HH:mm:ss 格式的字符串时间参数。 复合查询以上演示了几个基本查询，但是如果要实现多筛选条件的查询，就需要把多个基本查询进行组合，这里就用到了 bool 查询 现在来实现这个查询 BookQuery 参数封装使用 BookQuery 类封装查询参数12345678910111213141516171819202122232425262728293031@Datapublic class BookQuery &#123; private String queryString; private Integer page = 1; private Integer size = 20; private Integer wordsBegin; private Integer wordsEnd; private Integer sort; private Boolean vip; private Integer site; private Integer collection; private Integer click; private Integer popularity; private Integer goods; private Integer status; private Date updatetime;&#125; 查询测试1234567891011121314151617181920212223242526272829303132333435363738@Testpublic void findBook() &#123; BookQuery query = new BookQuery(); query.setQueryString(\"魔\"); query.setSite(2);// 1 是男生 2 是女生 query.setSort(29); // 29 是玄幻 query.setVip(true);// 查询 vip 作品 query.setWordsBegin(0); // 查询字数在 0-25w 之间的作品 query.setWordsEnd(500000); query.setPage(1);// 分页页码 query.setSize(10);// 每页显示数 // 复合查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 以下为查询条件, 使用 must query 进行查询组合 MultiMatchQueryBuilder matchQuery = QueryBuilders.multiMatchQuery(query.getQueryString(), \"name\", \"intro\", \"author\"); boolQuery.must(matchQuery); // 以下为过滤筛选条件，使用 filter 比使用 must query 性能要好 TermQueryBuilder siteQuery = QueryBuilders.termQuery(\"site\", query.getSite()); boolQuery.filter(siteQuery); TermQueryBuilder sortQuery = QueryBuilders.termQuery(\"sort\", query.getSort()); boolQuery.filter(sortQuery); TermQueryBuilder vipQuery = QueryBuilders.termQuery(\"vip\", query.getVip()); boolQuery.filter(vipQuery); RangeQueryBuilder wordsQuery = QueryBuilders.rangeQuery(\"words\").gt(query.getWordsBegin()).lt(query.getWordsEnd()); boolQuery.filter(wordsQuery); Sort sort = Sort.by(Sort.Direction.DESC, \"click\"); // 分页 同时根据 点击数 click 进行降序排列 PageRequest pageRequest = PageRequest.of(query.getPage() - 1, query.getSize(), sort); log.info(\"&#123;&#125;\", boolQuery); bookRepository.search(boolQuery, pageRequest) .forEach(e -&gt; log.info(\"作品信息：&#123;&#125;\", e));&#125; 查出结果：1232018-09-12 22:33:05.750 INFO 25896 --- [ main] i.g.mosiki.search.BookRepositoryTest : 作品信息：Book(id=7, words=345004, intro= 推荐《寻龙传》《魂摄天下》 作品属玄幻异界大陆风格！ 可惜频道不能更改只能在奇幻混！ 书友群：292... , name=灭魔成圣, sort=29, vip=true, site=2, author=等待潇湘诗社, collection=13, click=63263, popularity=2314, goods=5353, status=0, updatetime=Tue Sep 04 16:54:15 CST 2018)2018-09-12 22:33:05.751 INFO 25896 --- [ main] i.g.mosiki.search.BookRepositoryTest : 作品信息：Book(id=9, words=233000, intro= 一名地球的平凡的少年，因为一场游戏，获得死神的传承，从而穿越到另外一片陌生的大陆，从此开启了一段传奇的人生，九天星河，吾乃死神，掌控生死，判夺罪恶，我从没见过地狱，因为我的名字，便代表地狱，吾乃死神，吾名林天。（PS：单女主，爽文不虐心，主角以杀证道，杀该杀之人，不圣母，略腹黑） 各位书友要是觉得《带着死神去穿越》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！带着死神去穿越最新章节,带着死神去穿越无弹窗,带着死神去穿越全文阅读. , name=带着死神去穿越, sort=29, vip=true, site=2, author=梦侍, collection=6326, click=523, popularity=135, goods=34252, status=0, updatetime=Thu Dec 28 04:53:07 CST 2017)2018-09-12 22:33:05.751 INFO 25896 --- [ main] i.g.mosiki.search.BookRepositoryTest : 作品信息：Book(id=5, words=490000, intro= 富家子弟墨浞因为发现了村子中的秘密，在良心与亲情的折磨下，逃到了边境小城。因为一个香-艳而又恐怖的梦，墨浞经历了一些诡异的事，从而得知自己的前世与今生的使命。踏上藏地，历经磨难，克服了自己的心魔，战胜了... , name=伏魔, sort=29, vip=true, site=2, author=一叶style, collection=526, click=9, popularity=41516, goods=7687, status=0, updatetime=Thu Sep 06 04:54:05 CST 2018) 最后SpringDataElasticSearch 入门就到这里了，是不是很简单呢？ 本文示例项目地址：https://github.com/Mosiki/SpringDataElasticSearchQuickStartExample 有疑问?欢迎来信，给我写信 参考 https://blog.csdn.net/tianyaleixiaowu/article/details/77965257 https://tech.youzan.com/search-engine1/#43filteredquery","tags":[{"name":"SpringDataElasticSearch使用教程","slug":"SpringDataElasticSearch使用教程","permalink":"http://nullpointer.pw/tags/SpringDataElasticSearch使用教程/"},{"name":"SpringDataElasticSearch查询","slug":"SpringDataElasticSearch查询","permalink":"http://nullpointer.pw/tags/SpringDataElasticSearch查询/"},{"name":"SpringDataElasticSearch入门","slug":"SpringDataElasticSearch入门","permalink":"http://nullpointer.pw/tags/SpringDataElasticSearch入门/"}]},{"title":"和我一起打造个简单搜索之Logstash实时同步建立索引","date":"2018-09-13T13:48:38.000Z","path":"和我一起打造个简单搜索之Logstash实时同步建立索引.html","text":"用过 Solr 的朋友都知道，Solr 可以直接在配置文件中配置数据库连接从而完成索引的同步创建，但是 ElasticSearch 本身并不具备这样的功能，那如何建立索引呢？方法其实很多，可以使用 Java API 的方式建立索引，也可以通过 Logstash 的插件 logstash-input-jdbc 完成，今天来探讨下如何使用 logstash-input-jdbc 完成全量同步以及增量同步。 环境本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 默认你已经搭建好 es 的基础环境，如还未搭建好，请参考前文。接下来只讲解 logstash 的安装使用。本文使用最新版本的 logstash ，版本号为 6.4.0。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 数据准备 如图所示，如果要实现这个搜索，首先要创建相关的索引，筛选条件有男生/女生，还有分类，属性，字数，连载状态，品质等，排序条件有人气，时间，字数，收藏，推荐，点击。 这些数据正常都不会再同一张表当中，而是分布于不同的表中，但这些数据都与作品的 id 紧紧关联。本文为了方便演示，把这些数据都放在同一张表当中，如果在实际使用的过程当中，如果遇到多张表的情况，可以写 sql 进行联合查询，同样也可以建立索引，实现方式详见下文。 创建表结构12345678910111213141516171819CREATE TABLE `book` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `create_time` datetime(0) NULL DEFAULT NULL, `update_time` datetime(0) NULL DEFAULT NULL, `status` tinyint(11) NULL DEFAULT NULL, `name` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NULL DEFAULT NULL, `intro` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NULL, `icon` int(11) NULL DEFAULT NULL, `author` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NULL DEFAULT NULL, `words` int(11) NULL DEFAULT NULL, `collection` int(11) NULL DEFAULT NULL, `goods` int(11) NULL DEFAULT NULL, `click` int(11) NULL DEFAULT NULL, `site` tinyint(11) NULL DEFAULT NULL, `sort` tinyint(11) NULL DEFAULT NULL, `vip` tinyint(11) NULL DEFAULT NULL, `popularity` int(11) NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_bin ; 导入测试数据注：测试数据是通过爬虫在盗版网站抓的，个别数据胡乱填写的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (1, '2017-12-27 20:53:01', '2018-09-10 20:53:46', 0, '火爆娱乐天王', ' 有人说：“他是至高无上的音乐教皇！”有人说：“他是无人能及的影视国王！”还有人说：“他是神级作家、话剧大师、伟大的音乐家……”他使华夏元素狂暴冲击西方文化，却又能做出令西方人也不及的欧美音乐影视。他造就了无数的歌星影星，创作出无数的经典，建立了一个庞大的娱乐帝国。当所有人称颂他膜拜他的时候，只有他自己知道，他不过是一个意外来自异时空的文化使者。 ', 1, '茶与酒之歌', 230000, 4234, 42315, 523, 2, 9, 1, 12423);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (2, '2017-12-27 20:53:01', '2017-12-27 20:53:03', 0, '无限平行进化', ' 日复一日的枯燥生活让廖宇开始产生了难以抑制的厌倦。沉重庞大的经济负担使他的生活开始产生了变化。历史的缺失更是成为了他心中不散的谜团。某一天突如其来的陌生信息终于给他打开了一扇门。性格各异的X战警与复杂庞大的复仇者联盟会再度爆发怎样的战争。加勒比海上是否还会扬起第四艘传奇战舰的旗帜。铺天盖地的宇宙虫族与冰冷机械的人工智能谁才是最具有侵略性的文明。最终廖宇会如何选择，向左还是向右…… 各位书友要是觉得《无限平行进化》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！无限平行进化最新章节,无限平行进化无弹窗,无限平行进化全文阅读. ', 0, '雪色银狼CS', 555555, 450000, 13412, 555, 2, 9, 1, 342);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (3, '2017-12-27 20:53:02', '2018-09-15 20:53:55', 0, '逆天药神', ' 一个意外，让他在星空的彼岸两地相望。三人携手，杀出了这片大陆的四方威名。他，张凡，既然获得新生，就绝不再做弱者。既然要成为强者，就必须让这片星空颤抖！“既然是我的未来，那便只能由我主宰” ', 1, '枫叶', 340000, 213334, 3421, 42314, 2, 9, 1, 52315);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (4, '2017-12-27 20:53:02', '2018-08-16 20:53:59', 0, '锦绣风华之第一农家女1', ' 前世她是铁血手腕的帝国集团总裁，却被心爱之人设计，魂归天国。 再次睁眼，眼前的三间茅草屋，一对小瘦猴。 就算是她定力再强悍，在他们喊出那声“娘”的时候，还是让她差点没跳起来。 前世活到28还是清白如兰，一个穿越就让她勉强算是B的身材，孕育出一对儿女？ 当然，这还是其次，最重要的是她丫的居然是未婚生子，这在现代都遭人白眼的事情，那个天杀的能告诉她，这个身体的原主，是不是太牛叉了，居然没有被浸猪笼。 只是，当这对瘦的皮包骨的小包子在她跟前，肿着两对眼泡忍者泪花跑前跑后，就算是她再不想面对现实，也无法坐视不理。 既然让她再次重生，她势必要左手挥舞锄头，右手执笔算盘，带着一对可爱的包子，发家致富。 购田地，建豪宅，买下人，顾长工，一切都再朝着让她满意的方向前进，而那些眼红嫉妒之辈，完全都是她业余之时的消遣，根本就不是一个等级。 但是，常在河边走，哪有不湿鞋，这个道理她明白，却没想到那鞋子会湿的这么快，面对那个如同妖孽般，表面谦谦公子，风华无双，实则腹黑狡诈，怎么坑死她怎么来，还让她有火没处发。 精彩小剧场 当一对粉雕玉琢的包子被一个风华绝代的男人一手一个抱进来，君瑶真心的黑面了，泪奔了。 他们这两个没良心的到底明不明白，什么是引狼入室啊。 “多谢宁公子送他们回来，您看如今天色已晚，为了宁公子的名声，小妇人也不敢久留，宁公子请回吧。”她快步上前，一把一个把小包子从那个男人怀里蒿出来，快言快语的下了逐客令。 男人好看的眉毛微挑，随后一模情绪从眸中迅速划过，快的难以捕捉。 “无妨，我来陪陪这两个小家伙，君娘子不必多心。” “我…”她差点没被噎死，她有什么好多心的，就冲着他觊觎她的孩子，就不能让她留下。 只是她的话没有说完，就被男人贴面而来的俊彦吓得向后退去，而男人含笑的黑眸和清淡的话语，却让她差点怒火狂飙，“还是你想我把他们带回去？” 君瑶大惊，带回去？那是绝对不可能的，她的儿女，谁敢打主意，谁就没活路，话虽然很直白，却独独对他不管用。 ', 1, '席妖妖', 444902, 23, 5353, 21323, 2, 9, 1, 314);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (5, '2017-12-27 20:53:02', '2018-09-05 20:54:05', 0, '伏魔', ' 富家子弟墨浞因为发现了村子中的秘密，在良心与亲情的折磨下，逃到了边境小城。因为一个香-艳而又恐怖的梦，墨浞经历了一些诡异的事，从而得知自己的前世与今生的使命。踏上藏地，历经磨难，克服了自己的心魔，战胜了... ', 1, '一叶style', 490000, 526, 7687, 9, 2, 29, 1, 41516);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (6, '2017-12-27 20:53:02', '2018-09-10 00:54:10', 0, '强爱之独家拥有', ' 她是名贵千金，他是商界精英。 他们的订婚礼上，她惨遭背叛。 父亲气血攻心当场昏倒，再也没有醒来。 自己更被未婚夫床上的女人陷害入狱. 莫仲晖这个让她又爱又恨的男人几乎成了她这三年的梦靥。 三年的牢狱，再出来，她一无所有，而他已是江城翻手为云覆手为雨的神秘角色。 ', 1, '河清海晏七七', 980000, 513, 212342, 36236, 2, 29, 1, 1142314);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (7, '2017-12-27 20:53:02', '2018-09-04 08:54:15', 0, '灭魔成圣', ' 推荐《寻龙传》《魂摄天下》 作品属玄幻异界大陆风格！ 可惜频道不能更改只能在奇幻混！ 书友群：292... ', 1, '等待潇湘诗社', 345004, 13, 5353, 63263, 2, 29, 1, 2314);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (8, '2017-12-27 20:53:03', '2018-09-03 21:54:27', 0, '我的合租美女总裁', ' 和美女同居是一件很快乐很享受的事情吗？看看兵王曹小雷脸上蛋蛋的忧伤，大家还是醒醒吧！他好想好想告诉隔壁房间的刘薇薇，能不能别在客厅里黄瓜蘸蜂蜜吮来吮去？能不能晚上光溜溜精油按摩的时候拉好窗帘？我生理很正常，肠子很花花，爱好苍老湿，你丫别逼人太甚！ ', 1, '薯条', 890000, 5435, 3553, 543, 2, 29, 1, 412356);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (9, '2017-12-27 20:53:03', '2017-12-27 20:53:07', 0, '带着死神去穿越', ' 一名地球的平凡的少年，因为一场游戏，获得死神的传承，从而穿越到另外一片陌生的大陆，从此开启了一段传奇的人生，九天星河，吾乃死神，掌控生死，判夺罪恶，我从没见过地狱，因为我的名字，便代表地狱，吾乃死神，吾名林天。（PS：单女主，爽文不虐心，主角以杀证道，杀该杀之人，不圣母，略腹黑） 各位书友要是觉得《带着死神去穿越》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！带着死神去穿越最新章节,带着死神去穿越无弹窗,带着死神去穿越全文阅读. ', 0, '梦侍', 233000, 6326, 34252, 523, 2, 29, 1, 135);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (10, '2017-12-27 20:53:03', '2017-12-27 20:53:07', 0, '亡灵与芯片', ' 一位蝉联世界冠军数载的电子竞技玩家，在一次比赛中，天降黑色闪电，带着主角张远还有一枚地球上最先进的芯片、穿越到了剑与魔法的世界。故事没有yy，情节发展有迹可寻。没有种马，所有爱情起于点滴。更没有男主虎躯一震、从者云集，有的只是同生共死积累的友情，这、就是伙伴的意义。 各位书友要是觉得《亡灵与芯片》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！亡灵与芯片最新章节,亡灵与芯片无弹窗,亡灵与芯片全文阅读. ', 0, '冬冬东', 2559900, 5, 431, 42135, 2, 29, 1, 4124);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (11, '2017-12-27 20:53:03', '2017-12-27 20:53:08', 0, '穿越小道人', ' 小李飞刀里面:“飞刀厉害？我的符咒如何？”仙剑三:“这就是千里之外取人首级的剑啊！”红楼梦里面：“贾宝玉那块通灵宝玉应该是个灵宝级的，搞来搞来。”三国演义：“撒豆成兵，黄巾力士，《太平天书》，仙术啊，张角，你别跑！”西游记：“呼，终于到了仙界了。”三体：“二向箔，看我芥子须弥之术。”漫威:“oaa？什么东西？”小道人转身，一双眼看了过来：“呵！执笔者？” 各位书友要是觉得《穿越小道人》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！穿越小道人最新章节,穿越小道人无弹窗,穿越小道人全文阅读. ', 0, '楚骚', 455000, 342536, 23545, 53533254, 2, 29, 0, 2431);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (12, '2017-12-27 20:53:03', '2018-09-06 20:58:34', 0, '威武小娘子', ' 杜筱玖做了个噩梦，梦里娘被人害死，她也没得到好下场。一睁眼：成真了！既然有人要赶尽杀绝，就别怪她以牙还牙！谁知为娘报仇意外发现身世：哭，是捡的！好不容易有个对眼的少年：噗，是舅舅！既然她的人生是一个坑连着一个坑，不如凿成一条河，放开来造作吧！ 各位书友要是觉得《威武小娘子》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！威武小娘子最新章节,威武小娘子无弹窗,威武小娘子全文阅读. ', 1, '燕七爱吃鱼', 2389400, 325, 4325, 324, 2, 29, 0, 213);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (13, '2017-12-27 20:53:03', '2018-09-10 08:45:40', 0, '冰火破坏神', ' 当巨龙的时代渐渐终结，当术师们在星空下留下无数的财富和传说，一个个觉醒的少年，便开始踏上他们的征途。 ', 1, '无罪', 23452343, 325324, 52, 555, 2, 26, 0, 412);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (14, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '十年相恋不相知', ' 十年之前，她喜欢他，而他不知道。十年之后，他爱恋她，可她却不在。一段不掺杂任何其他利益的纯情爱恋之旅，究竟恋情的终点在何方？旅途的结局又将怎样？677577335本书群，各位喜欢本书的不喜欢本书的都可以进来，只要是喜欢看小说的都可以进来聊聊 各位书友要是觉得《十年相恋不相知》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！十年相恋不相知最新章节,十年相恋不相知无弹窗,十年相恋不相知全文阅读. ', 1, '青松不朽', 2355421, 4, 5534, 42314, 2, 26, 0, 231);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (15, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '终极教师', ' 朱雀中学校规： 第一条，禁止师生恋！ 第二条，禁止师生恋！ 第三条，禁止师生恋！ ———- 我坚定不移的认为穿白色半透明衬衣，里面的黑色Bra若隐若现的学生妹子才是最性感的女神！ ', 1, '柳下挥', 2143214, 523, 5555, 234, 2, 26, 0, 512);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (16, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '邪王绝宠蛇蝎嫡妃', ' “太子殿下，今日不是你毁婚，而是我毁婚。我花惊羽不嫁渣男，不嫁种猪男，要嫁便嫁宠我上天疼我入地，一生只娶一妻的男人。” 一道圣旨送到太子面前，太子脸色瞬间青黑一片，满堂宾客皆失色。 她，魍魉组织的金牌制毒师，一朝穿越成了任人欺凌的可怜太子妃，爹不疼妹不爱，还被百般欺凌。 骂我的，恶毒的骂回去。打我的，狠狠的打回去。算计我的，百倍千倍的算计回去。 可是这些渣男渣女一个个赶着送上门让她收拾，那就别怪她冷血无情，心狠手辣了，弄不死你也要弄残你。 他，帝国家喻户晓，闻之变色的嗜血邪王，从无一物入眼，却强势霸道的缠上她，眉眼灼灼，邪魅勾引：“我宠你上天，疼你入地，要不要？” 精彩对话一： “羽儿，这一次我真心诚意的向你提亲，娶你做我的东宫太子妃，未来燕云国的皇后娘娘。” “树不要皮必死无疑，人不要脸天下无敌，太子殿下的脸皮堪称一绝。” “羽儿，我错了，你原谅本宫一回吧。” “我看见你这张破脸就想弄残你，更别提上你这破船了，滚吧。” 精彩对话二： “北幽王殿下，你不是说你是断袖吗？”女子气急败坏的冷哼。 “不是我说的，是你说的。” “那你说什么难言之隐，说什么让我帮你治，原来是占我的便宜吃我的豆腐败。” “貌似本王才是比较吃亏的那一个，你吃我的豆腐比我吃你的豆腐多，”俊美霸气的男子一脸的委屈。 “意思是王爷你吃亏了？”女子微眯眼上，阴侧侧的开口。 “不吃亏，只要你对我负责就好。” “凭什么要我负责。” “本王负责也行，我宠你，我疼你，可好？” ', 1, '吴笑笑', 3859643, 245632, 4223, 43325, 2, 26, 0, 51251);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (17, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '神魔霸体', ' 拓拔野逆天重生、身藏神根、手握神符、拥有前世记忆，从小开始修炼最难修炼的炼体法决神魔九变。 神魔九变，每修炼成功一变，不但肉体力量暴增，还多一些神奇的本领。 为了修炼神魔九变，拓拔野走遍无数星宇世界，闯过无数险地绝域，大战神魔神兽鬼神。 最终，神魔九变大成，成就真正的神魔霸体，无敌诸天万界。 完本老书《六道仙尊》，五百二十万字，人品保证！放心收藏！ 老云新书群 229522850 ', 1, '云霆飞', 2456235, 3532, 5535, 5434, 2, 26, 0, 5154);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (18, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '妖娆召唤师', ' 名为“妖娆”的无良少女，被一枚诡异的黑暗魔珠附身，二次穿越异世。 她是被疯子带大的小乞丐，万年不遇的“废材”负灵根。 受到黑暗魔珠影响而带有被世人唾弃的暗之力的“邪恶魔女”！ 魔云宗要吞噬她：“你的力量化为我用可好？” 光明阵营要绞杀她：“魔女！哪里逃！” 强大战神要勒索她：“用这幻器，你不配！” 光与暗，皆不容她在世间存在！ 善与恶，谁才陪她直捣黄龙？ “想吞噬我的！把你们的宝藏都拿出来吧！” “想绞杀我的！用轮回鼎把你们都炼成活人药丹！” “想勒索我的！本姑娘要你看看什么才是强！盗！王！” 头带植系狗血牡丹皇！脚踏赤火烈焰炸毛鸡！手持杀人炼丹轮回鼎…… 强契异兽！暗驭魔仆！ 召唤！神兽级以下的喽啰通通让开！ “我以朱雀圣火，革新这个世界！” 且看女主以妩媚的眼波玩转世间高手，以腹黑强大的手段狂战天下群雄！ 本文一对一 爆笑+女强+腹黑+成长 小恶搞，真玄幻~ 此文从无断更记录，情节精彩，欢迎各路道友热情跳坑~ ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ ————人物打油诗一首———— ◆她有一个无聊的时候就会点起“灭魔神焰”烧柴玩的疯爹爹：“妖妖，我饿，我要吃大鸡腿！” ◆她有一只人见人不爱，花见花不开的植系战兽：“我是丑八怪，嗯，妖妖说过我是丑八怪，我的眼，只比星星璀璨那么一点点，我的脸，只比先天大帝帅那么一点点，我很自卑，请不要跟我讲话。” ◆她有一个钱比命重要，有钱就是娘的少年大管家，如果问他要钱还是要命，他会直接四仰八叉倒在地上：“大爷，奴家一夜，只要一个金币。” ◆她有一个骚包的红发追求者：“来的美女们，快留下你们的‘收藏’，不然本公子滚地半月，绝不出关！” O（∩＿∩）O~ ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ ----------公告板---------- ◆本文每日更新时间◆ 如无特别通知，每日上午九点左右 5000+ O（∩＿∩）O~ ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ 推荐玄幻好文《盛世邪凰》 BY：月清树影 她的倾情付出，终抵不过一句功高盖主。 亲友的倒戈，爱人的背叛，让她心如死灰。 唯有他依旧站在她身边，拥她入怀。 “以吾之心，予汝重生，转世之魂，生生相伴，不离不弃。契！” 浴火重生，再次睁眼，她已入魔！ 且看她如何在这荆棘中破开重重迷雾，凭借着自己的努力一步步走向云端，将曾经的耻辱一同踩在脚下！ 再世为人，她——注定光芒万丈！ ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ －－－－人物认养小店－－－ 【疯 子】 由 羽毛 认养 【妖 娆】 由 公子陌凌 认养 【翦 羽】 由 月清树影 认养 【白 夜 一】 由 刑冷墨 认养 【瓦 伦】 由 瑕恋 认养 【龙 觉】 由 莲心紫璃 认养 【丑 丑】 由 吾依糯 认养 【元 方】 由 小残 认养 【牧野寒江】 由 兔兔 认养 【纳 多 多】 由 妍儿 认养 【麒 麟 王】 由 954637819 认养 【烈焰风鹰】 由 夏轩月 认养 【拔 莉 儿】 由 云影悠然 认养 【青 君】 由 忘情紫雪 认养 【泥 巴 团】 由 媚妆 认养 【赤 火】 由 媚妆 认养 【二 毛】 由 尤兮 认养 【海涅宗师】 由 ysfengy 认养 【姬 天 白】 由xanxus1896 认养 【 炎 】 由 夏轩月 认养 【百 里 尘】 由 小可爱 认养 【战 虎】 由 蓝 认养 【灭 云 飞】 由 月依浅雪 认养 【血 十 三】 由 由夜 认养 【六 灵 珠】 由 kanshu001 认养 【囡 囡】 由1003695331认养 ◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆◆ －－－－好文推荐－－－ 我的旧文 《纵紫》 刑冷墨 《异火焚天》 月清树影 《盛世邪凰》 兔梓 《脱线黑公主》 仙魅 《妖仙魅世》 墨墨 《哥哥是只妖》 即墨血夜 《死神成长史》 ', 1, '翦羽', 1333413, 34, 6543, 678898, 2, 26, 0, 651223);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (19, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '永夜君王', ' 千夜自困苦中崛起，在背叛中坠落。自此一个人，一把枪，行在永夜与黎明之间，却走出一段传奇。若永夜注定是他的命运，那他也要成为主宰的王。 各位书友要是觉得《永夜君王》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！永夜君王最新章节,永夜君王无弹窗,永夜君王全文阅读. ', 1, '烟雨江南', 235212, 6342, 2343542, 6234, 2, 26, 0, 143212);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (20, '2017-12-27 20:53:04', '2018-09-14 20:54:49', 0, '雪中悍刀行', ' 有个白狐儿脸，佩双刀绣冬春雷，要做那天下第一。湖底有白发老魁爱吃荤。缺门牙老仆背剑匣。山上有个骑青牛的年轻师叔祖，不敢下山。有个骑熊猫扛向日葵不太冷的少女杀手。 这个江湖，高人出行要注重出尘装扮，女侠行走江湖要注意培养人气，宗派要跟庙堂打好关系。 而主角，则潇洒带刀，把江湖捅了一个通透。 各位书友要是觉得《雪中悍刀行》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！雪中悍刀行最新章节,雪中悍刀行无弹窗,雪中悍刀行全文阅读. ', 1, '烽火戏诸侯', 35525, 353, 5345, 624, 2, 26, 0, 2314);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (21, '2017-12-27 20:53:04', '2017-12-27 20:53:05', 0, '地球灾变', ' 【2017科幻征文】参赛作品 各位书友要是觉得《地球灾变》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！地球灾变最新章节,地球灾变无弹窗,地球灾变全文阅读. ', 0, '石慌', 25254, 46236, 335, 5234, 2, 26, 0, 412);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (22, '2017-12-27 20:53:05', '2018-09-14 20:54:49', 0, '求生欲', ' 这个世界，人并不是唯一的智慧生物，还有这比人更高级的生物，他们看待人的态度如何？老公，我要你杀了所有想伤害我的人。“她的野心太大，不是帮她，而是害她。”“她的一切，我负责。” 各位书友要是觉得《求生欲》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！求生欲最新章节,求生欲无弹窗,求生欲全文阅读. ', 1, '华尔街扛把子', 56135, 6353, 5666, 2345, 2, 26, 0, 51243);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (23, '2017-12-27 20:53:05', '2017-12-27 20:53:06', 0, '都市无限取钱系统', ' 自从得到无限取钱系统之后，叶凡的人生轨迹从此改变…… 每天早上醒来，不用担心没钱花，而应该担心这么多钱，要怎么花…… 只要是任何钱能够解决的问题，都不是问题，然而为了得到这些钱，不得不按照系统提示，完成一系列的任务。 任务失败的惩罚，将会是身体消失十分之一！ 各位书友要是觉得《都市无限取钱系统》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！都市无限取钱系统最新章节,都市无限取钱系统无弹窗,都市无限取钱系统全文阅读. ', 0, '飞天入地', 651341, 434, 5334, 234, 2, 26, 0, 215);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (24, '2017-12-27 20:53:05', '2017-12-27 20:53:07', 0, '荒岛饥荒', ' 威珥穿越到了一片海滩上，除了脑海中的饥荒制作系统，身上别无长物。祥和的岛屿上有着潜藏黑暗和秘密，为了生存，威珥不得不卷入其中。威珥开始寻找回家的路。致命的黑暗，神秘的阴影，圣洁的星灵。 威珥会回去吗？ 新书推荐：、白银霸主 、、大王饶命 、 各位书友要是觉得《荒岛饥荒》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！荒岛饥荒最新章节,荒岛饥荒无弹窗,荒岛饥荒全文阅读. ', 0, '凝心之火', 521333, 53534, 2436, 2345, 1, 26, 0, 415126);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (25, '2017-12-27 20:53:05', '2018-09-14 20:54:49', 0, '官道之步步高升', ' 青原市国资委普通科员楚天舒，因撞破办公室主任的好事而面临被发配的厄运，随后却经历了一系列匪夷所思的奇... ', 1, '北岸', 551612, 234632, 5243, 3245, 1, 26, 0, 124);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (26, '2017-12-27 20:53:05', '2018-09-14 20:54:49', 0, '傲神刀尊', ' 一颗桀骜的心，一把不羁的刀，一个傲视仙神的不朽传奇。你是谁？别问我是谁，请与我一战！——————QQ群：51765913 ', 1, '项华', 351314, 435, 45, 2534, 1, 26, 0, 6143);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (27, '2017-12-27 20:53:05', '2018-09-14 20:54:49', 0, '侯门医女,庶手驭夫', ' ', 1, '沧海明珠', 251234, 243544, 3425, 35235, 1, 26, 0, 421342314);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (28, '2017-12-27 20:53:06', '2017-12-27 20:53:13', 0, '掠夺诸天万界', ' 龙玄拥有掠夺万界的玉佩，以诸天万界的资源成无敌大主宰。武侠世界:天龙，笑傲，射雕，神雕，风云……仙侠世界:仙剑，花千骨，诛仙……动漫世界:火影，超神学院……小说世界:完美世界，斗破，遮天，斗罗……电影世界:西游降魔世界，僵尸先生，大话西游……神话世界:西游记，封神榜，洪荒…… 各位书友要是觉得《掠夺诸天万界》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！掠夺诸天万界最新章节,掠夺诸天万界无弹窗,掠夺诸天万界全文阅读. ', 0, '我原非凡', 521222, 24325, 2345, 666, 1, 26, 0, 411235);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (29, '2017-12-27 20:53:06', '2017-12-27 20:53:07', 0, '火影之最强虫师', ' 激萌的萝莉，热血的少年，为打破次元壁一往无前！ 各位书友要是觉得《火影之最强虫师》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！火影之最强虫师最新章节,火影之最强虫师无弹窗,火影之最强虫师全文阅读. ', 0, '来一刀', 544111, 325, 3434525, 623534, 1, 25, 0, 51244);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (30, '2017-12-27 20:53:07', '2018-09-14 20:54:49', 0, '陆战兵王', ' 少年从新兵蛋子一步步成长，磨练成一把隐去的刀锋，时刻为着梦想而奋斗，经历无尽的艰难困阻，最终了为真相而战斗！！ 我的书友交流群：241911158，我的QQ：807457338 ', 1, '可可比尔', 355555, 45346, 543, 5234, 1, 25, 0, 51261);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (31, '2017-12-27 20:53:07', '2017-12-27 20:53:12', 0, '变身之把反派养歪了肿么破', ' 激萌的萝莉，热血的少年，为打破次元壁一往无前！ 各位书友要是觉得《变身之把反派养歪了肿么破》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！变身之把反派养歪了肿么破最新章节,变身之把反派养歪了肿么破无弹窗,变身之把反派养歪了肿么破全文阅读. ', 0, '呆萌小总', 222235, 364326, 25342, 632, 1, 25, 0, 521341);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (32, '2017-12-27 20:53:07', '2018-09-14 20:54:49', 0, '唯愿情深不负婚', ' 渣男和小三联手设计，让她被扣上不干净的骂名，婆婆暴打，还逼着她净身出户。她没了家，也失去了腹中刚成型的孩子。走投无路，为了复仇，只能选择爬上渣男对手的床。他是天之骄子，她本以为镜花水月后再无交集，却.. 各位书友要是觉得《唯愿情深不负婚》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！唯愿情深不负婚最新章节,唯愿情深不负婚无弹窗,唯愿情深不负婚全文阅读. ', 1, '旧城雪', 5123566, 534, 234562, 636, 1, 25, 0, 42156);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (33, '2017-12-27 20:53:07', '2017-12-27 20:53:08', 0, '我的绝色美女特工老婆', ' 【火爆新书】地下世界的王者神秘归隐，就在各界大佬都松了一口气的时候，殊不知，远在佣兵禁地的华夏，掀起了一场前所未有的波澜！ 各位书友要是觉得《我的绝色美女特工老婆》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！我的绝色美女特工老婆最新章节,我的绝色美女特工老婆无弹窗,我的绝色美女特工老婆全文阅读. ', 0, '稣牧鱼', 351235, 2435, 45325, 5353, 1, 25, 0, 42);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (34, '2017-12-27 20:53:07', '2017-12-27 20:53:10', 0, '北雄', ' 本书简介： 大业六年，强盛的大隋迎来了转折点。 这一年，隋帝杨广开始准备征伐高丽，顺势拉开了隋末战乱的序幕。 接下来的几年间，天下板荡，群雄并起。 十八路反王，六十四路烟尘，草莽豪杰，门阀世家，纷纷粉墨登场，逐鹿天下。 北方突厥汗国，雄踞漠北，虎视眈眈。 内忧外患之下，一个强大的帝国，最终轰然崩塌。 这是个最具传奇色彩的时代，也同样是中原大地最为混乱黑暗的时节。 就在这样一个时候，一个来历奇异的边塞少年，带着草原的风寒，和一股满不在乎的劲头，一头扎进了这乱世漩涡之中。 各位书友要是觉得《北雄》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！北雄最新章节,北雄无弹窗,北雄全文阅读. ', 0, '河边草', 5235132, 523, 3425, 35, 1, 25, 0, 143165);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (35, '2017-12-27 20:53:07', '2017-12-27 20:53:10', 0, '异世界的战斗奶妈', ' 曾经超人气网络游戏“异世界，”曹玲敏有三个角色创下了传说记录。狂战士红昭愿，打出了“异世界”最强一击的爆发伤害；武圣朝朝暮暮，连续三次赛季拿下竞技场SOLO赛冠军；舰炮掌控者红绳结发，带领公会部下拿下七次世界级BOSS首杀；在其游戏终止营运的那一天，曹玲敏想起自己唯独没有玩过奶妈，结果等到营运时间结束，角色却无法退出游戏，拥有童颜贫乳外表的曹玲敏展开了前所未有的第四次传说…… 各位书友要是觉得《异世界的战斗奶妈》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！异世界的战斗奶妈最新章节,异世界的战斗奶妈无弹窗,异世界的战斗奶妈全文阅读. ', 0, '夏伊水心', 5661233, 26, 35, 3462, 1, 24, 0, 4112);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (36, '2017-12-27 20:53:07', '2018-09-14 20:54:49', 0, '武极阴阳', ' 天地始分阴阳，亘古平衡。有人企图以外力主宰五行大陆，颠覆世界！一个身具花瓶体质的少年，无形中成了对抗强大外力的主要人选。十二岁起步，能否担起重任？莫名魂力、五行之子、红颜知己、阴阳互修……这里是武动乾坤的精彩世界！P:新人新书，更新稳定。书友群249540907。请多支持！ ', 1, '楚松源', 455432, 345, 3, 23634, 1, 24, 1, 4145);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (37, '2017-12-27 20:53:08', '2017-12-27 20:53:09', 0, '苍灵主宰', ' 万灵世界，天地无垠，宗族千万，强人辈出！苍穹之下，表面群雄割据，内里暗流涌动，只待大浪淘沙，伺机乘风而上……平凡小子韩烨，习神秘功法横空出世，踏复仇之路，入修炼征途，掀起不可阻挡之洪流！自此，山河日月变…… 各位书友要是觉得《苍灵主宰》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！苍灵主宰最新章节,苍灵主宰无弹窗,苍灵主宰全文阅读. ', 0, '人生好梦', 561355, 6666, 62, 54325, 1, 24, 1, 2143);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (38, '2017-12-27 20:53:08', '2017-12-27 20:53:11', 0, '定位寻宝系统', ' 别人用聊天软件泡妞把妹子，轻松一下附近人就是美女如云...可是我的附近人里，没有什么美女直播...搜索一下附近人，精确定位出来的...不是灵丹妙药，就是古董文物...上到天材地宝，下到补肾壮阳...都只因脑海之中有着一个系统，可以看到别人看不到的搜索信息...别说我身上带着BUG，我真的就是想泡妹子而已...设定好自己的人生，走出一个不一样的人生巅峰... 各位书友要是觉得《定位寻宝系统》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！定位寻宝系统最新章节,定位寻宝系统无弹窗,定位寻宝系统全文阅读. ', 0, '沐雪沉风', 123344, 5325, 266, 3425, 1, 24, 1, 12342);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (39, '2017-12-27 20:53:08', '2018-09-14 20:54:49', 0, '不败战神', ' 少年，就是要在阳光下挥洒汗水！ 少年，就是要不断战斗，然后胜利！ 无尽天路之旅，无尽征途，见证少年的热血和传奇！ 心怀野望，烈血如燃！ 永远少年，不败战神！ 各位书友要是觉得《不败战神》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！ ', 1, '方想', 54633, 2534, 3262, 63263, 1, 24, 1, 2521);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (40, '2017-12-27 20:53:08', '2018-09-14 20:54:49', 0, '至尊神帝', ' 至尊神帝的简介：【重磅推荐低调哥完本小说《至尊龙帝》与《逍遥狂少》】一个从小被人嘲笑为废物的少年，在一次危险的狩猎中，遭人打断四肢，石沉深潭，不料却因祸得福，收了一名上古仙人的残魂为仆。在仙仆的激励下，他凭借强悍的意志力与不懈的努力，在庄中比武大会上一鸣惊人，彻底告别了废物的称号，之后以武入道，冲破了体内堵塞的经脉，拥有了五行元力的超强修炼天赋。从此，叶天决心倾毕生之力，逆天改命，去追寻修炼的极致境界。一路杀仙弑神，彪悍的修炼之路无需解释。 ', 1, '低调哥', 432513, 4532, 32234, 3254, 1, 22, 1, 51);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (41, '2017-12-27 20:53:08', '2018-09-14 20:54:49', 0, '紫阳', ' 紫阳 ', 1, '风御九秋', 5513243, 276236, 32445, 6666, 1, 22, 1, 245);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (42, '2017-12-27 20:53:08', '2017-12-27 20:53:10', 0, '刘备的日常', ' 【历史新纪元脑洞风暴征文】参赛作品 各位书友要是觉得《刘备的日常》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！刘备的日常最新章节,刘备的日常无弹窗,刘备的日常全文阅读. ', 0, '熏香如风', 521111, 6234, 264534, 3425, 1, 22, 1, 6124);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (43, '2017-12-27 20:53:08', '2017-12-27 20:53:12', 0, '狂武仙皇', ' 我为蝼蚁，世人可欺，世人为蝼蚁，我可欺世人。一息上存，战斗不止。即便是死，也要死的惊天动地。 各位书友要是觉得《狂武仙皇》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！狂武仙皇最新章节,狂武仙皇无弹窗,狂武仙皇全文阅读. ', 0, '秋风若水', 356623, 626, 62, 23466, 1, 22, 1, 5124);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (44, '2017-12-27 20:53:08', '2018-09-14 20:54:49', 0, '新刺客列传', ' 躲在阴暗的角落，冷静思考，仔细观察周围的一切动向，更要小心同行来抢生意。某一刻，目标出现，迎面微笑走过。刺出一剑！ 各位书友要是觉得《新刺客列传》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！新刺客列传最新章节,新刺客列传无弹窗,新刺客列传全文阅读. ', 1, '世尘风', 351334, 5234, 2345, 3425, 1, 22, 1, 42315);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (45, '2017-12-27 20:53:08', '2018-09-14 20:54:49', 0, '不朽神瞳', ' 这是一个神秘的世界，这里有最强的武道，最神秘的瞳术！八世历，九为尊！一代太古大能，历经八世磨砺而归，他想不明白，这一世为什么那么多法宝找他认主，为什么会有那么多美女来给他暖床，但是他知道，他将拥美并踩着敌人的尸体，走上至尊之路！萧寻：“这一世，我要让所有人都在我的脚下颤抖！绝色美女们，来陪哥哥快乐双修。” ', 1, '风舞天下', 566666, 525, 25342, 45326, 1, 22, 1, 51261);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (46, '2017-12-27 20:53:09', '2017-12-27 20:53:09', 0, '花都小仙医', ' 小仙医携造化宝塔，纵横都市，宝塔在手，美女我有！赚最多的钱，踩最牛的人，娶最美的老婆，专治各种不服。 各位书友要是觉得《花都小仙医》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！花都小仙医最新章节,花都小仙医无弹窗,花都小仙医全文阅读. ', 0, '十三刀', 542135, 2642, 3245, 26346, 1, 22, 1, 2143);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (47, '2017-12-27 20:53:09', '2018-09-14 20:54:49', 0, '女神总裁爱上我', ' 身怀异能却甘于平淡的酒店小保安，与美若天仙却冷若冰霜的总裁有了交集，她主动求婚他却说不，领证当晚陪伴他的居然是另个女人。从此，各色美女投怀送抱，要命的是每个女人都带着一堆甩不掉的麻烦…… ', 1, '愤怒小鸟', 111111, 62624, 52352, 6234, 1, 22, 1, 23415);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (48, '2017-12-27 20:53:09', '2017-12-27 20:53:18', 0, '萌宝小神龙：妖皇盛宠不良妃', ' 震惊！绝美少女棺材产子，儿子居然是个头上有犄角的妖怪。凰清妤捶地大哭。奇怪！为什么她都有孩子了，还有男人纠缠不休啊？凰清妤哭笑不得。某天，妖孽男子将她咚在树上，“撩完就想跑，你要对我负责！”她勾唇一笑，将他反压，“抱歉，我更喜欢在上面。”作为一代医毒双绝的杀手女王，说她是废物？放屁，我来告诉你什么是绝世天才！只是万万没想到惹上了一个比她更绝世更妖孽的男人，从此你追我逃，你杀人我递刀，一起称（huo）霸（hai）天下。 各位书友要是觉得《萌宝小神龙：妖皇盛宠不良妃》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！萌宝小神龙：妖皇盛宠不良妃最新章节,萌宝小神龙：妖皇盛宠不良妃无弹窗,萌宝小神龙：妖皇盛宠不良妃全文阅读. ', 0, '姒糖', 353251, 252, 24, 26345, 1, 22, 1, 4126);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (49, '2017-12-27 20:53:09', '2018-09-14 20:54:49', 0, 'X处首席特工皇妃', ' ', 1, '逐云之巅', 3521434, 262345, 2543, 6236, 1, 22, 1, 416213);INSERT INTO `book`(`id`, `create_time`, `update_time`, `status`, `name`, `intro`, `icon`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity`) VALUES (50, '2017-12-27 20:53:09', '2017-12-27 20:53:10', 0, '九转圣灵决', ' 五百年前，他心爱的女子，竟爬上了自己同父异母哥哥的床。毒酒穿肠，一代天骄，就此陨落。五百年后，他重生归来。却发现毒害他的狗男女。已统领七海八州。号称“凤后”“龙帝”威震八方。当龙吟剑出，雷霆破空，震破苍穹，山河破碎，星辰陨落。“待我重修30年，必将踏破星河，血洗金銮殿！”……… 各位书友要是觉得《九转圣灵决》还不错的话请不要忘记向您QQ群和微博里的朋友推荐哦！九转圣灵决最新章节,九转圣灵决无弹窗,九转圣灵决全文阅读. ', 0, '雨墨风起', 511333, 2534, 2345, 6234524, 1, NULL, 1, 51262); 创建索引使用 postman 等工具创建 es 索引，其中使用到了 ik 以及 pinyin 分词器，具体配置可以参考我前面的文章。 PUT: http://192.168.200.192:9200/novel123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&#123; \"settings\":&#123; \"number_of_shards\":5, \"number_of_replicas\":1, \"analysis\":&#123; \"analyzer\":&#123; \"pinyin_analyzer\":&#123; \"tokenizer\":\"ik_smart\", \"filter\":[ \"full_pinyin_no_space\", \"full_pinyin_with_space\", \"first_letter_pinyin\" ] &#125; &#125;, \"filter\":&#123; \"full_pinyin_no_space\":&#123; \"type\":\"pinyin\", \"first_letter\":\"none\", \"padding_char\":\"\" &#125;, \"full_pinyin_with_space\":&#123; \"type\":\"pinyin\", \"first_letter\":\"none\", \"padding_char\":\" \" &#125;, \"first_letter_pinyin\":&#123; \"type\":\"pinyin\", \"first_letter\":\"only\", \"padding_char\":\"\" &#125; &#125; &#125; &#125;, \"mappings\":&#123; \"book\":&#123; \"properties\":&#123; \"id\":&#123; \"type\":\"integer\" &#125;, \"words\":&#123; \"type\":\"integer\" &#125;, \"intro\":&#123; \"analyzer\":\"ik_smart\", \"search_analyzer\":\"ik_smart\", \"type\":\"text\" &#125;, \"name\":&#123; \"analyzer\":\"pinyin_analyzer\", \"search_analyzer\":\"pinyin_analyzer\", \"type\":\"text\" &#125;, \"sort\":&#123; \"type\":\"integer\" &#125;, \"updatetime\":&#123; \"type\":\"date\", \"format\":\"yyyy-MM-dd HH:mm:ss\" &#125;, \"vip\":&#123; \"type\":\"boolean\" &#125;, \"site\":&#123; \"type\":\"integer\" &#125;, \"author\":&#123; \"analyzer\":\"pinyin_analyzer\", \"search_analyzer\":\"pinyin_analyzer\", \"type\":\"text\" &#125;, \"collection\":&#123; \"type\":\"integer\" &#125;, \"click\":&#123; \"type\":\"integer\" &#125;, \"popularity\":&#123; \"type\":\"integer\" &#125;, \"goods\":&#123; \"type\":\"integer\" &#125;, \"status\":&#123; \"type\":\"integer\" &#125; &#125; &#125; &#125;&#125; 注意，这里索引的所有字段都是小写的，不要包含大写，否则后续会出现问题。索引创建好了之后，安装 logstash。 安装 logstashTips: 因为 logstash 启动的时候，会占用较高的 CPU ，建议不要放在 es 集群的服务器上，最好换一台服务器进行安装。123wget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.0.tar.gztar -xvf logstash-6.4.0.tar.gz 因为建立索引需要使用到 logstash-input-jdbc，所以先配置这个插件，这个插件在 logstash 5.0 之后就默认自带了，无需再次安装。 配置 logstash同步之全量同步1234567891011121314151617181920## 进入 logstash 根目录cd /usr/local/es/logstash-6.4.0## 创建存放 logstash-input-jdbc 相关配置以及依赖的目录mkdir logstash-input-jdbccd logstash-input-jdbc## 创建两个目录mkdir lib &amp; mkdir conf## 下载 jdbc 所需的 mysql 驱动到 lib 目录wget http://central.maven.org/maven2/mysql/mysql-connector-java/6.0.6/mysql-connector-java-6.0.6.jar -P lib/## 进入 conf/ 目录cd conf/## 创建两个文件，一个是 jdbc 的 sql，另一个是 logstash-input-jdbc 的配置文件vim mysql2es.sql 首先编写 sql， 构造索引所需的数据。由于是第一次同步到 es,所以进行全量同步，不过这里的全量也并非是一次性把查出库内所有数据，而是伪全量的增量同步。 mysql2es.sql 内容为12345678910111213141516171819SELECT `id`, IFNULL( `update_time`, '1970-01-01 08:00:00' ) AS `updatetime`, `status`, `name`, IFNULL( `intro`, '' ) AS `intro`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity` FROM book WHERE id &gt;= :sql_last_value and id &lt; :sql_last_value + 11 注意为了避免一次性查询，对数据库造成太大压力，因此这里使用了增量的方式来完成初始化同步到 es，这里的 :sql_last_value 是 logstash 上一步同步的最后的值，这里为 id，也可以是时间。因为演示用的测试数据较少，所以每次只同步了 10 条记录，如果实际使用，一次同步 1000 条比较合适，即 :sql_last_value + 1001 sql 查询出的字段名要与索引字段名称对应上，否则无法映射同步。 这里的 mysql2es.sql 将在 logstash-input-jdbc 中用到，创建文件 mysql2es.conf1vim mysql2es.conf mysql2es.conf 的内容是：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152input &#123; stdin &#123; &#125; jdbc &#123; # 数据库 jdbc_connection_string =&gt; &quot;jdbc:mysql://192.168.199.192:3306/novel&quot; # 用户名密码 jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # jar包的位置 jdbc_driver_library =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/lib/mysql-connector-java-6.0.6.jar&quot; # mysql的Driver jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; # 读取这个sql statement_filepath =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/mysql2es.sql&quot; # 指定时区 jdbc_default_timezone =&gt; &quot;Asia/Shanghai&quot; # 每分钟执行一次同步(分 时 天 月 年)，比如每十分钟(*/10 * * * *) schedule =&gt; &quot;* * * * *&quot; #索引的类型 type =&gt; &quot;book&quot; use_column_value =&gt; &quot;true&quot; #tracking_column_type: 递增字段的类型，numeric 表示数值类型, timestamp 表示时间戳类型 tracking_column_type =&gt; &quot;numeric&quot; # 递增字段 tracking_column =&gt; &quot;id&quot; # 保存每次同步时递增字段的最后一个值到这个文件 last_run_metadata_path =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/full_sync&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;192.168.199.192:9200&quot; # index 索引名 index =&gt; &quot;novel&quot; # 需要关联的数据库中有有一个id字段，对应索引的id号 document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 启动logstash同步1/usr/local/es/logstash-6.4.0/bin/logstash -f /usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/mysql2es.conf 启动后稍等一会儿，如果配置正确会打印出执行的 sql:1SELECT `id`,IFNULL(`update_time`,'1970-01-01 08:00:00') AS `updatetime`,`status`,`name`,IFNULL(`intro`,'') AS `intro`,`author`,`words`,`collection`,`goods`,`click`,`site`,`sort`,`vip`,`popularity` FROM book WHERE id&gt; 0 AND id&lt; 0+11 查看 ElasticSearchHead 控制台，发现也已经有了 10 条索引数据。由于每次同步 10 条，每分钟同步一次，5分钟后，测试的 50条记录也已经全部被同步到 es 里了。 ctrl + c 停止同步进程，查看文件1cat /usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/full_sync 文件中的数字正是上次最后同步的 id logstash增量同步根据 id 进行同步，如果对数据库中已被同步到 es 的数据进行了修改，这个数据也不会被同步更新到 es 当中去。 根据 id 每次同步 1000 条，这种同步方式也只适合第一次全量进行初始化时候使用，后续增量同步最好根据时间戳的方式完成。 修改 mysql2es.sql1234567891011121314151617181920SELECT `id`, IFNULL( `update_time`, '1970-01-01 08:00:00' ) AS `updatetime`, `status`, `name`, IFNULL( `intro`, '' ) AS `intro`, `author`, `words`, `collection`, `goods`, `click`, `site`, `sort`, `vip`, `popularity` FROM book WHERE update_time &gt;= convert_tz(:sql_last_value, '+00:00','-08:00') order by update_time asc 注意这里使用到了 convert_tz 这个函数，原因是 logstash 会在同步时在最后同步时间增加 8 个小时，因此需要使用函数，减去 8 个小时才是正确的时间。 因为logstash 记录最后一次同步的值是最后一条记录的，所以，最好根据 update_time 进行升序排序，即取的值是离现在最近的时间。 修改 mysql2es.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152input &#123; stdin &#123; &#125; jdbc &#123; # 数据库 jdbc_connection_string =&gt; &quot;jdbc:mysql://192.168.199.192:3306/novel&quot; # 用户名密码 jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;123456&quot; # jar包的位置 jdbc_driver_library =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/lib/mysql-connector-java-6.0.6.jar&quot; # mysql的Driver jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; # 读取这个sql statement_filepath =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/mysql2es.sql&quot; # 指定时区 jdbc_default_timezone =&gt; &quot;Asia/Shanghai&quot; # 每分钟 schedule =&gt; &quot;* * * * *&quot; #索引的类型 type =&gt; &quot;book&quot; use_column_value =&gt; &quot;true&quot; #tracking_column_type: 递增字段的类型，numeric 表示数值类型, timestamp 表示时间戳类型 tracking_column_type =&gt; &quot;timestamp&quot; # 递增字段 tracking_column =&gt; &quot;updatetime&quot; # 保存每次同步时递增字段的最后一个值到这个文件 last_run_metadata_path =&gt; &quot;/usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/incr_sync&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;192.168.199.192:9200&quot; # index 索引名 index =&gt; &quot;novel&quot; # 需要关联的数据库中有有一个id字段，对应索引的id号 document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 修改增量的字段为 updatetime，且字段类型设置为 timestamp，指定记录最后同步时间的文件为 incr_sync ，同时在 conf 目录下创建这个文件 incr_sync，设置一下初始值，即同步大于设置这个值的数据。123456789cd /usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/vim incr_conf# 文件内容如下，这里具体时间根据你的情况自行修改--- !ruby/object:DateTime &apos;2018-09-04 16:47:14.000000000 Z&apos;## 注意修改只需要改 2018-09-04 16:47:14 这一部分，其他地方就不要改，改了格式可能会不对。 保存退出，执行同步命令1/usr/local/es/logstash-6.4.0/bin/logstash -f /usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/mysql2es.conf 过一会儿，正常输出 sql 语句1SELECT `id`,IFNULL(`update_time`,&apos;1970-01-01 08:00:00&apos;) AS `updatetime`,`status`,`name`,IFNULL(`intro`,&apos;&apos;) AS `intro`,`author`,`words`,`collection`,`goods`,`click`,`site`,`sort`,`vip`,`popularity` FROM book WHERE update_time&gt; &apos;2018-09-04 16:47:14&apos; order by update_time asc incr_sync 文件中的值，会因为每次同步同步取的是最后一条记录的值，所以最好对时间进行排序 配置文件中，当在input的jdbc下，增加type属性时，会导致该索引下增加type字段。所以sql查询出的字段不要用type，如果有，as成其他的名字，不然的话，这里判断会有异常 logstash 同步后台运行上面的运行命令如果推出终端或者按下 ctrl + c，同步就会终止，所以我们要让同步任务在后台运行。1nouhup /usr/local/es/logstash-6.4.0/bin/logstash -f /usr/local/es/logstash-6.4.0/logstash-input-jdbc/conf/mysql2es.conf &amp; 使用 nohup 和 * 号将要执行的语句包裹起来，就可以实现后台运行了，同时会在当前执行命令的目录生成一个 nohup.out 的文件，这里 logstash 的运行日志会被写入到这个文件当中。 1tail -fn300 nohup.out 有疑问?欢迎来信，给我写信 参考 https://www.cnblogs.com/licongyu/p/5383334.html https://blog.csdn.net/laoyang360/article/details/51747266 https://segmentfault.com/a/1190000011784259 https://wenchao.ren/archives/393#8 http://www.cnblogs.com/xuwenjin/p/8989043.html","tags":[{"name":"ElasticSearch索引同步创建","slug":"ElasticSearch索引同步创建","permalink":"http://nullpointer.pw/tags/ElasticSearch索引同步创建/"},{"name":"logstash全量同步建立es索引","slug":"logstash全量同步建立es索引","permalink":"http://nullpointer.pw/tags/logstash全量同步建立es索引/"},{"name":"logstash增量同步建立es索引","slug":"logstash增量同步建立es索引","permalink":"http://nullpointer.pw/tags/logstash增量同步建立es索引/"},{"name":"logstash-input-jdbc增量同步时间不对","slug":"logstash-input-jdbc增量同步时间不对","permalink":"http://nullpointer.pw/tags/logstash-input-jdbc增量同步时间不对/"},{"name":"logstash-input-jdbc建立索引","slug":"logstash-input-jdbc建立索引","permalink":"http://nullpointer.pw/tags/logstash-input-jdbc建立索引/"}]},{"title":"和我一起打造个简单搜索之IK分词以及拼音分词","date":"2018-09-13T13:46:52.000Z","path":"和我一起打造个简单搜索之IK分词以及拼音分词.html","text":"elasticsearch 官方默认的分词插件，对中文分词效果不理想，它是把中文词语分成了一个一个的汉字。所以我们引入 es 插件 es-ik。同时为了提升用户体验，引入 es-pinyin 插件。本文介绍这两个 es 插件的安装。 环境本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 ik 分词器 和 pinyin 分词器在 github 仓库可以找到，注意版本与 elasticsearch 的版本需要对应，本文使用 5.5.3 版本 ik 分词器下载 pinyin 分词器下载 如果 elasticsearch 与我的版本不同，可以去官方 github 对应的仓库查看对应版本的 ik 以及 pinyin 分词器。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 安装 ik 分词器前文中，我们已经安装了 elasticsearch，我们在其基础上安装分词器12345678910111213141516171819202122cd /usr/local/es/## 下载 ik 分词器wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.3/elasticsearch-analysis-ik-5.5.3.zip## 使用 unzip 解压## 如果未安装，通过 yum instal unzip 进行安装unzip elasticsearch-analysis-ik-5.5.3.zip## 在 elasticsearch 安装目录下的 plugins 文件夹下创建 ik 目录mkdir /usr/local/es/master/plugins/ik## 将解压后的 ik 目录下的所有文件移动到 /usr/local/es/master/plugins/ik/mv /usr/local/es/elasticsearch/* /usr/local/es/master/plugins/ik/## 重启 elasticsearch 或启动 master elasticsearch## 注意启动时，需要切换到 esuser 用户## 启动成功后，日志里会打印此行[2018-09-02T06:33:43,703][INFO ][o.e.p.PluginsService ] [master] loaded plugin [analysis-ik] 验证 ik 分词器是否生效打开 Restlet Client 或者 postman 工具 GET http://192.168.199.192:9200/_analyze?analyzer=ik_smart&amp;text=&quot;中华人民共和国国歌&quot; 移除名为 ik 的analyzer和tokenizer,请分别使用 ik_smart 和 ik_max_wordik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。 这里 analyzer 可以指定分词类型，发送请求返回结果：123456789101112131415161718&#123; \"tokens\": [ &#123; \"token\": \"中华人民共和国\", \"start_offset\": 1, \"end_offset\": 8, \"type\": \"CN_WORD\", \"position\": 0 &#125;, &#123; \"token\": \"国歌\", \"start_offset\": 8, \"end_offset\": 10, \"type\": \"CN_WORD\", \"position\": 1 &#125; ]&#125; 我这里指定的 analyzer 为 ik_smart，即粗粒度分词，可以看到 ik 分词器已经生效了。接下来继续安装 pinyin 分词器。 安装 pinyin 分词器1234567891011121314151617181920212223cd /usr/local/es/## 下载 pinyin 分词器wget https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v5.5.3/elasticsearch-analysis-pinyin-5.5.3.zip## 使用 unzip 解压## 如果未安装，通过 yum instal unzip 进行安装unzip elasticsearch-analysis-pinyin-5.5.3.zip## 在 elasticsearch 安装目录下的 plugins 文件夹下创建 pinyin 目录mkdir /usr/local/es/master/plugins/pinyin## 将解压后的 ik 目录下的所有文件移动到 /usr/local/es/master/plugins/pinyin/mv /usr/local/es/elasticsearch/* /usr/local/es/master/plugins/pinyin/## 重启 elasticsearch 或启动 master elasticsearch## 注意启动时，需要切换到 esuser 用户## 启动成功后，日志里打印[2018-09-02T07:04:56,203][INFO ][o.e.p.PluginsService ] [master] loaded plugin [analysis-ik][2018-09-02T07:04:56,203][INFO ][o.e.p.PluginsService ] [master] loaded plugin [analysis-pinyin] 验证 pinyin 分词器是否生效打开 Restlet Client 或者 postman 工具 GET http://192.168.199.192:9200/_analyze?analyzer=pinyin&amp;text=&quot;李小龙&quot; 这里 analyzer 可以指定分词为 pinyin，发送请求返回结果：1234567891011121314151617181920212223242526272829303132&#123; \"tokens\": [ &#123; \"token\": \"li\", \"start_offset\": 1, \"end_offset\": 2, \"type\": \"word\", \"position\": 0 &#125;, &#123; \"token\": \"xiao\", \"start_offset\": 2, \"end_offset\": 3, \"type\": \"word\", \"position\": 1 &#125;, &#123; \"token\": \"long\", \"start_offset\": 3, \"end_offset\": 4, \"type\": \"word\", \"position\": 2 &#125;, &#123; \"token\": \"lxl\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"word\", \"position\": 2 &#125; ]&#125; 观察结果，说明 pinyin 分词器也已经生效了。 集群其他节点分词器安装现在只对 master 进行了安装，其他 slave 也需要安装，这里可以通过拷贝的方式直接来完成安装了。123cp -r master/plugins/ slave1/cp -r master/plugins/ slave2/ 如果其他节点在不同服务器上，通过 scp 命令拷贝即可。 有疑问?欢迎来信，给我写信 参考 https://github.com/medcl/elasticsearch-analysis-pinyin/issues/19","tags":[{"name":"ElasticSearch之IK分词器安装","slug":"ElasticSearch之IK分词器安装","permalink":"http://nullpointer.pw/tags/ElasticSearch之IK分词器安装/"},{"name":"ElasticSearch之pinyin分词器安装","slug":"ElasticSearch之pinyin分词器安装","permalink":"http://nullpointer.pw/tags/ElasticSearch之pinyin分词器安装/"},{"name":"ElasticSearch之IK分词器与pinyin分词器一起使用","slug":"ElasticSearch之IK分词器与pinyin分词器一起使用","permalink":"http://nullpointer.pw/tags/ElasticSearch之IK分词器与pinyin分词器一起使用/"}]},{"title":"和我一起打造个简单搜索之ElasticSearch入门","date":"2018-09-13T13:45:26.000Z","path":"和我一起打造个简单搜索之ElasticSearch入门.html","text":"本文简单介绍了使用 Rest 接口，对 es 进行操作，更深入的学习，可以参考文末部分。 环境本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 基础概念索引–相当于数据库类型–相当于表文档–相当于一条记录分片–对索引进行分片，分布于集群各个节点上，降低单个节点的压力备份–拷贝分片就完成了备份 基本语法 索引索引类型 结构化索引 特点：通过接口创建，可以指定 mappings url:port/索引名/类型名/_mappings 非结构化索引 特点：通过 elasticsearch head 创建 mapping 为空 使用 RestClient/PostMan 创建结构化索引创建一个 people 的索引，包含一个类型 man 12345678910111213141516171819202122232425&#123; \"settings\":&#123; \"number_of_shards\":3, \"number_of_replicas\":1 &#125;, \"mappings\":&#123; \"man\":&#123; \"properties\":&#123; \"name\":&#123; \"type\":\"text\" &#125;, \"country\":&#123; \"type\":\"keyword\" &#125;, \"age\":&#123; \"type\":\"integer\" &#125;, \"date\":&#123; \"type\":\"date\", \"format\":\"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" &#125; &#125; &#125; &#125;&#125; keyword 与 text 的区别type 类型为 keyword 的时候，ES 不会对其进行分词，而 text 会被分词 es 通过 rest 接口对数据操作插入数据 指定文档 id 插入 PUT 请求： ip:port/索引/类型/id 自动产生文档 id 插入 POST 请求： ip:port/索引/类型 修改数据 直接修改 POST 请求： ip:port/索引/类型/id/_update 删除数据/删除索引 删除文档 DELETE 请求 ip:端口/索引/类型/id 删除索引 DELETE 请求 ip:端口/索引 查询数据 简单查询 GET 请求： ip:端口/索引/类型/id 条件查询 POST请求： ip:端口/索引/_search 指定条件查询,分页与排序 12345678910111213141516&#123; \"query\":&#123; \"match\":&#123; \"name\":\"WeJan\" &#125; &#125;, \"sort\":[ &#123; \"age\":&#123; \"order\":\"asc\" &#125; &#125; ], \"from\" : 0, \"size\" : 10 &#125; 聚合查询 POST请求： ip:端口/索引/_search 分组，可以多个分组 123456789&#123; \"aggs\":&#123; \"group_by_age\":&#123; \"terms\":&#123; \"field\":\"age\" &#125; &#125; &#125;&#125; 聚合计算 123456789&#123; \"aggs\":&#123; \"age_count\":&#123; \"stats\":&#123; \"field\":\"age\" &#125; &#125; &#125;&#125; sum avg max min count 高级查询 习语匹配（全词匹配） 1234567&#123; \"query\":&#123; \"match_phrase\":&#123; \"name\":\"WeJan\" &#125; &#125;&#125; 多字段匹配，多个字段包含query 12345678&#123; \"query\":&#123; \"multi_match\":&#123; \"query\":\"WeJan\", \"fields\":[\"author\", \"title\"] &#125; &#125;&#125; query_string 文本查询 1234567&#123; \"query\":&#123; \"query_string\":&#123; \"name\":\"(WeJan AND Jan) OR 哈哈\" &#125; &#125;&#125; 针对多个字段的文本查询 12345678&#123; \"query\":&#123; \"query_string\":&#123; \"name\":\"WeJan OR 哈哈\", \"fields\":[\"author\", \"title\"] &#125; &#125;&#125; 字段查询,比如年龄，分类 1234567&#123; \"query\":&#123; \"term\":&#123; \"age\":25 &#125; &#125;&#125; 范围查询，可以对时间字段进行查询，现在使用 “now”替代 12345678910&#123; \"query\":&#123; \"range\":&#123; \"age\":&#123; \"gte\":26, \"lte\":36 &#125; &#125; &#125;&#125; 参考 慕课网ElasticSearch入门","tags":[{"name":"ElasticSearch语法入门","slug":"ElasticSearch语法入门","permalink":"http://nullpointer.pw/tags/ElasticSearch语法入门/"}]},{"title":"和我一起打造个简单搜索之ElasticSearch集群搭建","date":"2018-09-13T13:44:35.000Z","path":"和我一起打造个简单搜索之ElasticSearch集群搭建.html","text":"我们所常见的电商搜索如京东，搜索页面都会提供各种各样的筛选条件，比如品牌、尺寸、适用季节、价格区间等，同时提供排序，比如价格排序，信誉排序，销量排序等，方便了用户去找到自己心里理想的商品。 站内搜索对于一个网站几乎是标配，只是搜索的强大与否的区别，有的网站只支持关键词模糊搜索，而淘宝，京东提供了精细的筛选条件，同时支持拼音搜索等更方便的搜索方式。 由于笔者在一家做网络文学的公司工作，所以实现就是以小说为商品的搜索，具体可以参考起点网小说的搜索。 如图所示，起点网的搜索提供了关键词搜索和排序条件以及筛选条件，接下来，我们一起来实现这个吧~ 环境本文以及后续 es 系列文章都基于 5.5.3 这个版本的 elasticsearch ，这个版本比较稳定，可以用于生产环境。 系列文章 一、和我一起打造个简单搜索之ElasticSearch集群搭建 二、和我一起打造个简单搜索之ElasticSearch入门 三、和我一起打造个简单搜索之IK分词以及拼音分词 四、和我一起打造个简单搜索之Logstash实时同步建立索引 五、和我一起打造个简单搜索之SpringDataElasticSearch入门 六、和我一起打造个简单搜索之SpringDataElasticSearch关键词高亮 … 环境准备之 ES 集群搭建master 配置1234567891011121314151617181920212223242526272829303132333435## 下载 elasticsearchwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.3.tar.gz## 创建目录mkdir /usr/local/es## 解压文件到 es 目录tar -xvf elasticsearch-5.5.3.tar.gz -C /usr/local/es## 修改 es 的配置文件cd /usr/local/es/elasticsearch-5.5.3/configvim elasticsearch.yml## 在文件末尾增加http.cors.enabled: truehttp.cors.allow-origin: \"*\"cluster.name: es-searchnode.name: slave1node.master: truenetwork.host: 0.0.0.0## 修改 es 的 jvm 设置，如果不调节，可能启动一个 master, slave 就没足够内存来启动了vim jvm.options修改-Xms2g -Xmx2g为-Xms512m-Xmx512m注意，如果是线上，这个内存就不要修改了，使用默认的内存 2G 即可。 slave 配置123456789101112131415161718192021222324252627282930313233343536## 准备搭建一个伪集群 1个master + 2个slavecd /usr/local/es## 将目录重命名为 master[root@localhost es]# mv elasticsearch-5.5.3/ master##拷贝两份为 slave[root@localhost es]# cp -r master/ slave1[root@localhost es]# cp -r master/ slave2## 修改两个 slave 的配置### 修改 slave1 的配置[root@localhost es]# vim slave1/config/elasticsearch.ymlhttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;cluster.name: es-searchnode.name: slave1## 注意 http 端口不要设置一样，以免冲突http.port: 8200#node.master: truenetwork.host: 0.0.0.0### 修改 slave2 的配置[root@localhost es]# vim slave2/config/elasticsearch.ymlhttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;cluster.name: es-searchnode.name: slave2## 注意 http 端口不要设置一样，以免冲突http.port: 7200#node.master: truenetwork.host: 0.0.0.0 添加用户12345678## 启动 es 不能使用 root 用户，所以先需要增加新的用户[root@localhost es]# adduser esuser[root@localhost es]# chown -R esuser /usr/local/es/## 切换到 esuser 用户[root@localhost es]# su esuser[esuser@localhost es]$ chmod 777 /usr/local/es/ 启动集群中的 master123456789101112131415161718192021222324252627282930313233343536373839404142# 先测试能否正常启动[esuser@localhost es]$ /usr/local/es/master/bin/elasticsearch# 查看打印的日志信息[2018-09-02T01:45:21,125][INFO ][o.e.g.GatewayService ] [master] recovered [0] indices into cluster_state[2018-09-02T01:45:21,138][INFO ][o.e.h.n.Netty4HttpServerTransport] [master] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;[2018-09-02T01:45:21,138][INFO ][o.e.n.Node ] [master] started## 启动失败提示ERROR: [2] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]## 切换到 root 用户，修改系统配置su root# 输入登录密码vim /etc/security/limits.conf## 在文件末尾增加，不要去掉前面的 * 号* soft nofile 300000* hard nofile 300000* soft nproc 102400* soft memlock unlimited* hard memlock unlimited## 对 sysctl.conf 文件 进行修改echo &quot;vm.max_map_count=262144&quot; &gt; /etc/sysctl.confsysctl -p## 修改完毕，切换回 esuser 用户身份su esuser## 尝试启动[esuser@localhost es]$ /usr/local/es/master/bin/elasticsearch# 提示已经启动成功了[2018-09-02T02:10:14,285][INFO ][o.e.h.n.Netty4HttpServerTransport] [master] publish_address &#123;192.168.199.192:9200&#125;, bound_addresses &#123;[::]:9200&#125;[2018-09-02T02:10:14,285][INFO ][o.e.n.Node ] [master] started[2018-09-02T02:10:14,289][INFO ][o.e.g.GatewayService ] [master] recovered [0] indices into cluster_state 验证启动使用浏览器访问 http://ip:9200 ip 替换你的 ip 地址，我的是 http://192.168.199.192:9200 浏览器响应内容12345678910111213&#123; name: \"master\", cluster_name: \"es-search\", cluster_uuid: \"JoNUMEKFS06NHNS7p3bdWg\", version: &#123; number: \"5.5.3\", build_hash: \"9305a5e\", build_date: \"2017-09-07T15:56:59.599Z\", build_snapshot: false, lucene_version: \"6.6.0\" &#125;, tagline: \"You Know, for Search\"&#125; 注意：如果无法访问，请关闭防火墙 后台守护进程启动 es 集群前文是直接启动，如果按下 ctrl + c 或者结束 ssh 会话，es 会立即停止退出，因此需要通过守护进程后台启动 123456789101112131415161718192021222324252627282930313233343536373839404142434445[esuser@localhost es]$ /usr/local/es/master/bin/elasticsearch -d## 查看是否启动成功ps -ef | grep elasticsearch## 正常可以看到一个 elasticsearch 进程## 如前文一样，分别测试两个 slave 是否可以正常启动### 测试slave1[esuser@localhost es]$ /usr/local/es/slave1/bin/elasticsearch -d### 浏览器访问 http://ip:8200，响应为：&#123; &quot;name&quot;:&quot;slave1&quot;, &quot;cluster_name&quot;:&quot;es-search&quot;, &quot;cluster_uuid&quot;:&quot;JoNUMEKFS06NHNS7p3bdWg&quot;, &quot;version&quot;:&#123; &quot;number&quot;:&quot;5.5.3&quot;, &quot;build_hash&quot;:&quot;9305a5e&quot;, &quot;build_date&quot;:&quot;2017-09-07T15:56:59.599Z&quot;, &quot;build_snapshot&quot;:false, &quot;lucene_version&quot;:&quot;6.6.0&quot; &#125;, &quot;tagline&quot;:&quot;You Know, for Search&quot;&#125;### 测试slave2[esuser@localhost es]$ /usr/local/es/slave2/bin/elasticsearch -d### 浏览器访问 http://ip:8200，响应为：&#123; name: &quot;slave2&quot;, cluster_name: &quot;es-search&quot;, cluster_uuid: &quot;JoNUMEKFS06NHNS7p3bdWg&quot;, version: &#123; number: &quot;5.5.3&quot;, build_hash: &quot;9305a5e&quot;, build_date: &quot;2017-09-07T15:56:59.599Z&quot;, build_snapshot: false, lucene_version: &quot;6.6.0&quot; &#125;, tagline: &quot;You Know, for Search&quot;&#125; 至此完成了 es 集群(伪)的搭建。 环境准备之 elasticsearch head 安装为了方便我们观察调试，安装这个 es 插件。 安装步骤参考官方 github 1234567# 把插件安装到 es 目录下cd /usr/local/es/git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm installnpm run start 在 npm install 这一步，由于国内网络环境的原因，可能会失败，可以npm 换源重试。 启动完成后，浏览器访问 http://ip:9100，我的是 http://192.168.199.192:9100/ 这里需要修改连接地址，为你的 es 所在的 ip:9200。后面集群健康为绿色为正常。 到这里，插件也就安好了。 分词商品搜索，分词是必不可少的，开源的中文分词最有名的莫过于 IK 分词了，同时为了给用户提供更好的体验，同时配置 pinyin 分词，即输入拼音也可以进行搜索，网上也有对应的分词器，在下文中我们一起来配置分词器。 有疑问?欢迎来信，给我写信","tags":[{"name":"ElasticSearch环境搭建","slug":"ElasticSearch环境搭建","permalink":"http://nullpointer.pw/tags/ElasticSearch环境搭建/"}]},{"title":"爬虫元素选择技巧","date":"2018-08-31T14:11:06.000Z","path":"爬虫元素选择技巧.html","text":"前言作为一个爬虫框架，WebMagic 提供了多种选择器便于我们的使用。使用 Selectable 来对内容进行链式抽取，常用的抽取方式有：CSS 选择器、XPath，正则表达式，JsonPath。今天写写如何利用一些工具，来快捷编写这些选择语法。 工具工欲善其事必先利其器，有了一件趁手的兵器，干起活来也利索。工具主要用到了 Chrome 谷歌浏览器 Chrome 插件 XPath Helper(如果你翻不了 Q 的话，可以在这里 下载 插件) 选择对象为了方便实验，我选择对 掘金 (https://juejin.im/) 这个网站进行实验。以下内容默认使用的是 Chrome。 定义一个假的需求吧，需要对掘金热门进行爬取，如下所示。 这里我们只想要获取这里的所有的标题 Xpath 抽取具体步骤： 点击页面空白处，使用快捷键 ctrl + shift + x 调出 Xpath Helper 工具，调出后，按住 shift 移动鼠标可以选择元素，并且该元素的 xpath 会显示在上方框内 可以看到，工具生成的 XPath 路径非常长，需要进行优化。这里可以使用谷歌浏览器的开发者工具很方便得获取 Xpath 路径，打开开发者工具，使用开发者工具面板上最左侧的箭头选择工具，选择一个标题元素，右侧就会自动定位到该元素的源码处，右键源码里的这个元素，选择 Copy XPath。 清空 Xpath Helper 中的 QUERY 框内的内容，粘贴 Chrome 开发者工具拷贝的 Xpath 路径，可以清晰对比看到路径相纸上一步获取的要简短很多，我们可以在这个基础上进行进一步地优化。 由于我们需要获取的是所有的标题，所以修改 Xpath，同时 Xpath Helper 高亮了被选择的元素。因为当前标题是在一个列表页当中，把 li[2] 中的值修改为 * 就是选择所有了，// 表示从根元素查找，替代掉了前面多余的元素， XPath 基础语法可以上 Google 搜索一下。 CSS 选择器有的网站的 HTML 结构不适合使用 XPath 来写，会写的很长，可以尝试使用 css 选择器来完成，这里还是借助 Chrome 的开发者工具。 同上一步一样，选择一个元素后，选择 Copy selector 可以在 Chrome 开发者工具的 console 里验证选择器是否正确,这里正确打印出了这个元素。在 webmagic 里通过 css 方法或者 $ 方法传递 css 选择器参数，得到对象. json path 方式详见我的另外一篇博客 正则表达式方式这个就不用多说了，通过正则抽取正文符合的内容","tags":[{"name":"爬虫技巧","slug":"爬虫技巧","permalink":"http://nullpointer.pw/tags/爬虫技巧/"}]},{"title":"聊聊我的规划","date":"2018-08-26T14:21:10.000Z","path":"聊聊我的规划.html","text":"前言去年的这个时候，闲着无事刷知乎，看到一个问题觉得很有意思，【天天写业务代码的程序员，怎么成为技术大牛，开始写技术代码？】，当时的自己确实也一直是在写业务方面的代码，写的时间久了，面对着源源不断的需求，只能硬着头皮去应对，内心难免有所困扰；看到标题，我也不禁在想，什么时候我也能成为技术大牛呢？（对技术有所追求的 Coder 目标应该都是成为一个技术大牛吧）抱着这种心态，我点进了问题。 我该怎么做？看了几个回答，几乎都在讲大道理，看了感觉是这么一回事很有道理的样子，但是并没有起到什么实际作用。但是看到了李运华的回答之后，“3D 理论”(Do more、Do better、Do exercise)对于我，用醍醐灌顶来形容也不为过，他的回答不仅解答了我的疑惑，同时也给出了具体的实际方法。兴奋之余，结合自己的工作情况，将原回答中 Do more 进行了小小改动，随后在笔记上做了一点相关的总结。 我把 “3D 理论” 分成了两个场景，一是在公司，另一个是在家。因为在当前的公司当中，不可能把大把的正常工作时间用于新的技术学习之上，所以，我认为在公司中能做的只有 1，2，3 这三点，而第 4 点只能利用你的业余时间来进行。 当然除了那种没有任务也必须硬是要待到 10 点以后才能下班的公司除外，这种公司，其实挤出几个小时，用于自己的学习应该也是可以的。如果可以准时下班，除掉一天的睡觉，吃饭，工作，交通的时间，可以支配的时间其实还是挺充裕的，前提是不玩游戏不刷剧，如果沉迷的话可以想一些措施来强制约束自己。 后记到现在 2018 年 8 月份，已经过去了一年。很惭愧的是，笔者认为自己对于 “3D 理论”的执行力度还是远远不够，理论中的目前也只做到了其中一点，在公司里对自己负责的业务进行了较大的调整优化，也达到了预期的可靠稳定。 随着工作年限的增长，我就越发觉得程序员是个不学习就一定会被淘汰掉的职业，经常能看到某某公司辞退 40 岁以上程序员的新闻，就愈发感觉压力逐渐变大（虽然前几天我才过完 25 岁生日），其实换一个思路想，如果一个程序员在公司中是难以被取代的，那么也不会是被辞退那波人中的一名吧！ 也是从去年的 7 月开始，约束自己每月至少要写两篇博客的目标也达到了，也算是执行了 “3D 理论”中的一点吧。 现在在公司中，负责的内容也发生了转变，纯业务代码写的比以前要少很多，更多的是要去关注基础服务稳定，对原有的项目优化，服务拆分等方面；但是又深觉自己的技术功底还很欠缺，忙着补充新的知识，于是在工作的产出就不是那么得多了，这也是让我很头疼的事情，毕竟工作的产出关系到自己的绩效。 可能每个时期都会出现令我头疼的事情，也许这也是让我突破自我的契机。 持续践行 “3D理论”，做一个践行的程序员，目标或许已在不远处啦~","tags":[]},{"title":"WebMagic实现分布式抓取以及断点抓取","date":"2018-08-04T15:30:29.000Z","path":"WebMagic实现分布式抓取.html","text":"前言从去年到今年，笔者主要负责的是与合作方的内容对接，新增的合作商不是很多的情况下，在我自从去年引入了 WebMagic 这个爬虫框架之后，基本很少需要去关注维护爬虫，做的最多的是新接入合作商去写对应爬虫抓取模板。 因为在代码中实现了增量抓取，单机也足以承担日常的抓取工作。 在前两周，由于公司拓展新的业务渠道，需要接入的合作商瞬间增加了 3 倍，又被要求在 2 天内全部接入，那两天和另外一个同事，几乎都在忙着适配模板。 急速增加合作商的同时，服务器无法承受压力，频繁爆出 OOM 异常，导致抓取大批量失败，其中最多的一个合作商接口，需要解析下载的页面近 500w 个，单机抓取已无法满足需求，需要多台服务器同时抓取。 但鉴于当时需求紧，没有时间对爬虫部分代码进行重构升级，单机抓取也不行，而且会影响正常抓取任务的执行，于是临时想了个办法在其他服务器上抓取某个合作商，才坎坷解决了这个问题，但这也并非长久之计。 分布式抓取基础前提之一因为刚刚引入 WebMagic 这个框架的时候，还不是太熟悉，使用的 Scheduler 是默认基于内存的队列 QueueScheduler ，当待抓取的 URL 太多时，内存就被占满了，从而导致 OOM。 如果要实现分布式抓取，前提需要使用基于 Redis 的 RedisScheduler。 在创建爬虫的时候，手动设置 Scheduler 为 RedisScheduler。 1spider.setScheduler(new RedisScheduler(jedisPool)); RedisScheduler 需要传入 JedisPool 参数。 如果使用的是 SpringBoot，可以声明一个 RedisConfig 的配置类。 123456789101112131415161718192021222324252627282930@Configurationpublic class RedisConfig &#123; @Value(\"$&#123;spring.redis.host&#125;\") private String host; @Value(\"$&#123;spring.redis.port&#125;\") private int port; @Value(\"$&#123;spring.redis.password&#125;\") private String password; @Value(\"$&#123;spring.redis.timeout&#125;\") private int timeout; @Value(\"$&#123;spring.redis.jedis.pool.max-idle&#125;\") private int maxIdle; @Value(\"$&#123;spring.redis.jedis.pool.max-wait&#125;\") private long maxWaitMillis; @Bean public JedisPool redisPoolFactory() &#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxIdle(maxIdle); jedisPoolConfig.setMaxWaitMillis(maxWaitMillis); return new JedisPool(jedisPoolConfig, host, port, timeout, password); &#125;&#125; 如果使用的是 Spring，可以在 XML 中配置声明一个 Bean 节点。12345678910111213&lt;bean id=\"poolConfig\" class=\"redis.clients.jedis.JedisPoolConfig\"&gt; &lt;property name=\"maxActive\" value=\"3000\" /&gt; &lt;property name=\"maxIdle\" value=\"100\" /&gt; &lt;property name=\"maxWait\" value=\"1000\" /&gt; &lt;property name=\"testOnBorrow\" value=\"true\"/&gt;&lt;/bean&gt;&lt;bean id=\"jedisPool\" class=\"redis.clients.jedis.JedisPool\"&gt; &lt;constructor-arg index=\"0\" ref=\"poolConfig\" /&gt; &lt;constructor-arg index=\"1\" value=\"127.0.0.1\"/&gt; &lt;constructor-arg index=\"2\" value=\"6379\"/&gt; &lt;constructor-arg index=\"3\" value=\"60000\" /&gt; &lt;constructor-arg index=\"4\" value=\"eCii8TH3xR8\"/&gt;&lt;/bean&gt; 声明了 JedisPool 之后，直接在代码中注入即可。12@Autowiredprivate JedisPool jedisPool; 分布式抓取基础前提之二仅仅配置了 RedisScheduler，还无法达成我们的进行分布式抓取的目的，如果需要进行分布式抓取，其队列应该是共享的，即多台服务器的多个爬虫使用同一个 Redis URL 队列，取 URL 或者添加 URL 都是同一个。 又因为是 WebMagic 在帮助我们管理 Scheduler，所以 URL 的维护也是 WebMagic 在做。 先看一段 WebMagic 的源码 123456789101112131415161718public void run() &#123; checkRunningStat(); initComponent(); logger.info(\"Spider &#123;&#125; started!\",getUUID()); while (!Thread.currentThread().isInterrupted() &amp;&amp; stat.get() == STAT_RUNNING) &#123; final Request request = scheduler.poll(this); if (request == null) &#123; if (threadPool.getThreadAlive() == 0 &amp;&amp; exitWhenComplete) &#123; break; &#125; // wait until new url added waitNewUrl(); &#125; else &#123; // ...... &#125; &#125; // ......&#125; 可以看到 WebMagic 抓取的时候通过这行代码获取队列中待抓取的 URL 地址。 1Request request = scheduler.poll(this); 而这个 this 是指实现了 Task 接口的对象，即把当前的 Spider 对象作为参数传入。 因为我们使用了 RedisScheduler，所以进入该类的 poll() 方法查看。 1String url = jedis.lpop(getQueueKey(task)); 通过 task 的 UUID 获取到队列的 key，然后利用 redis 的 list 的 lpop 命令从队列左侧弹出一个带抓取的 URL，构造 Request 对象。 同样的查看 poll 上面的 pushWhenNoDuplicate 方法，是将待抓取请求的 URL push到队列的右侧，而这个队列也是通过 Spider 的 UUID 里唯一确定的。 1jedis.rpush(getQueueKey(task), request.getUrl()); 所以，如果要实现分布式同时抓取同一个队列，就需要保持 多个 Spider 的 UUID 是一致的 实现分布式抓取用过 WebMagic 的人都知道，爬虫启动需要给他一个起始 URL，然后通过这个 URL 获取新的 URL；所以如果需要进行分布式抓取，肯定爬虫的起始 URL 是不能相同的，因为WebMagic 会对重复的 URL 进行自动去重。 因此爬虫的架构图从 变成了如下架构 即保证多个爬虫使用同一个 Redis 队列。具体思路就是第一只通过起始 URL 爬虫启动的时候，记录启动爬虫的设置UUID，然后启动其他爬虫的时候，设置爬虫的 UUID 为记录的 UUID 的值。 代码中体现的就是如下所示： 启动其他爬虫的时候，手动从队列中获取 URL 设置为启动 URL 即可。 分布式爬虫任务调度笔者实现的爬虫启动是通过定时任务启动的，因为其他爬虫与第一只爬虫的入口不同，因此定义了两个任务去调度，并且两个任务之间有 30s 的间隔时间，防止第一只爬虫还未添加 URL 到队列当中，而造成其他爬虫无 URL 可抓取情况的发生。 基于这个思路，因 URL 放在 Redis 之中，所以同时也可以实现 断点抓取。 结语WebMagic 的源码很简洁易懂，可以学习到很多东西，尤其是多线程以及锁的应用，很值得借鉴学习。","tags":[{"name":"WebMagic分布式抓取","slug":"WebMagic分布式抓取","permalink":"http://nullpointer.pw/tags/WebMagic分布式抓取/"},{"name":"WebMagic断点抓取","slug":"WebMagic断点抓取","permalink":"http://nullpointer.pw/tags/WebMagic断点抓取/"}]},{"title":"IDEA远程调试Tomcat","date":"2018-07-28T02:40:28.000Z","path":"IDEA远程调试Tomcat.html","text":"前言开发新功能时，经常会遇到在本地环境运行没问题，但是部署到了 Linux 服务器上跑的时候就出现问题这种情况；还有一种情况是需要定位线上问题，这两种情况以前的做法就是加 log 日志输出，总是不能精准定位问题所在，如果能够像本地环境一样调试代码，那这个问题也就迎刃而解了。 开始之前本文介绍 IDEA 与 Tomcat 的远程调试，且笔者在日常工作当中也是这么做的，网上很多都是互相抄袭，没有经过验证的伪原创，让人不快。 一、添加运行项IDEA 添加远程调试项 填写服务器 IP ，端口使用默认的即可，复制第二行参数 二、修改服务器中的 Tomcat 配置连接远程服务器，进入 Tomcat 根目录下的 bin 目录，修改 catalina.sh 文件 定位到 108 行左右，找到 cygwin=false 这一行， 在这一行的前一行添加 JAVA_OPTS=&quot;&quot; 然后将从 IDEA 中复制的那串参数粘贴到双引号之间，保存退出。 然后启动 Tomcat，如果 Tomcat 是已经启动的话，必须要重启一下。 三、调试环境修改为了调试方便，需要暂时关闭掉防火墙，否则可能无法连接。1service iptables stop 四、IDEA 远程调试回到 IDEA，点击 Debug 按钮，启动刚才添加的 Remote，如果以上没有配置错，那么控制台将打印出 此时就和本地调试一样打断点调试就行了 五、解决IDEA 无法连接远程 Tomcat如果提示无法连接，如下图 则检查以上配置是否正确，确认都正确的话，可能是服务器防火墙的原因导致，需要关闭防火墙。 如果服务器是阿里云的，还需要配置 ECS 服务器的安全组，开放 5005 远程调试端口。 详见第八条 六、解决断点不生效注意： 启动 Remote 的 Debug 之后，需要等待断点变成如下图时，再去访问接口；如果断点中间没有出现对号，说明断点还未生效，需要等待一会儿。 如果有时候死活也没法断点，就把服务器上的 Tomcat 重启一下试试！ 七、其他问题因为在 catalina.sh 中添加了远程调试的参数，导致使用 tomcat 的 bin/shutdown.sh 时，可能无法完全停止服务器。可以直接 kill 掉服务器，如果不再进行远程调试，最后把远程调试参数注释掉重启。 八、阿里云 ECS 安全组设置进入 ECS 控制台，点击安全组 然后选择 创建安全组 然后配置该安全组的规则 点击公网入标签，再点击右上角 添加安全组规则 授权对象 填写你的 ip 地址即可。 添加完安全组之后，进入 ECS 实例 选择右侧 加入安全组，选择加入刚才添加的安全组即可。","tags":[{"name":"Tomcat远程调试","slug":"Tomcat远程调试","permalink":"http://nullpointer.pw/tags/Tomcat远程调试/"},{"name":"解决IDEA无法远程调试Tomcat","slug":"解决IDEA无法远程调试Tomcat","permalink":"http://nullpointer.pw/tags/解决IDEA无法远程调试Tomcat/"},{"name":"解决IDEA远程调试断点不生效","slug":"解决IDEA远程调试断点不生效","permalink":"http://nullpointer.pw/tags/解决IDEA远程调试断点不生效/"}]},{"title":"微信公众号强制关注","date":"2018-07-07T10:36:59.000Z","path":"微信公众号强制关注的实现.html","text":"前言微信官方规定是不允许诱导关注的，所以强制关注就更加不可能的，但是道高一丈魔高一丈，总是有各种对策来绕过这些规定。 比如笔者所在的公司行业，有公司开发功能进行推广小说，访问推广链接后，阅读到某一章时，弹出二维码才能继续阅读，其实这里就是变相的微信公众号强制关注了。关注他的二维码之后，公众号就会自动发送消息，给出继续阅读的链接地址，读者点进去之后才可以接着读。 其实这个功能我之前也写过一篇博文，基于微信事件二维码推广，实现的功能就和上述一致，但是，由于使用到了微信事件二维码，所以只能使用微信的服务号，而普通的订阅号没有这个接口权限。那么订阅号怎么实现呢？也是被我生生想出了个法子，来达到类似服务号使用参数二维码的效果，而且服务号与订阅号通用。 背景公司新开发了一个分销平台项目，可以让商户即个人公众号主拥有自己的小说网站，收入与公司进行分成，从而达到了双收的效果。 商户为了增加粉丝数量，于是我们开发了上述需求的功能。但是这些商户基本上都用的是订阅号，前文也提到，订阅号无法使用参数二维码接口，所以只能另辟蹊径。 流程图 流程梳理首先我们在后台配置微信二维码，然后针对某一本小说，指定开始章节与显示二维码章节的 ID，生成推广链接，如下格式：http://m.domain.com/link?code=7788，以下简称 A 链接。然后商户发推文，设置阅读原文的链接地址为 A 链接。 当用户访问这个 A 链接的时候，跳转阅读页，待其阅读到二维码章节时，将二维码章节的下一章节 ID 写入 Cookie 之中，同时弹出二维码提示关注。 用户关注，触发了微信的 SUBSCRIBE 事件，给用户发送继续阅读的消息，如下格式：http://m.domain.com/reading，以下简称 B 链接，该链接是固定不变的。 如果用户点击了该消息，则打开了 B 链接，读取之前存储的 Cookie 信息，得到继续阅读的章节 ID，然后组装阅读 URL，通过 redirect 跳转即可。（如果用户是正常关注，也会发送消息，如果点击，找不到该 Cookie，跳转首页即可） 这是针对公司需求开发的，其实，主要的地方在于参数的传递。除了关注事件，微信还有许多其他事件可供使用，比如菜单点击事件，至于怎么用，就看你的了。","tags":[{"name":"微信公众号强制关注","slug":"微信公众号强制关注","permalink":"http://nullpointer.pw/tags/微信公众号强制关注/"},{"name":"订阅号参数二维码","slug":"订阅号参数二维码","permalink":"http://nullpointer.pw/tags/订阅号参数二维码/"}]},{"title":"Java实现排行榜基于Redis","date":"2018-06-10T01:45:52.000Z","path":"Java实现排行榜基于Redis.html","text":"前言排行榜作为互联网应用中几乎必不可少的一个元素，其能够勾起人类自身对比的欲望，从而来增加商品的销量。排行榜的实现方式基本大同小异，大部分都基于 Redis 的有序集合 sorted set 来实现。不久前，负责开发一个活动，就有排行榜这个需求，笔者也使用 Redis 进行了实现。本文通过了商品销售排行榜这一模型，来进行演示。 需求 按照商品销量进行排行 可以获得指定商品的排名 显示实时销售动态情况 需求分析分析需求，以上这些都可以通过 Redis 的有序集合相关命令进行实现，首先看一下使用到的具体 Redis 命令。12345678910111213141516171819202122232425262728293031323334redis&gt; ZADD bangdan 1 &quot;one&quot;(integer) 1# 对有序集合中指定成员的分数加上增量redis&gt; zadd bangdan 1 &quot;one&quot; 4 &quot;three&quot; 3 &quot;two&quot;(integer) 2# 将一个或多个成员以及分数加入到有序集合中redis&gt; zrange bangdan 0 11) &quot;one&quot;2) &quot;three&quot;# 按照 score 升序排列 ,取出前两名redis&gt; zscore bangdan three&quot;4&quot;# 获得榜单中指定元素的scoreredis&gt; zrank bangdan one(integer) 0# 在升序榜中的名次 第一返回0# 第三个需求需要使用 Redis 的 list 来进行实现redis&gt; LPUSH dynamic abc(integer) 1# 向队列左侧头部 push 数据redis&gt; LINDEX dynamic 0&quot;abc&quot;# 通过索引获取列表中的元素redis&gt; LTRIM dynamic 0 2&quot;abc&quot;# 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除 排行榜预览按照需求开发，最后的效果如下： 以下通过 Java 代码实现。 通过 Java 实现排行榜引入依赖项目中使用到了 Redis，因此需要引入相关依赖，为了简明演示，这里没有使用 JedisPool。12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt;&lt;/dependency&gt; 开发逻辑 页面上点击一次购买按钮，则对该手机的销量加 1，同时将销售动态添加到队列当中 12jedis.zincrby(Constants.SALES_LIST, 1, String.valueOf(phoneId));jedis.lpush(Constants.BUY_DYNAMIC, msg); 获得排行榜 12// 按照scope升序排名，取出前五jedis.zrevrangeWithScores(Constants.SALES_LIST, 0, 4); 获得指定手机的排名情况 1jedis.zrevrank(Constants.SALES_LIST, String.valueOf(phoneId)); 获得销售动态，此处只取 3 条，同时队列只保存最新的 20 条动态 123456789101112131415List&lt;DynamicVO&gt; dynamicList = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 3; i++) &#123; String result = jedis.lindex(Constants.BUY_DYNAMIC, i); if (StringUtils.isEmpty(result)) &#123; break; &#125; String[] arr = result.split(Constants.separator); long time = Long.valueOf(arr[0]); String phone = arr[1]; DynamicVO vo = new DynamicVO(); vo.setPhone(phone); vo.setTime(StringUtil.showTime(new Date(time))); dynamicList.add(vo);&#125;jedis.ltrim(Constants.BUY_DYNAMIC, 0, 19); 因为排行榜这种实时性比较强的数据，更新比较快，个人觉得没有必要进行持久化，如果 Redis 的排行榜数据丢失，可以通过代码重新计算排行，通过 zadd 命令，重新添加到 Redis 中即可。 12345Map&lt;String, Double&gt; map = new HashMap&lt;&gt;(); map.put(\"1\", 4.0); map.put(\"2\", 2.0); map.put(\"3\", 3.0);jedis.zadd(Constants.SALES_LIST, map); 源码下载点我下载","tags":[{"name":"Redis排行榜","slug":"Redis排行榜","permalink":"http://nullpointer.pw/tags/Redis排行榜/"},{"name":"Java排行榜","slug":"Java排行榜","permalink":"http://nullpointer.pw/tags/Java排行榜/"}]},{"title":"Dom4j生成XML","date":"2018-06-04T08:50:29.000Z","path":"Dom4J生成XML.html","text":"前言在看本篇文章之前，先探讨一下常见的两种数据格式，一种是 JSON，另外一种就是 XML，在日常工作中不可避免地都会对这两种数据格式进行接触。 JSON 格式灵活，有大量的支持库，操作起来极为方便；而 XML 操作起来就不是很顺手了。 JSON 格式灵活的就会导致结构不够统一规范，而 XML 有了 XML Schema 以及 DTD 约束进行校验，也减少了数据格式不规范的问题。 对于以上这点，个人是深有体会，因为在工作中需要经常和其他公司的技术打交道。公司合作有内容输出以及内容引入两种，内容引入就是去抓取其他公司的授权接口内容，文档制定的数据格式是 JSON 的，但是可能对方参照我方文档进行接口开发时，会忽略掉一些事项，比如数据类型、时间格式等等，造成我方抓取之后，出现错误的情况； 而在和百度内容输出进行接口对接的时候，百度需要 XML 格式的数据，如果我方接口中的数据不符合对方的 XML 约束的话，对方就能马上停止抓取，反馈问题，由此避免错误情况的发生。 平时为了图方便，很多接口都使用 JSON，对于 XML 也比较生疏，但还是不可避免接触到使用，开发过微信公众号的都知道，微信接口基本都使用的是 XML。在开发了微信图文回复之后，需要组装 XML 发送给微信服务器，可能有人说，那用字符串拼装不也是可以的么，干嘛那么麻烦非要用 DOM4J 呢？确实公司中一些项目的微信支付，也使用的是字符串拼接 XML 的方式，但是多掌握一些技术也并不是什么坏事！ 环境准备首先引入 dom4j 依赖12345&lt;dependency&gt; &lt;groupId&gt;org.dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 需求接到需求，要求用户发送作品名称，自动图文回复搜索相关作品，微信的接口是这样的： 需要的 XML 文档格式如下： 123456789101112131415161718192021&lt;xml&gt; &lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;12345678&lt;/CreateTime&gt; &lt;MsgType&gt;&lt;![CDATA[news]]&gt;&lt;/MsgType&gt; &lt;ArticleCount&gt;2&lt;/ArticleCount&gt; &lt;Articles&gt; &lt;item&gt; &lt;Title&gt;&lt;![CDATA[title]]&gt;&lt;/Title&gt; &lt;Description&gt;&lt;![CDATA[description]]&gt;&lt;/Description&gt; &lt;PicUrl&gt;&lt;![CDATA[picurl]]&gt;&lt;/PicUrl&gt; &lt;Url&gt;&lt;![CDATA[url]]&gt;&lt;/Url&gt; &lt;/item&gt; &lt;item&gt; &lt;Title&gt;&lt;![CDATA[title]]&gt;&lt;/Title&gt; &lt;Description&gt;&lt;![CDATA[description]]&gt;&lt;/Description&gt; &lt;PicUrl&gt;&lt;![CDATA[picurl]]&gt;&lt;/PicUrl&gt; &lt;Url&gt;&lt;![CDATA[url]]&gt;&lt;/Url&gt; &lt;/item&gt; &lt;/Articles&gt;&lt;/xml&gt; 需求实现其实这个接口也就是组装数据，然后发送给微信就可以了。 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; // 1. 创建文档对象 Document doc = DocumentHelper.createDocument(); // 2. 新增根节点 Element rootXML = doc.addElement(\"xml\"); // 3. 向根节点添加子节点 rootXML.addElement(\"ToUserName\").addCDATA(\"toUser\"); rootXML.addElement(\"FromUserName\").addCDATA(\"fromUser\"); rootXML.addElement(\"CreateTime\").setText(\"12345678\"); rootXML.addElement(\"MsgType\").addCDATA(\"news\"); rootXML.addElement(\"ArticleCount\").setText(\"2\"); // 创建节点 Element subElement = DocumentHelper.createElement(\"Articles\"); for (int i = 0; i &lt; 2; i++) &#123; Element itemElement = DocumentHelper.createElement(\"item\"); itemElement.addElement(\"Title\").addCDATA(\"title\"); itemElement.addElement(\"Description\").addCDATA(\"description\"); itemElement.addElement(\"PicUrl\").addCDATA(\"picurl\"); itemElement.addElement(\"Url\").addCDATA(\"url\"); subElement.add(itemElement); &#125; // 向根节点追加节点 rootXML.add(subElement); // 4. 输出xml字符串 System.out.println(rootXML.asXML());&#125; 小结最近博客文章都是个人的总结性文章，没有涉及什么难的技术，希望对你有所帮助！","tags":[{"name":"XML 生成","slug":"XML-生成","permalink":"http://nullpointer.pw/tags/XML-生成/"},{"name":"Dom4j","slug":"Dom4j","permalink":"http://nullpointer.pw/tags/Dom4j/"}]},{"title":"jsonpath解析","date":"2018-05-19T09:46:15.000Z","path":"jsonpath解析.html","text":"前言在工作中，经常会遇到从一串 JSON 中提取一个或多个字段的情况，常用的做法就是将其反序列化为 JSONObject 对象，然后从对象中获取，如果是 JSONArray 就进行迭代获取，总之比较麻烦。可以使用 JsonPath 快速提取所需信息。 JSONPATH 简明语法 JsonPath 描述 $ 根节点 @ 当前节点 .or[] 子节点 .. 选择所有符合条件的节点 * 所有节点 [] 迭代器标示，如数组下标 [,] 支持迭代器中做多选 [start:end:step] 数组切片运算符 ?() 支持过滤操作 () 支持表达式计算 尝试 JsonPath 前提准备maven 工程引入 jsonpath 的依赖12345&lt;dependency&gt; &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt; &lt;artifactId&gt;json-path&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt;&lt;/dependency&gt; 有人说，FastJson 自带了 JSONPath，为什么不用要用这个呢？我只能回 FastJson 确实很好用，但是 FastJson 的 JsonPath 是真难用！ JsonPath 简单入门版给定一串 JSON 如下所示： 123456789101112131415161718192021222324252627282930313233343536&#123; \"store\": &#123; \"book\": [ &#123; \"category\": \"文学\", \"author\": \"曹雪芹\", \"title\": \"红楼梦\", \"price\": 47.20 &#125;, &#123; \"category\": \"心理\", \"author\": \"凯利·麦格尼格尔\", \"title\": \"自控力\", \"price\": 30.20 &#125;, &#123; \"category\": \"励志\", \"author\": \"史蒂芬·柯维\", \"title\": \"高效能人士的七个习惯\", \"isbn\": \"7515326395\", \"price\": 51 &#125;, &#123; \"category\": \"小说\", \"author\": \"毛姆\", \"title\": \"月亮与六便士\", \"isbn\": \"7533936027\", \"price\": 19.50 &#125; ], \"bicycle\": &#123; \"color\": \"red\", \"price\": 19.95 &#125; &#125;&#125; 需要按照条件提取信息123456789101112131415161718192021222324252627282930313233343536public class JsonPathExample &#123; public static void main(String[] args) throws IOException &#123; // 读取上面的那串 JSON File file = new File(JsonPathExample.class.getClassLoader().getResource(\"store.json\").getPath()); String storeJson = FileUtils.readFileToString(file); // 注意使用 JsonPath ，不要使用 FastJson 的 JSONPath // 输出第一本书的作者 println(JsonPath.read(storeJson, \"$.store.book[0].author\").toString()); // 输出结果： 曹雪芹 // 输出所有书的作者 println(JsonPath.read(storeJson, \"$.store.book[*].author\").toString()); // 输出结果： [\"曹雪芹\",\"凯利·麦格尼格尔\",\"史蒂芬·柯维\",\"毛姆\"] // 输出分类为文学的书信息 println(JsonPath.read(storeJson, \"$.store.book[?(@.category =='文学')]\").toString()); // 输出结果： [&#123;\"author\":\"曹雪芹\",\"price\":47.2,\"category\":\"文学\",\"title\":\"红楼梦\"&#125;] // 输出价格大于 50 的书 println(JsonPath.read(storeJson, \"$.store.book[?(@.price &gt; 50)]\").toString()); // 输出结果: [&#123;\"author\":\"史蒂芬·柯维\",\"price\":51,\"isbn\":\"7515326395\",\"category\":\"励志\",\"title\":\"高效能人士的七个习惯\"&#125;] // 输出 book[*] 中包含 isbn 的书 println(JsonPath.read(storeJson, \"$.store.book[?(@.isbn)]\").toString()); // 输出结果: [&#123;\"author\":\"史蒂芬·柯维\",\"price\":51,\"isbn\":\"7515326395\",\"category\":\"励志\",\"title\":\"高效能人士的七个习惯\"&#125;,&#123;\"author\":\"毛姆\",\"price\":19.5,\"isbn\":\"7533936027\",\"category\":\"小说\",\"title\":\"月亮与六便士\"&#125;] // 输出 json 中所有的 price println(JsonPath.read(storeJson, \"$..price\").toString()); // 输出结果: [19.95,47.2,30.2,51,19.5] &#125; private static void println(String str) &#123; System.out.println(str); &#125;&#125; 以上基本上，简单提取需要 JSON 中的信息便已经足够了，以下为扩展内容，也是我在工作中使用到的。 JsonPath 复杂结构版给定一串 JSON 如下所示12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; \"bookId\": 7333, \"volumeDetailList\": [ &#123; \"title\": \"卷一 恢弘世界\", \"volumeId\": 28585, \"chapterDetailList\": [ &#123; \"chapterId\": 11719110, \"free\": true, \"name\": \"第1章 上九天\", \"price\": 0, \"words\": 1678, \"contentId\": 2930434 &#125;, &#123; \"chapterId\": 1719111, \"free\": true, \"name\": \"第2章：揽月\", \"price\": 0, \"words\": 2390, \"contentId\": 2930444 &#125; ] &#125;, &#123; \"title\": \"卷二 在人家\", \"volumeId\": 285852, \"chapterDetailList\": [ &#123; \"chapterId\": 1719120, \"free\": false, \"name\": \"第3章：千年后之始\", \"price\": 19, \"words\": 2989, \"contentId\": 29540933 &#125;, &#123; \"chapterId\": 17133111, \"free\": false, \"name\": \"第4章：破冰而生\", \"price\": 30, \"words\": 3409, \"contentId\": 29540988 &#125; ] &#125; ]&#125; 读取需要的相关信息1234567891011121314151617181920212223242526public class JsonPathExample2 &#123; public static void main(String[] args) throws IOException &#123; // 用于读取上面的那串 JSON File file = new File(JsonPathExample2.class.getClassLoader().getResource(\"chapterlist.json\").getPath()); String chapterlistJson = FileUtils.readFileToString(file); // 得到书的所有卷名称 println(JsonPath.read(chapterlistJson, \"$..title\").toString()); // 输出结果: [\"卷一 恢弘世界\",\"卷二 在人家\"] // 得到书的所有章节名称 println(JsonPath.read(chapterlistJson, \"$..name\").toString()); // 输出结果: [\"第1章 上九天\",\"第2章：揽月\",\"第3章：千年后之始\",\"第4章：破冰而生\"] // 上面两个非常简单，下面这个需要注意 volumeDetailList 与 chapterDetailList 都是数组，且是数组嵌套 // 得到 chapterId 为 17133111 的 contentId println(JsonPath.read(chapterlistJson, \"$.volumeDetailList[*].chapterDetailList[?(@.chapterId == '17133111')].contentId\").toString()); // 简化写法 println(JsonPath.read(chapterlistJson, \"$..[?(@.chapterId == '17133111')].contentId\").toString()); // 输出结果: 29540988 &#125; private static void println(String str) &#123; System.out.println(str); &#125;&#125; 常用JsonPath在线解析工具网站 http://jsonpath.com/ http://jsonpath.herokuapp.com/ 这两个网站都可以在线写 JsonPath 语句，可以用来校验写的是否正确而不用去跑代码程序，比较方便 参考文章 http://goessner.net/articles/JsonPath/ https://www.cnblogs.com/weilunhui/p/3857366.html 小结在工作中使用到的时候，也不熟悉 JsonPath 的语法，写出了比较复杂的 JsonPath，但是在写这篇博客的时候，又写了一般，发现了有很多自己写的 JsonPath 语句是可以精简的，比如倒数第二条 JsonPath 语句可以精简为它下面那句。 在输出的时候，可以发现自己认知中不足之处，写博客还是能给自己带来很多好处的，希望自己能够笔耕不息~","tags":[{"name":"JSONPath解析","slug":"JSONPath解析","permalink":"http://nullpointer.pw/tags/JSONPath解析/"},{"name":"JSONPath在线解析","slug":"JSONPath在线解析","permalink":"http://nullpointer.pw/tags/JSONPath在线解析/"},{"name":"JSONPath多层嵌套解析","slug":"JSONPath多层嵌套解析","permalink":"http://nullpointer.pw/tags/JSONPath多层嵌套解析/"}]},{"title":"解决WebMagic抓HTTPS时出现SSLException","date":"2018-05-12T07:03:47.000Z","path":"解决WebMagic抓HTTPS时出现SSLException.html","text":"前言在今年二月份在项目中引入了 WebMagic 技术，用来抓取合作方的书籍，详见之前文章：WebMagic之爬虫监控，这两天新接入了一个合作商，对方接口采取的是 HTTPS 协议，而以前合作商的接口全都是 HTTP 的，在接入这家合作商的时候，发现了问题，只要是 HTTPS 的 URL 全部无法爬取。 一、问题复现在 WebMagic-core 包中，us.codecraft.webmagic.processor.example.GithubRepoPageProcessor 这个包下面是作者放的示例代码，拷贝这个类代码到新建的类中，右键运行这个类，运行后控制台报出错误：[WARN ] 2018-05-12 13:28:11,828 download page https://github.com/code4craft error [us.codecraft.webmagic.downloader.HttpClientDownloader.download(HttpClientDownloader.java:91)] 进入这个类，定位到 HttpClientDownloader 源代码 85 行12345678910111213141516public Page download(Request request, Task task) &#123; ...... try &#123; httpResponse = httpClient.execute(requestContext.getHttpUriRequest(), requestContext.getHttpClientContext()); page = handleResponse(request, task.getSite().getCharset(), httpResponse, task); onSuccess(request); logger.info(\"downloading page success &#123;&#125;\", request.getUrl()); return page; &#125; catch (IOException e) &#123; logger.warn(\"download page &#123;&#125; error\", request.getUrl(), e); onError(request); return page; &#125; finally &#123; ... &#125;&#125; 在 85 行此处断点 Debug 运行，发现此行抛出了 SSLException 异常。异常的内容是：javax.net.ssl.SSLException: Received fatal alert: protocol_version 二、解决问题方法（一）出现异常后，通过 Google 搜索了一下，找到了 WebMagic 作者黄大的解决方式，详见 Https下无法抓取只支持TLS1.2的站点 于是按照黄大说的方式来做 首先复制源码中的 HttpClientGenerator 与 HttpClientDownloader 到自己的项目中。 修改 HttpClientGenerator 的代码，只需要修改 buildSSLConnectionSocketFactory 这个方法为如下即可。 1234567891011121314 ...private SSLConnectionSocketFactory buildSSLConnectionSocketFactory() &#123; try &#123; return new SSLConnectionSocketFactory(createIgnoreVerifySSL(), new String[]&#123;\"SSLv3\", \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\"&#125;, null, new DefaultHostnameVerifier()); // 优先绕过安全证书 &#125; catch (KeyManagementException e) &#123; logger.error(\"ssl connection fail\", e); &#125; catch (NoSuchAlgorithmException e) &#123; logger.error(\"ssl connection fail\", e); &#125; return SSLConnectionSocketFactory.getSocketFactory();&#125; ... 注意： DefaultHostnameVerifier 需要使用 org.apache.http.conn.ssl.DefaultHostnameVerifier，而不要使用 sun.net.www.protocol.https.DefaultHostnameVerifier 修改 HttpClientDownloader 中引用的 HttpClientGenerator 为你修改后的类。 设置爬虫 Spider 的 Downloader 为 你修改的 HttpClientDownloader。 1Spider.create(new GithubRepoPageProcessor()).setDownloader(new HttpClientDownloader()).addUrl(\"https://github.com/code4craft\").thread(5).run(); 运行后，发现控制台已经可以正常输出所抓取网页的内容了 三、解决问题方法（二）既然对方接口地址是 HTTPS 的，那我们访问他的 HTTP 接口不就没有这个问题了么？但是事实是，访问对方 HTTP 接口时，会自动跳转到 HTTPS 网址，所以这条路行不通。这时我忽然想到，之前使用 Nginx 解决前端跨域的方法，使用我方的 HTTP 域名，反向代理到对方的 HTTPS 网址是不是就解决了问题呢？于是对 Nginx 进行配置。 123456789server &#123; listen 80; server_name partner.domain.com; location / &#123; proxy_set_header Host github.com; proxy_pass https://github.com/; &#125;&#125; 启动/重启 Nginx，将爬虫的地址由 https://github.com/code4craft 修改为 http://partner.domain.com/code4craft，使用 WebMagic 默认的 Downloader，启动爬虫 public class GithubRepoPageProcessor implements PageProcessor { private Site site = Site.me().setRetryTimes(3).setSleepTime(1000).setTimeOut(10000); @Override public void process(Page page) { System.out.println(page.getRawText()); } @Override public Site getSite() { return site; } public static void main(String[] args) { Spider.create(new GithubRepoPageProcessor()).addUrl(\"http://partner.domain.com/code4craft\").thread(5).run(); } } 发现控制台成功打印出了页面的内容，测试中，如果页面，页面的其他请求还是会错误，只有第一个页面是正确的，因此此方法只适合与 Ajax 类似的接口，我们是这样处理抓取 Https 问题的，因为第一种方法对于我们 JDK1.6 项目无效，第一种方法在 JDK 1.8 测试时可以正常抓取的。 后记如果第一种方法解决不了你的问题，那就跟我一样弄一个临时域名反向代理吧，Nginx 在这方面还是蛮好用的。","tags":[{"name":"WebMagic出现SSLException","slug":"WebMagic出现SSLException","permalink":"http://nullpointer.pw/tags/WebMagic出现SSLException/"},{"name":"爬虫异常","slug":"爬虫异常","permalink":"http://nullpointer.pw/tags/爬虫异常/"},{"name":"反向代理到其他域名","slug":"反向代理到其他域名","permalink":"http://nullpointer.pw/tags/反向代理到其他域名/"}]},{"title":"Charles抓包实战详解","date":"2018-04-25T14:19:22.000Z","path":"Charles抓包实战详解.html","text":"前言通过上一篇文章，想必你已经掌握了如何正确安装抓包神器 Charles，如果还是抓不了包，可以再看看。 今天要做是抓包实战，因为我在做网络文学的公司就职，所以就拿网络文学的 APP 掌阅开刀好了，以下进入抓包实战，体会 Charles 的强大之处！。 掌握了下面的抓包技巧，对于某些校验并不是十分严格的 APP 来说，破解他们的接口实在是轻而易举。比如修改个 isVip 参数为 true，就变身超级 VIP等；之前用这套抓包技巧，找了一个网校随便修改，发现要价上千至上万的课程竟然可以跳过校验直接被我播放下载，也是醉了。 希望大家掌握了抓包不要干违法的事情，本文目的在于让技术人员了解漏洞，加固校验，不被他人有机可乘，以免造成公司损失。 测试环境环境： Windows Charles iphone 7 掌阅 APP 7.4.2 前提：手机已经连接 Charles 代理，并可以抓取 HTTPS 数据包。 一、Charles 的断点功能 Breakpoints 实战首先登录掌阅 APP ，清空Charles 的抓包列表，以防止干扰实验，然后进入 APP 的个人中心，可以看到图中圈出的数值，这里我们对其数值进行修改。 此时看到 Charles 的请求列表中出现了 APP 的请求。 通过简单分析，可以知道个人中心的那些数据是从这个请求中获取到的，那么我们便修改这个请求，右键这个请求，选择 Breakpoints，这样就给这个请求加上了断点。此时切换 App 到 书架再切换回个人中心，会再次发送该请求，Charles 会弹出请求的断点，因为这里不需要修改请求参数，直接放行即可，选择 Execute；放行后，会再次弹出断点，此时就是响应返回的断点了，选择 Edit Response， 底部选择 JSON text 可以修改返回值， 我这里把 2 修改成了 2222，修改完之后，点击Execute，回到 App 发现已经更改了。 测试中发现，掌阅 App 的请求时间如果过长，修改之后也会不生效。这就需要使用 Charles 的下一个牛 X 的功能了，Map 映射，让 Charles 自动返回我们想要的返回值。 二、Charles 的映射功能 Map 实战上一个实验进行的时候，如果请求的响应时间过长，掌阅的 App 就会抛弃本次请求的结果，不对数据进行修改，因此我们需要更快的去修改返回值。 以前用 Fiddler 对于这种响应时间有要求的 App，反正我是束手无策的，APP 总是提示网络异常，响应超时等。如果是更改一两个返回值还可以，如果是多个，是真没辙了，所以 Charles 的 Map 功能是很强大的。 重复上一步，抓到个人中心的数据包，就是这个请求。 右键选择 Save Response 到一个文件当中去。 再次右键，选择该请求，为了不干扰实验，取消勾选上一步中的Breakpoints，然后再右键该请求，选择最下方 Map Local，选择刚才保存的文件。 注意这里的 Query，因为掌阅 APP 在请求时，附带了签名时间戳等参数，这里设置为 * 之后，就会忽略这些参数。设置完成后，修改刚才保存的文件，使用 NotePad++ 等编辑器打卡，修改里面的 JSON 内容。我改了部分数据，重新进入个人中心，查看 Charles 的请求内容，发现已经修改了 同时看 App 的个人中心也已经修改了，可以改很大的值，然后向不懂的人炫耀炫耀了哈哈哈~ 三、Charles 的 Rewrite 实战Rewrite 是对于 map 功能的一个补充，可以设置较为复杂的规则，下面演示替换 Body 内的用户名称 选择菜单栏 Tools -&gt; Rewrite，选择开启 Rewrite, 并添加一条规则。替换 Body 体内的 We Jan 为 CCTV APP再次发起请求，响应如下 参考其实日常中，我们需要用到的地方基本以上三个都可以包含到了，其他深入学习的，可以参考下面的链接。 http://www.heyuan110.com/2015/08/15/Charles%E8%BE%85%E5%8A%A9%E8%B0%83%E8%AF%95%E6%8E%A5%E5%8F%A3/ http://blog.devtang.com/2015/11/14/charles-introduction/","tags":[{"name":"Charles抓包","slug":"Charles抓包","permalink":"http://nullpointer.pw/tags/Charles抓包/"},{"name":"抓包破解","slug":"抓包破解","permalink":"http://nullpointer.pw/tags/抓包破解/"},{"name":"抓包实战","slug":"抓包实战","permalink":"http://nullpointer.pw/tags/抓包实战/"}]},{"title":"Charles抓包之HTTPS抓包配置","date":"2018-04-15T12:36:40.000Z","path":"Charles抓包之HTTPS抓包配置.html","text":"前言由于工作中经常需要配置客户端开发人员对接接口，有时候对接地不太顺利，因此需要经常性地对公司 APP 进行抓包看请求，找出具体的原因。 在公司中开发使用的 Windows 台式电脑，抓包工具选择了 Fiddler，这个工具刚开始学会抓包的时候，觉得真乃神器！在经过几次开发电脑升级之后，重新安装 Fiddler 后，和之前的设置相同，但是死活抓不了包，或者抓不了 Https 的包， 总之总会出现各种奇怪的问题，如果出了问题，简直让人抓狂！ 去年年底，给自己换了一台 MBP，日常学习中也需要用到抓包，为了和公司中的工具保持一致，想下载 Fiddler 的 Mac 版，结果并没有。于是下载了 Charles (花瓶)，用了花瓶之后，发现其比 Fiddler 更加好用~ 在一次受够了 Fiddler 的莫名其妙地抓不了 Https，怒卸载之，寻寻觅觅，找到了 Win 版的 Charles (之前还以为没有 Win 版呢)，经过简单配置，开始了愉快的抓包~ 抓包准备之 Charles 下载以及激活Charles 软件下载 下载安装完，在开始菜单中找到 Charles 打开，提示有 30 天的试用期。如果 30 天到期的话，可以选择购买一个 license ，30 刀。如果不想买，可以参考此文继续试用，Charles Proxy使用笔记（文内有好礼）。选择 Help-Registered 填入 license 即可激活。 抓包准备之 Charles 配置 HTTPS 抓包基础配置如今 Https 大行其道，苹果更是强制要求 APP 通信必须要使用 Https 协议，如果只是抓普通的 HTTP 请求，无需复杂配置即可抓包，这也表明了 HTTP 的安全性很低。 Charles 默认不对 HTTPS 的请求开启抓包，所以，需要先开启所有 HTTPS 请求的抓包。 进入菜单 Proxy-SSL Proxying Settings选择开启 SSL Proxy，并 Add 一条 Location 记录，Host 填写 *，代表匹配所有网址，端口 Port，填写443，即 HTTPS 端口。 选择保存。 基础配置到此就完成了，以下本文按照两方面来描述，Charles 抓 HTTPS。 Charles 抓 Windows 上的 HTTPS 请求进入 Help 中，选择 SSL Proxying 设置，安装根证书 注意这一步一定要将证书存储到 受信任的根证书颁发机构，否则抓包 HTTPS 会提示证书不受信任！！！嗯，搞了好久，血一般的教训！ 安装期间会有安全警告，选择是即可。 访问 https://www.baidu.com后，查看 Charles，已经可以看到 Https 请求下的数据了。 Charles 抓 APP 的 HTTPS 请求想要 Charles 抓 HTTPS 的数据包，需要设置代理并安装 Charles 的中间 SSL 证书。 选择 Help-SSL Proxying 点击会弹出对话框如下 按照提示，将手机连入电脑同一个网络，WIFI 处设置代理，代理的地址以及端口按照对话框中进行设置。 设置成功返回之后，此时电脑上 Charles 会自动弹出提醒，提示是否允许请求代理通过，选择 Allow 允许即可。 连上代理之后，使用系统自带浏览器( iPhone 使用 Safari)访问地址 chls.pro/ssl，如果是 iPhone 会自动弹出安装描述文件，选择安装输入密码后无视警告安装即可。因为手边没有安卓设备，不过流程应该类似，就是安装一个 CA 证书文件。 手机访问 https://m.baidu.com，查看 Charles ，发现此时已经可以拿到 HTTPS 数据内容了，如果没设置成功，里面都将会是 unknown。 常见问题 开启 SSL Proxy 代理后，提示证书不受信任或者无效，PC 端一般都是因为证书放错了位置。 在 IOS11上测试时，抓包显示unknown 需要进入通用-关于本机-证书信任设置，将新增的证书进行开启操作。 1. 小米系统的手机安装证书方式比较繁琐，需要进入开发者选项中，在信用证书中进行安装（Fiddler安装证书是这样的，Charles 应该也差不多） 最后说几句善用搜索引擎，因为你遇到的问题肯定有别人都遇到过了，如果你搜不到，肯定是你的搜索姿势不对~ 实在有问题解决不了，可以找我哟! 给我写信","tags":[{"name":"Charles抓包","slug":"Charles抓包","permalink":"http://nullpointer.pw/tags/Charles抓包/"},{"name":"HTTPS抓包","slug":"HTTPS抓包","permalink":"http://nullpointer.pw/tags/HTTPS抓包/"},{"name":"Windows抓包","slug":"Windows抓包","permalink":"http://nullpointer.pw/tags/Windows抓包/"},{"name":"抓包显示unknown","slug":"抓包显示unknown","permalink":"http://nullpointer.pw/tags/抓包显示unknown/"}]},{"title":"微信域名防止屏蔽","date":"2018-03-27T13:11:17.000Z","path":"微信域名防止屏蔽.html","text":"前言这几日，由于遭到他人的恶意投诉举报，导致了微信中阅读业务的根域名被屏蔽，提示如图 由于我司是做网络文学的，作者的内容中不可避免地会出现些微露骨的内容，但是这些内容是经过了编辑审核之后才公开的，也不至于太露骨，但举报就被封禁了。 公司的用户有很多是通过微信公众号进行阅读，这一封，就许多用户过来反馈了。 切换域名之痛既然根域名被封了，就只能切换域名了，因为之前代码中有不少的地方域名写死了，没有提取成公共变量，改了好久，才替换成了新的域名。 眼看已经修改完了，线上也正常了，都已经晚上9点多，准备下班回家了。就这时候，群里又反馈网站进不去了，打开一开，又**被封了，当时的心情真是想问候一下恶意举报人的全家了！ 没法子，继续换域名，还好之前提前备案了几个域名，没想到现在全用上了。 有了上一次的经验，现在修改起来也快多了，修改上线完事都已经 10 点多了，恶意举报的人这时候也消停了下来没有继续投诉了，我们也终于可以下班，害的我们大周六的过来加了一天的班！ 第二天，也就是周日。群里又反应被封了，真是崩溃。最开始的域名被微信封禁后，公司第一时间进行了申诉，此时收到通知，说检测后没有违规内容，就进行了解封。可能是别人恶意利用了微信的封禁系统，此时觉得微信的这套系统做的真烂！于是我们开发人员又是一番替换，把现在被封的域名切换了回去。 经过这几波操作，这两天网站的收入骤减，那帮使坏的人是趁你病要你命的节奏！ 痛定思过总结了一下这次事件的教训，主要的原因就是因为小说的内容页被频繁恶意举报导致域名被封。 这就导致了即使换了域名，比如支付，登录都受到了影响，而且微信支付限制了每个月修改支付域名的次数，不得已我们只好换了另外一个服务号才重新打通了支付。 接下来需要做的就是，将可能还会被举报的小说内容页的域名拆分出去，不和网站主要域名一样。 比如首页、个人中心是 m.aaa.com, 将内容页的域名修改成 m.bbb.com。 我们的阅读页 URL 地址为 https://m.aaa.com/book/1010545066/385773672，则在 nginx 匹配到 ^/book/([0-9]+)/([0-9]+) 就进行跳转到 http://m.bbb.com/book/1010545066/385773672，于是，修改 nginx 的配置，对 URL 进行匹配。 1234567891011121314151617181920212223242526272829server &#123; listen 443; server_name m.aaa.com; # ... set $temp_content_host &quot;http://m.bbb.com&quot;; location ~ ^/book/([0-9]+)/([0-9]+) &#123; # If WeChat browser, jump to temporary domain name if ($http_user_agent ~* (Micromessenger) ) &#123; return 301 $temp_content_host$request_uri; &#125; proxy_pass http://domainAserver; &#125; location / &#123; proxy_pass http://domainAserver; &#125;&#125;server &#123; listen 80; server_name m.bbb.com; # ... location / &#123; proxy_pass http://domainAserver; &#125;&#125; 因为只有在微信浏览器内部，才存在被封禁的风险，而在其他浏览器内没有，一旦被微信封禁，内容页的域名需要一起更换，这样可能会不利于搜索引擎的收录，因此加了一个判断浏览器 UA，微信的 UA 是 Micromessenger 如果是在微信浏览器内部，nginx 才 301 跳转到 m.bbb.com，如果不是，则保持之前的域名不变。 这样就解决了问题，一旦 m.bbb.com 域名被封禁了，就是 aaa 域名的 nginx 修改 $temp_content_host 这个变量的值为新的域名然后 reload 一下 nginx 即可。 总结可能是公司战略上的问题，太过于依赖微信的公众号，在微信的生态中，微信想弄死你轻而易举，你却无可奈何，尽量把用户引导到客户端上才是上上策。 参考 http://www.ttlsa.com/nginx/how-to-block-user-agents-using-nginx/ http://lzl124631x.github.io/2016/04/08/check-wechat-user-agent.html","tags":[{"name":"微信域名封禁","slug":"微信域名封禁","permalink":"http://nullpointer.pw/tags/微信域名封禁/"}]},{"title":"微信分享踩坑之旅","date":"2018-03-17T08:32:52.000Z","path":"微信分享踩坑之旅.html","text":"前言三月伊始，此月任务重心在于用户的拉新与传播，自然也少不了新增加些许需求。由我负责用户分享传播的需求，主要任务就是在于微信分享。在做之前觉得这个很简单，就几个接口的事情罢，实际上做起来，却是不如意料那般简单，踩坑颇多。 在做这个分享需求之前，和同事们也已经讨论过大致的流程。无非常用的手法，针对用户生成唯一二维码，通过微信分享出去，然后其他人注册填写邀请码获取奖励。 在填写邀请码这里，我参考了一些主流 APP 的做法。正常是复制邀请码然后用户找到入口粘贴邀请码并进行激活，但是发现有个别的 APP 体验很好，可以做到复制邀请码后，直接抵达 APP（网易蜗牛读书） 内部 WebView 中自动填写邀请码。 在对这个 APP 抓包分析之后发现了其中的奥妙所在，这是他们分享页面的地址： https://du.163.com/static/wechatFallback.html?openurl=nesnailreader://webview?url=https://du.163.com/invite?code=EPPEG&amp;inside=true&amp;downloadurl=https://du.163.com/download 原来他们使用了一种叫做 schema 的技术，可以直接在浏览器中打开 APP。于是开始了模仿之旅。 知道了其他 APP 的做法后，便询问了客户端的同事，能不能实现同样的效果，他们都是说以前没有这样做过，但是可以做。协调了前端与客户端同事，共同开发这个看起来挺简单的功能需求。客户端同事都很给力，虽然出现了一些小插曲，但很快便调通了功能，剩下了 WAP 版本微信分享，由于之前都没有做过，踩了不少的坑，分享这次踩坑，这也是本文重心所在。 坑一后台代码开发不必多说，跟着文档开发，因为需要获取 微信公众号的 AccessToken 所有需要在微信后天白名单处添加本机 IP 。 在微信开发者工具中，调试微信 JSSDK 时，发现 JS-SDK 提示签名错误｛errmsg：config: invalid signature｝，但是通过 微信 JS 接口签名校验工具 提交相关参数进行校验后，发现程序生成的和微信网页生成的签名是一样的,但是这里就一直提示是无效的签名。 我查看了 JS-SDK 下注册 config 信息处的输入参数，发现本该提交五个参数，但是只提交了四个，漏提交了nonceStr 参数，如果你发现程序生成的和微信网页生成的签名是一样的，但是提示无效签名，务必检查是否是这里的问题。还有提交了五个参数，但是参数的大小写不正确的的，比如 nonceStr 写成了 nonceStr 也是不行的。可以对比下图进行校验。 坑二坑一种提到了 AccessToken，使用 JSSDK 时还需要另外一个 ticket，这二者都有超时时间的限制，如果过期了，那么 config 是无论如何也不会初始化成功的。 坑三踩了前两个坑之外，其实 WAP 版的开发也都差不多调通了，但是分享之后，分享人进入分享的链接后，二次分享时，便又出现了无效签名的情况，仔细排查之后，发现是分享后，微信会在分享的链接后面自动加上了一些参数，正是因为这些参数的存在，导致签名失败。 刚开始没有细看文档，因此分享 URL 的地址就没有进行动态传递到后台进行签名，所以导致了二次分享出现了问题，于是修改为了动态传递当前 URL 到后台进行签名，才解决了这个问题。 需要注意的是，如果当前的 URL 中包含了中文，需要进行 encodeURIComponent 后传递，再在后台进行 URL 解码处理。另外编码之后最好再将参数中的 &amp; 临时替换为 %26，后台再替换为 &amp;，如果直接传到后台，存在多个参数的情况下，后台将只会接收到第一个参数。 小插曲 安卓客户端进行开发的时候，出现了很奇怪的状况，就是在 WebView 里面无法获取当前 APP 登录的用户信息，即 WebView 发送请求到服务器，发现获取不到 Cookie 中登录用户信息的值，而 IOS 客户端便没有出现这种情况。 之前安卓在做签到功能的时候，也是使用的 WebView，负责签到后台开发的同事也遇到了这种情况，当时没有彻底解决这种问题，使用了一种折中的方法来实现，让安卓客户端传递一串包含用户信息的加密串参数，后台解析这个参数判断用户。 谷歌查找这种情况之后，发现安卓需要将 Cookie 同步到 WebView 中，询问安卓同事之后，他说同步过了啊，这就很奇怪了！ 继续万能谷歌查找，然后看了一下同事的客户端代码，发现他同步 Cookie 到 WebView 部分的代码并没有设置 Cookie 的 domain 与 path，而后台获取登录用户信息时，只去获取指定域名指定路径下的 Cookie 值，自然是获取不到 WebView 里登录用户信息的值了。 知道了问题，解决起来就简单多了，只要将需要同步的每一条 Cookie 信息设置 Domain 与 Path 即可，后台也就可以拿到同步的 Cookie 信息了。 1234public static boolean syncCookie(String url, String cookie) &#123; cookie += \";Max-Age=3600\" + \";Domain=.cctv.com\" + \";Path = /\" cookieManager.setCookie(url, cookie);&#125;","tags":[{"name":"微信分享","slug":"微信分享","permalink":"http://nullpointer.pw/tags/微信分享/"},{"name":"WebView同步Cookie问题","slug":"WebView同步Cookie问题","permalink":"http://nullpointer.pw/tags/WebView同步Cookie问题/"}]},{"title":"SpringBoot与WebSocket集成","date":"2018-02-28T14:17:09.000Z","path":"SpringBoot与WebSocket集成.html","text":"前言目前，公司的项目部署方式还是使用的原始的方式，通过手动运行脚本去部署。每次全部服务器部署一遍，需要耗费的时间很长，如果遇到错误，也不能立刻进行回滚，只能将错误解决提交或者将备份代码进行还原。 在持续集成大行其道的今天，这种原始的方式实在是不应该再使用了！在调研了 Jekins 等开源持续集成工具之后，发现对于我们公司而言，比如 Jekins 的上手难度还是较高的，配置也比较复杂繁琐，也就没有采用。 于是想着自己能不能也搞一个简化版适用于公司的自动化部署系统呢？于是在开发的过程中，遇到了本文的问题，为了实时展示服务器命令输出情况，我使用了 WebSocket 技术来实现，同时使用 SpringBoot 框架进行快速搭建项目，该部署系统目前未开发完成，姑且作为一个练手项目。 WebSocket 介绍对于 WebSocket 就不多说什么了，直接引用维基百科的解释。 WebSocket是HTML5开始提供的一种浏览器与服务器间进行全双工通讯的网络技术。 WebSocket通信协议于2011年被IETF定为标准RFC 6455，WebSocketAPI被W3C定为标准。 在WebSocket API中，浏览器和服务器只需要要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。 易于理解的版本，可以看看阮一峰的 WebSocket 教程 下面讲解 WebSocket 与 SpringBoot 的集成使用。 服务端环境准备 SpringBoot 项目引入 WebSocket 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt;&lt;/dependency&gt; 新建 WebSocketHandler， 用于处理请求，以及向客户端发送消息 1234567891011121314151617181920212223242526272829303132@Componentpublic class WebSocketHandler extends AbstractWebSocketHandler &#123; private static CopyOnWriteArraySet&lt;WebSocketSession&gt; webSocketSet = new CopyOnWriteArraySet&lt;&gt;(); @Override public void afterConnectionEstablished(WebSocketSession session) throws Exception &#123; // 接收到WebSocket会触发 System.out.println(session.getUri().getQuery()); webSocketSet.add(session); session.sendMessage(new TextMessage(\"欢迎！！！\")); &#125; @Override public void afterConnectionClosed(WebSocketSession session, CloseStatus closeStatus) throws Exception &#123; // 关闭WebSocket会触发 System.out.println(\"关闭WebSocket会触发\"); &#125; @Override public void handleTransportError(WebSocketSession session, Throwable exception) throws Exception &#123; exception.printStackTrace(); &#125; /** * 广播消息 */ public void sendMessage() throws IOException &#123; for (WebSocketSession session : webSocketSet) &#123; session.sendMessage(new TextMessage(\"欢迎你来啦！\")); &#125; &#125;&#125; 新建 Config 类，用于接收 WebSocket 请求 123456789101112@Configuration@EnableWebSocketpublic class WebSocketConfig implements WebSocketConfigurer &#123; @Autowired private WebSocketHandler webSocketHandler; @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry webSocketHandlerRegistry) &#123; webSocketHandlerRegistry.addHandler(webSocketHandler,\"/webSocket\"); &#125;&#125; 客户端环境准备 新建页面 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt; &lt;!--标准HTML5--&gt;&lt;html lang=\"zh-CN\"&gt;&lt;body&gt;&lt;h1&gt;Hello World!&lt;/h1&gt;&lt;/body&gt;&lt;script type=\"text/javascript\"&gt; var websocket = null; if('WebSocket' in window) &#123; websocket = new WebSocket('ws://localhost:8080/webSocket?code=abcd'); &lt;!--此处的地址即第二步指定的接口地址，可以传递参数--&gt; &#125;else &#123; alert('该浏览器不支持websocket!'); &#125; websocket.onopen = function (event) &#123; alert('建立连接'); &#125; websocket.onclose = function (event) &#123; alert('连接关闭'); &#125; websocket.onmessage = function (event) &#123; alert('收到消息:' + event.data) &#125; websocket.onerror = function () &#123; alert('websocket通信发生错误！'); &#125; window.onbeforeunload = function () &#123; websocket.close(); &#125;&lt;/script&gt; 注意：如果websocket 的 url写的是localhost:8080，则当前页面的地址也应是localhost:8080域名下，不可以是127.0.0.1:8080 ，否则会提示 403，如果用真实域名也同理。 新建 Controller 跳转到该页面 12345678@Controllerpublic class HomeController &#123; @GetMapping(value = \"/home\") public ModelAndView home() &#123; return new ModelAndView(\"home\"); &#125;&#125; 进行测试 启动项目 访问 http://localhost:8080/home 浏览器控制台打印结果： 应用 服务器向浏览器发送通知可以使用 WebSocket 聊天室 资源下载 点我下载示例源码 参考 http://www.ruanyifeng.com/blog/2017/05/websocket.html","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://nullpointer.pw/tags/SpringBoot/"},{"name":"WebSocket","slug":"WebSocket","permalink":"http://nullpointer.pw/tags/WebSocket/"}]},{"title":"WebMagic之爬虫监控","date":"2018-02-26T22:16:05.000Z","path":"WebMagic之爬虫监控.html","text":"前言年前闲着无聊，研究了一阵子爬虫技术，接触到爬虫框架 WebMagic，感觉很好用。在之后的工作中，接手了新站与第三方接口对接的工作，主要的工作是去抓取对方接口的内容；初始的时候，之前负责该工作的同事，是手动使用多线程去抓取，在应用的过程当中暴露了不少问题。比如对于接口内容超级多的时候，虽然使用了多线程，但是抓取的效率很低，而且也没有实现增量抓取，每次都需要去全量抓取，跑一次基本需要好几天-.-；小说是连载的情况下，这种问题是亟需解决的。 趁着熟悉了新兵器 WebMagic, 果断在项目中进行引入，解决以上问题。功能上线后，替换了原有的多线程抓取，目前已经十分稳定， 基本上配置好任务之后，就无需再人工干预了。 以下，正文是基于我学习 WebMagic 时练手项目，功能和在公司开发的差不多，只不过我本地开发的是去抓取盗版网站的内容。 项目预览 菜单管理 爬虫任务管理 实现了爬虫的状态监控，以及可视化启停 初入手兵器-基本使用 爬虫套路分析先看官方文档的总体架构图 大部分模块WebMagic已经提供了默认实现。 一般来说，对于编写一个爬虫，PageProcessor是需要编写的部分，而Spider则是创建和控制爬虫的入口。 得益于 WebMagic 框架的良好封装，对于框架的使用者来说，所需要编写的代码几乎只有爬虫的逻辑代码，而对于怎么爬，维护任务队列的事情，WebMagic 都可以替我们做好。开始我们的爬虫之旅吧！ 引入依赖本文中所使用到的项目是基于 Maven 的 SSM 项目，在 pom.xml 中引入 WebMagic 的依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-core&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-extension&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt;&lt;/dependency&gt; 基本类图 先将对应的处理类进行抽象出来，方便统一处理。 每个爬虫都有其对应的配置信息 Site 是 抓取网站的相关配置，包括编码、抓取间隔、重试次数 对应的实现类重写 process 方法，在方法中实现对应的爬虫逻辑处理 启动爬虫 爬虫的使用就简单带过，具体可以将本文与官方文档结合使用，官方文档的示例只是基于 main 方法。 爬虫监控 扩展源码 为了实现项目预览的效果，实现爬虫的状态监控，需要对爬虫进行扩展。因为官网提供的方式功能不足以达到在页面展示的效果。 添加监控非常简单，获取一个 SpiderMonitor 的单例 SpiderMonitor.instance()，并将你想要监控的 Spider 注册进去即可。你可以注册多个 Spider 到 SpiderMonitor 中。 查看 SpiderMonitor 源代码后，如果调用的是 获取一个 SpiderMonitor 的单例 SpiderMonitor 的 注册方法，发现 WebMagic 将每只爬虫的状态对象 SpiderStatusMXBean 全部添加到一个 List 集合当中去，这样就难以区分具体是哪一只爬虫的状态，所以我们需要对 SpiderMonitor 进行扩展。 将 SpiderMonitor 中的 private List&lt;SpiderStatusMXBean&gt; spiderStatuses = new ArrayList&lt;SpiderStatusMXBean&gt;(); 修改为 Map 集合，key 选择 Spider 的 UUID 作为唯一区分爬虫的标记。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596@Experimentalpublic class MySpiderMonitor &#123; private static MySpiderMonitor INSTANCE = new MySpiderMonitor(); private AtomicBoolean started = new AtomicBoolean(false); private Logger logger = LoggerFactory.getLogger(getClass()); private MBeanServer mbeanServer; private String jmxServerName; private Map&lt;String,MySpiderStatus&gt; spiderStatuses = new HashMap&lt;String,MySpiderStatus&gt;(); protected MySpiderMonitor() &#123; jmxServerName = \"WebMagic\"; mbeanServer = ManagementFactory.getPlatformMBeanServer(); &#125; public Map&lt;String,MySpiderStatus&gt; getSpiderStatuses() &#123; return spiderStatuses; &#125; /** * Register spider for monitor. * * @param spiders spiders * @return this */ public synchronized MySpiderMonitor register(Spider... spiders) throws JMException &#123; for (Spider spider : spiders) &#123; MyMonitorSpiderListener monitorSpiderListener = new MyMonitorSpiderListener(); if (spider.getSpiderListeners() == null) &#123; List&lt;SpiderListener&gt; spiderListeners = new ArrayList&lt;SpiderListener&gt;(); spiderListeners.add(monitorSpiderListener); spider.setSpiderListeners(spiderListeners); &#125; else &#123; spider.getSpiderListeners().add(monitorSpiderListener); &#125; MySpiderStatus spiderStatusMBean = getSpiderStatusMBean(spider, monitorSpiderListener); registerMBean(spiderStatusMBean); spiderStatuses.put(spider.getUUID(),spiderStatusMBean); &#125; return this; &#125; protected MySpiderStatus getSpiderStatusMBean(Spider spider, MyMonitorSpiderListener monitorSpiderListener) &#123; return new MySpiderStatus(spider, monitorSpiderListener); &#125; public static MySpiderMonitor instance() &#123; return INSTANCE; &#125; public class MyMonitorSpiderListener implements SpiderListener &#123; private final AtomicInteger successCount = new AtomicInteger(0); private final AtomicInteger errorCount = new AtomicInteger(0); private List&lt;String&gt; errorUrls = Collections.synchronizedList(new ArrayList&lt;String&gt;()); @Override public void onSuccess(Request request) &#123; successCount.incrementAndGet(); &#125; @Override public void onError(Request request) &#123; errorUrls.add(request.getUrl()); errorCount.incrementAndGet(); &#125; public AtomicInteger getSuccessCount() &#123; return successCount; &#125; public AtomicInteger getErrorCount() &#123; return errorCount; &#125; public List&lt;String&gt; getErrorUrls() &#123; return errorUrls; &#125; &#125; protected void registerMBean(SpiderStatusMXBean spiderStatus) throws MalformedObjectNameException, InstanceAlreadyExistsException, MBeanRegistrationException, NotCompliantMBeanException &#123; ObjectName objName = new ObjectName(jmxServerName + \":name=\" + spiderStatus.getName()); if(mbeanServer.isRegistered(objName)==false) &#123; mbeanServer.registerMBean(spiderStatus, objName); &#125; &#125;&#125; 需要注意的是，SpiderMonitor 中使用的 SpiderStatus 也需要进行一同扩展。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class MySpiderStatus implements SpiderStatusMXBean &#123; protected final Spider spider; protected Logger logger = LoggerFactory.getLogger(getClass()); protected final MySpiderMonitor.MyMonitorSpiderListener monitorSpiderListener; public MySpiderStatus(Spider spider, MySpiderMonitor.MyMonitorSpiderListener monitorSpiderListener) &#123; this.spider = spider; this.monitorSpiderListener = monitorSpiderListener; &#125; public Spider getSpider() &#123; return this.spider; &#125; public String getName() &#123; return spider.getUUID(); &#125; public int getLeftPageCount() &#123; if (spider.getScheduler() instanceof MonitorableScheduler) &#123; return ((MonitorableScheduler) spider.getScheduler()).getLeftRequestsCount(spider); &#125; logger.warn(\"Get leftPageCount fail, try to use a Scheduler implement MonitorableScheduler for monitor count!\"); return -1; &#125; public int getTotalPageCount() &#123; if (spider.getScheduler() instanceof MonitorableScheduler) &#123; return ((MonitorableScheduler) spider.getScheduler()).getTotalRequestsCount(spider); &#125; logger.warn(\"Get totalPageCount fail, try to use a Scheduler implement MonitorableScheduler for monitor count!\"); return -1; &#125; @Override public int getSuccessPageCount() &#123; return monitorSpiderListener.getSuccessCount().get(); &#125; @Override public int getErrorPageCount() &#123; return monitorSpiderListener.getErrorCount().get(); &#125; public List&lt;String&gt; getErrorPages() &#123; return monitorSpiderListener.getErrorUrls(); &#125; @Override public String getStatus() &#123; return spider.getStatus().name(); &#125; @Override public int getThread() &#123; return spider.getThreadAlive(); &#125; public void start() &#123; spider.start(); &#125; public void stop() &#123; spider.stop(); &#125; @Override public Date getStartTime() &#123; return spider.getStartTime(); &#125; @Override public int getPagePerSecond() &#123; int runSeconds = (int) (System.currentTimeMillis() - getStartTime().getTime()) / 1000; return getSuccessPageCount() / runSeconds; &#125;&#125; 重写爬虫启动处代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Servicepublic class WebMagicService &#123; @Resource private ApplicationContext context; @Resource private TaskService taskService; public void run(TaskDTO taskDTO, boolean runAsync) throws Exception &#123; MySpiderMonitor spiderMonitor = MySpiderMonitor.instance(); String ruleJson = taskDTO.getTaskRuleJson(); WebMagicConfig config = JSONObject.parseObject(ruleJson, WebMagicConfig.class); SpiderConfig spiderConfig = config.getSpider(); AbstractPageProcess pageProcess = context.getBean(spiderConfig.getProcesser(), AbstractPageProcess.class); pageProcess.init(config); pageProcess.setUuid(taskDTO.getSpiderUUID()); Spider spider = Spider.create(pageProcess).thread(spiderConfig.getThread()); spider.setUUID(taskDTO.getSpiderUUID()); List&lt;String&gt; pipelines = spiderConfig.getPipeline(); for (String pipeline : pipelines) &#123; Pipeline bean = context.getBean(pipeline, Pipeline.class); if (bean != null) &#123; spider.addPipeline(bean); &#125; &#125; // 设置Downloader // 设置Scheduler // 注册爬虫 spiderMonitor.register(spider); spider.addUrl(spiderConfig.getStartUrl()); if (runAsync) &#123; spider.runAsync(); &#125; else &#123; spider.run(); &#125; &#125; /** * 爬虫状态监控 * @return */ public List&lt;TaskDTO&gt; runTaskList() &#123; MySpiderMonitor spiderMonitor = MySpiderMonitor.instance(); Map&lt;String, MySpiderStatus&gt; spiderStatuses = spiderMonitor.getSpiderStatuses(); List&lt;TaskDTO&gt; taskDTOList = taskService.findAll(); for (TaskDTO taskDTO : taskDTOList) &#123; MySpiderStatus spiderStatus = spiderStatuses.get(taskDTO.getSpiderUUID()); if (spiderStatus == null) &#123; taskDTO.setRunState(Spider.Status.Stopped.name()); &#125; else &#123; taskDTO.setRunState(spiderStatus.getStatus()); &#125; &#125; return taskDTOList; &#125; public TaskDTO stop(TaskDTO taskDTO) &#123; MySpiderMonitor spiderMonitor = MySpiderMonitor.instance(); Map&lt;String, MySpiderStatus&gt; spiderStatuses = spiderMonitor.getSpiderStatuses(); MySpiderStatus spiderStatus = spiderStatuses.get(taskDTO.getSpiderUUID()); if (spiderStatus != null) &#123; spiderStatus.stop(); spiderStatus.getSpider().close(); &#125; return taskDTO; &#125;&#125; 创建爬虫时，将爬虫注册到 MySpiderMonitor 中，之后通过 getSpiderStatuses 方法即可获取所有爬虫的状态了。 资源下载 点我下载源码","tags":[{"name":"webmagic","slug":"webmagic","permalink":"http://nullpointer.pw/tags/webmagic/"},{"name":"爬虫监控","slug":"爬虫监控","permalink":"http://nullpointer.pw/tags/爬虫监控/"}]},{"title":"SpringDataJPA快速入门","date":"2018-01-27T13:02:48.000Z","path":"SpringDataJPA快速入门.html","text":"前言之前在学习 SpringBoot 框架的时候，使用到了 SpringData JPA，但是当时只是简单的查询，没有用到稍微复杂的查询。 JPA 的 JPQL 语法规则对于简单的查询实属利器，大大加快了开发速度。不久前，在公司将用户推荐功能单独抽取出为一个独立项目，由于公司一直沿用的底层框架太老，只能使用 JDK1.6，JDK 1.9都出来了，实在不能忍😅，果断引入了 SpringData JPA。 然后最近公司其他同事接手了该项目，但是不太了解 SpringData JPA 的使用，于是有了此文，不会就可以直接让他看本篇博客了哈哈。 环境准备这里不讲解 SpringData JPA 与框架的整合，只讲解 JPA 语法的使用 Entity 实体类1234567891011121314@Entity // 表示为一个实体类@Table(\"employee\") // 表名public class Employee &#123; @Id //主键标识注解 @GeneratedValue // 主键生成方式 private Integer id; private String name; private Integer age; //Getter/Setter省略&#125; Repository 接口1234// 继承 JpaRepository 接口，第一个参数为查询的实体类，第二个为实体类的主键数据类型public interface EmployeeRepository extends JpaRepository&lt;Employee, Integer&gt;&#123;&#125; 插入测试数据123456789@Testpublic void testAdd() throws Exception &#123; for (int i = 0; i &lt; 100; i++) &#123; Employee employee = new Employee(); employee.setAge(i); employee.setName(\"test\" + i); employeeRepository.save(employee); &#125;&#125; JPA 查询语法讲解使用 JPQL 进行查询1234567891011121314151617181920212223242526272829303132333435363738394041// 新增employeeRepository.save(employee);// where name = ?1employeeRepository.findByName(\"test1\")// where name = ?1 and age = ?2employeeRepository.findByNameAndAge(\"test1\", 20);// where age between ?1 and ?2 【包含头尾】List&lt;Employee&gt; byAgeBetween = employeeRepository.findByAgeBetween(10, 16);// where age &lt; ?1List&lt;Employee&gt; list = employeeRepository.findByAgeLessThan(10);// where age &gt; ?1List&lt;Employee&gt; list = employeeRepository.findByAgeGreaterThan(90);// where name is null 【不包含为空字符串的数据】List&lt;Employee&gt; list = employeeRepository.findByNameIsNull();// where name like \"test9%\" 以test9为开头的nameList&lt;Employee&gt; list = employeeRepository.findByNameLike(\"test9%\");或者List&lt;Employee&gt; employees = employeeRepository.findByNameStartingWith(\"test\")// where name like \"test_\" 以test开头，且后面只模糊匹配一位List&lt;Employee&gt; list2 = employeeRepository.findByNameLike(\"test_\");// where name like \"%6\" 模糊匹配以6结尾的List&lt;Employee&gt; employees = employeeRepository.findByNameEndingWith(6)// where name in (?1, ?2)List&lt;String&gt; names = Arrays.asList(\"test1\", \"test2\");List&lt;Employee&gt; employees = employeeRepository.findByNameIn(names);// where age &lt;&gt; ?1List&lt;Employee&gt; employees = employeeRepository.findByAgeNot(99);// where name = ?1 order by age desc List&lt;Employee&gt; findByNameOrderByAgeDesc(\"test\"); 使用自定义 Sql 以及 原生 Sql 查询12345678910111213/** EmployeeRepository.java 添加方法 */// 根据姓名与年龄查找，[通过占位符获取参数值]@Query(\"select o from Employee o where name = ?1 and age = ?2\")List&lt;Employee&gt; queryEmployeeByParams(String name, Integer age);// 根据姓名与年龄查找，[通过命名参数获取参数值]，必须使用 @Param 注解@Query(\"select o from Employee o where name = :name and age = :age\")List&lt;Employee&gt; queryEmployeeByParams2(@Param(\"name\") String name, @Param(\"age\") Integer age);// 原生SQL，与上面不同的是，上面使用的是对象名称以及对象属性名称，Native SQL使用数据库表名以及字段名@Query(nativeQuery = true, value = \"select * from employee where name = :name and age = :age\")List&lt;Employee&gt; queryEmployeeByParams3(@Param(\"name\") String name, @Param(\"age\") Integer age); JPA 更新操作12345/** 需要搭配使用 @Query 与 @Modifying 和 @Transactional 注解使用*/@Modifying@Query(\"update Employee o set o.age = ?2 where o.id = ?1\")Integer updateAge(Integer id, Integer age); 在 Service 层调用1234567@Autowiredprivate EmployeeRepository employeeRepository;@Transactional // 必须开启事务public void update(Integer id, Integer age) &#123; employeeRepository.update(id, age);&#125; 分页查询1234567891011121314151617181920212223242526// EmployeeRepository 接口定义Page&lt;Employee&gt; findByNameStartingWith(String name, Pageable pageable);// 测试类 EmployeeRepositoryTest.java// 普通分页查询@Testpublic void testFindByNameStartingWith() &#123; // 注意 page 从 0 开始 Pageable request = new PageRequest(0, 10); Page&lt;Employee&gt; result = employeeRepository.findByNameStartingWith(\"test\", request); for (Employee employee : result.getContent()) &#123; System.out.println(employee); &#125;&#125;// 带排序条件的分页查询@Testpublic void testFindByNameStartingWith() &#123; Sort.Order order = new Sort.Order(Sort.Direction.DESC, \"id\"); Sort sort = new Sort(order); Pageable request = new PageRequest(0, 10, sort); Page&lt;Employee&gt; result = employeeRepository.findByNameStartingWith(\"test\", request); for (Employee employee : result.getContent()) &#123; System.out.println(employee); &#125;&#125; 动态 SQL 查询在 Java 开发中，动态 SQL 是必不可少的，JPA 也可以实现，Repository 多继承一个接口 JpaSpecificationExecutor 即可。123456789101112131415// 修改之前的 EmployeeRepository, 使其多继承 JpaSpecificationExecutor 接口public interface EmployeeRepository extends JpaRepository&lt;Employee, Integer&gt;, JpaSpecificationExecutor&lt;Employee&gt;&#123; // ......&#125;Pageable request = new PageRequest(0, 10);Specification&lt;Employee&gt; specification = new Specification&lt;Employee&gt;() &#123;public Predicate toPredicate(Root&lt;Employee&gt; root, CriteriaQuery&lt;?&gt; query, CriteriaBuilder cb) &#123; Path&lt;Integer&gt; path = root.get(&quot;age&quot;); return cb.gt(path, 50); &#125; &#125;;Page&lt;Employee&gt; all = employeeRepository.findAll(specification, request); 后记这里感谢一下慕课网，快速入门多亏了 imooc 上的课程。 参考课程： https://www.imooc.com/learn/821","tags":[{"name":"JPA","slug":"JPA","permalink":"http://nullpointer.pw/tags/JPA/"}]},{"title":"Mac下VMwareFusion安装CentOS并设置静态IP","date":"2018-01-21T09:03:09.000Z","path":"Mac下VMwareFusion安装CentOS并设置静态IP.html","text":"前言MAC下VMwareFusion安装CentOS及设置固定IP之前写过一篇文章，关于虚拟机的快速安装的教程 CentOS快速安装，当时是学习中用到了 Linux 环境，为了以后无需重复进行虚拟机繁杂配置，比如配置网络连接外网，才制作的镜像。为了开发调试方便，镜像设置为了固定 IP，所以导入该镜像的时候，需要简单设置以下即可 访问外网环境。当时使用的是 Windows 环境，前不久入手了 MBP，也需要用到虚拟机环境，但是 MAC 环境下并没有 虚拟网络编辑器,所以设置静态 IP 费了好一般功夫。本文介绍虚拟机快速导入镜像，修改静态IP。 环境准备 VMware Fusion CentOS 镜像(链接：http://pan.baidu.com/s/1nuKxhpf 密码：8dt8) 导入虚拟机镜像下载镜像后解压，打开 Vmware Fusion 软件，选择导入 选择镜像文件，选择 ovf 文件后打开。 导入过程不再赘述。 为静态IP做准备打开 VMware Fusion 设置。 进入网络选项卡，添加一个网卡。 打开终端，输入以下命令： 123cd /Library/Preferences/VMware\\ Fusion/sudo vim networking 红色圈内是我们需要关注的地方,修改完之后保存退出。 再修改另外一个文件。 12cd /Library/Preferences/VMware\\ Fusion/vmnet2sudo vim nat.conf 使网络配置生效重新打开VMware的网络配置，为了使刚才的修改生效，我们重新设置保存一下。步骤是，先取消勾选将 Mac 主机连接到该网络 点击应用；再勾选上 将 Mac 主机连接到该网络，点击应用。 修改虚拟机的静态 IP打开虚拟机12账号: root密码: 123456 修改第一个文件1vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改 DEVICE 为 eth1;修改 IPADDR 地址为 192.168.200.3 ~ 192.168.200.254 之间的值;修改 GATEWAY 为 nat.conf 中配置的 ip 地址 192.168.200.2 修改另外一个文件1vi /etc/udev/rules.d/70-persistent-net.rules 12345注释掉 eth0 所在行，结果是# PCI device 0x8086:0x100f (e1000)#SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;00:0c:29:30:a4:b5&quot;, ATTR&#123;type&#125;==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot;# PCI device 0x8086:0x100f (e1000)SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;00:0c:29:cd:05:ea&quot;, ATTR&#123;type&#125;==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth1&quot; 重启 虚拟机 1reboot 重启成功后，输入 ifconfig 提示： 此时已经也可以 ping 通外网了，在 mac 的终端中也可以 ping 通虚拟机。 后记为了在使用过程中，虚拟机环境被折腾坏，可以做一下快照。到时候可以直接恢复到此快照，免去再次安装的步骤。 参考 http://blog.csdn.net/zhishengqianjun/article/details/77046796","tags":[{"name":"VMware Fusion","slug":"VMware-Fusion","permalink":"http://nullpointer.pw/tags/VMware-Fusion/"},{"name":"静态IP","slug":"静态IP","permalink":"http://nullpointer.pw/tags/静态IP/"},{"name":"mac","slug":"mac","permalink":"http://nullpointer.pw/tags/mac/"}]},{"title":"基于微信事件二维码推广","date":"2018-01-06T11:53:36.000Z","path":"基于微信事件二维码推广.html","text":"前言之前写了一篇关于推广系统设计的博客，在那篇博客中介绍了统一的推广地址生成的方法，只举例说明了一种推广的方式，本篇博客讲解另外一种推广方式，基于微信事件二维码的方式。需要注意的是，事件二维码只有服务号有此接口权限，订阅号是没有该接口权限的，当然如果没有服务号的话，想学习一下接口的使用，微信官方提供的测试号可以使用该接口。 谈谈事件二维码二维码的便利性毋庸置疑，尤其是在微信中可以通过长按识别的方式识别二维码，让二维码的使用更加便捷频繁。微信推出的事件二维码大大提高二维码的交互能力，以下内容摘自官网介绍。 为了满足用户渠道推广分析和用户帐号绑定等场景的需要，公众平台提供了生成带参数二维码的接口。使用该接口可以获得多个带不同场景值的二维码，用户扫描后，公众号可以接收到事件推送。 事件二维码可以产生事件，并且二维码是可以携带参数的，这就可以用作推广。比如我司，针对某一本书的某一章节生成事件二维码，用户扫描二维码后，如果未关注公众号，会进入关注公众号页面，方便用户进行关注，关注后，后台接收到微信的事件推送，得到了存储的书与章节的信息，则可以直接发送消息，引导用户进入推广的章节页面，以下为我司微信推广的流程图。 创建事件二维码为了便于演示，我的项目是基于 SpringBoot 搭建。本项目使用的是微信测试号，可以在此处进行申请 微信测试号。appID 与 appsecret 在微信测试号中可以得到。 之前在公司写这套推广系统的时候，都是自己看微信公众平台文档，自己写的基础代码，比如封装请求参数，参数拼接，最近在 Github 上发现了一个封装得不错的 微信开发 SDK，对微信的开放平台、公众平台、小程序都有对应的子项目。秉着不重复造轮子的原则，我们引入这套 SDK，也就无需自己再进行封装了，还可以学习封装的源代码。 首先添加 Maven 依赖，这里引入操作微信公众号的 SDK 12345&lt;dependency&gt; &lt;groupId&gt;com.github.binarywang&lt;/groupId&gt; &lt;artifactId&gt;weixin-java-mp&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt;&lt;/dependency&gt; 然后配置微信公众平台参数 appID 与 appsecret，新建微信平台配置信息 Properties 1234567891011121314import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Data@Component@ConfigurationProperties(prefix = \"wechat\")public class WechatProperties &#123; private String appId; private String seecret; private String token;&#125; application.yml 中添加 appID 与 appsecret 1234wechat: appId: wx1234567890 seecret: c912345678900987654321 token: QAnEo9760zDywWMvTKCxx 创建微信配置类，声明两个 Bean。 12345678910111213141516171819202122@Componentpublic class WeChatConfig &#123; @Resource private WechatProperties wechatProperties; @Bean public WxMpService wxMpService() &#123; WxMpService wxMpService = new WxMpServiceImpl(); wxMpService.setWxMpConfigStorage(wxMpConfigStorage()); return wxMpService; &#125; @Bean public WxMpConfigStorage wxMpConfigStorage() &#123; WxMpInMemoryConfigStorage wxMpConfigStorage = new WxMpInMemoryConfigStorage(); wxMpConfigStorage.setAppId(wechatProperties.getAppId()); wxMpConfigStorage.setSecret(wechatProperties.getSeecret()); wxMpConfigStorage.setToken(wechatProperties.getToken()); return wxMpConfigStorage; &#125;&#125; 创建 WechaptService 以及其实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Servicepublic class WechapServiceImpl implements WechatService &#123; @Resource private WxMpService wxMpService; /** * 创建永久事件二维码 * @param param 附加的参数 * @param needShortUrl 是否转换为短链接 * @return 微信二维码地址 */ @Override public String createQrCode(String param, boolean needShortUrl) &#123; WxMpQrcodeService qrService = wxMpService.getQrcodeService(); try &#123; WxMpQrCodeTicket ticket = qrService.qrCodeCreateLastTicket(param); String url = qrService.qrCodePictureUrl(ticket.getTicket(), needShortUrl); return url; &#125; catch (WxErrorException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 创建临时事件二维码 * @param param 附加的参数 * @param expireSeconds 有效时间 单位为秒，最大2592000（即30天） * @param needShortUrl 是否转换为短链接 * @return 微信二维码地址 */ @Override public String createTempQrCode(String param, Integer expireSeconds, boolean needShortUrl) &#123; WxMpQrcodeService qrService = wxMpService.getQrcodeService(); try &#123; WxMpQrCodeTicket ticket = qrService.qrCodeCreateTmpTicket(param, expireSeconds); String url = qrService.qrCodePictureUrl(ticket.getTicket(), needShortUrl); return url; &#125; catch (WxErrorException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 可以看到，简单的几行语句就可以生成事件二维码了。这里的参数 param 就是附加到二维码中的参数（可以放入推广相关参数，如推广链接ID），用户扫描后，微信服务器会推送相应的事件，可以在事件参数中可以获得 param。 创建 Spring Junit 测试该方法，生成事件二维码 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class WechapServiceImplTest &#123; @Resource private WechatService wechatService; @Test public void createQrCode() throws Exception &#123; String url = wechatService.createQrCode(\"hello wechat\", true); System.out.println(url); &#125;&#125; 打开生成的二维码链接，短链接 URL 格式如： https://w.url.cn/s/xxxxx 使用微信进行扫描，会进入提示关注的页面。 接收微信事件推送参数二维码已经生成了，但是你发现扫描关注之后，神马事情也没有发生，说好的发送消息呢！这是因为你没有接收微信的扫码关注事件推送和对其进行处理呢。 创建 Controller 用于微信接口请求校验，然后启动项目 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788@Controller@RequestMapping(value = \"/wechat/auth\")public class WeChatController &#123; @Autowired private WxMpService wxService; @Autowired private WxMpMessageRouter router; @GetMapping(produces = \"text/plain;charset=utf-8\") public void authGet( @RequestParam(name = \"signature\", required = false) String signature, @RequestParam(name = \"timestamp\", required = false) String timestamp, @RequestParam(name = \"nonce\", required = false) String nonce, @RequestParam(name = \"echostr\", required = false) String echostr, HttpServletResponse response) &#123; if (StringUtils.isAnyBlank(signature, timestamp, nonce, echostr)) &#123; throw new IllegalArgumentException(\"请求参数非法，请核实!\"); &#125; if (this.wxService.checkSignature(timestamp, nonce, signature)) &#123; try &#123; response.getWriter().print(echostr); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; return; &#125; System.out.println(1111); &#125; @PostMapping(produces = \"application/xml; charset=UTF-8\") @ResponseBody public String post(@RequestBody String requestBody, @RequestParam(\"signature\") String signature, @RequestParam(\"timestamp\") String timestamp, @RequestParam(\"nonce\") String nonce, @RequestParam(name = \"encrypt_type\", required = false) String encType, @RequestParam(name = \"msg_signature\", required = false) String msgSignature) &#123; if (!this.wxService.checkSignature(timestamp, nonce, signature)) &#123; throw new IllegalArgumentException(\"非法请求，可能属于伪造的请求！\"); &#125; String out = null; if (encType == null) &#123; // 明文传输的消息 WxMpXmlMessage inMessage = WxMpXmlMessage.fromXml(requestBody); WxMpXmlOutMessage outMessage = this.route(inMessage); if (outMessage == null) &#123; return \"\"; &#125; out = outMessage.toXml(); &#125; else if (\"aes\".equals(encType)) &#123; // aes加密的消息 /*WxMpXmlMessage inMessage = WxMpXmlMessage.fromEncryptedXml( requestBody, this.wxService.getWxMpConfigStorage(), timestamp, nonce, msgSignature); WxMpXmlOutMessage outMessage = this.route(inMessage); if (outMessage == null) &#123; return \"\"; &#125; out = outMessage .toEncryptedXml(this.wxService.getWxMpConfigStorage());*/ &#125; return out; &#125; private WxMpXmlOutMessage route(WxMpXmlMessage message) &#123; try &#123; return this.router.route(message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 设置微信消息推送接口，用于接收微信推送的请求，这里需要填写外网域名，如果你有自己的服务器和域名，可以直接填入；如果你没有，可以使用 ngrok 进行内网穿透，得到一个外网域名。 比如我的微信请求校验地址为 127.0.0.1:8080/wechat/auth， 通过内网映射转发后得到的外网地址为 http://vcmq.free.ngrok.cc/wechat/auth，填写这个域名， Token 用于校验服务器，之前的 yml 文件中已指定了 token 的值，填写该值即可，点击确认。 微信扫描事件二维码，微信服务器推送请求到 post 方法，调用 route 方法，进行处理。事件二维码文档地址 事件KEY值，qrscene_为前缀，后面为二维码的参数值。 可以针对这种情况的进行处理。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Componentpublic class SubscribeHandler extends AbstractHandler &#123; @Override public WxMpXmlOutMessage handle(WxMpXmlMessage wxMessage, Map&lt;String, Object&gt; context, WxMpService weixinService, WxSessionManager sessionManager) throws WxErrorException &#123; this.logger.info(\"新关注用户 OPENID: \" + wxMessage.getFromUser()); /*// 获取微信用户基本信息 WxMpUser userWxInfo = weixinService.getUserService() .userInfo(wxMessage.getFromUser(), null); if (userWxInfo != null) &#123; // TODO 可以添加关注用户到本地 &#125;*/ WxMpXmlOutMessage responseResult = null; try &#123; responseResult = handleSpecial(wxMessage); &#125; catch (Exception e) &#123; this.logger.error(e.getMessage(), e); &#125; if (responseResult != null) &#123; return responseResult; &#125; try &#123; return new TextBuilder().build(\"感谢关注\", wxMessage, weixinService); &#125; catch (Exception e) &#123; this.logger.error(e.getMessage(), e); &#125; return null; &#125; /** * 处理特殊请求，比如如果是扫码进来的，可以做相应处理 */ private WxMpXmlOutMessage handleSpecial(WxMpXmlMessage wxMessage) throws Exception &#123; /* 微信返回数据示例 &#123; \"createTime\": 1515235853, \"event\": \"subscribe\", \"eventKey\": \"qrscene_hello wechat\", \"fromUser\": \"onbpEv3umpi7ihx381D-vpw1SEwE\", \"msgType\": \"event\", \"ticket\": \"gQA\", \"toUser\": \"gh_96fd4ea38axx\" &#125;*/ // 如果是扫描 if (wxMessage.getEventKey().startsWith(\"qrscene_\")) &#123; // 在此处进行绑定操作 System.out.println(); &#125; System.out.println(JSONObject.toJSONString(wxMessage)); return null; &#125;&#125; 资源下载示例工程下载 参考 https://github.com/Wechat-Group/weixin-java-tools/blob/master/demo.md","tags":[{"name":"推广系统","slug":"推广系统","permalink":"http://nullpointer.pw/tags/推广系统/"},{"name":"推广","slug":"推广","permalink":"http://nullpointer.pw/tags/推广/"}]},{"title":"2017年终总结","date":"2017-12-30T10:02:57.000Z","path":"2017年终总结.html","text":"前言一年时间倏然而逝，今日已是年终岁尾矣！ 总是感觉今年过得太快了，很多次下班的时候，同事说：“明天周五了”，而我的反应就是：“诶？又要放假了吗？我怎么感觉前天才上班的啊”。以前还在上学的时候，总是感叹，这一天天为何如此漫长呢！随着年龄的增长，仿佛时间的流速也同时加快了一般，每天总是觉得是在和时间赛跑，让我有些焦虑。 今年过的让我感觉很快，但是觉得比以前充实，因为做到的事情比以前多很多。 2017 我做了哪些事 记录 2017 整年收支从 2017 年开始，比较认真的记录了自己的日常开支情况，记账 APP 显示已经坚持 记账 575 天了。我个人记账就是为了防止自己过度消费，毕竟移动支付在中国这么发达，钱已经变成了一串数字，不注意就全花光了。上个月买了一台 MBP，用于日常学习，之前的旧电脑已经带不动 IDE 了，索性换了，真的是血贵，不过写代码真爽！有的支出就是无法避免，能做的就是减少不必要的支出；记账这件事情上，做的不太好的是虽然记录的比较详尽，但是缺少总结；来年需要做阶段性的总结，比如每个月总结一次，找出不必要支出，下次尽量避免，努力实行“开源节流”。 开始基金定投年初的时候，看了一些理财的入门书籍，基本的理财技能还是需要掌握的，于是从四月份开始走上了养基之路，目前为止只定投了一只股基，收益尚可，做的不太好的地方就是把鸡蛋都放在了同一个篮子里，明年要着手于分散投资。 记录 2017 整年时间去年看了《奇特的一生》这本书，感觉时间不够用的我，尝试学习 柳比歇夫 的时间统计法，记录自己每天的时间清单，用的是 IOS 上的 APP：「时间块」 通过软件的统计功能，比较直观了反映了我的时间都去哪儿了。自我感觉比往年用在学习上的时间要多很多，但是预料不及的是我在玩（游戏、动漫、出外游玩等花费的时间）上的投入时间和蹉跎的时间（如漫无目的地上网浏览、做无任何意义事情的时间）之和，竟然狂甩学习时间几条街，说明自己对于时间的利用还是太低太低。最让我震惊的是用于交通(坐车，步行，等车等)的时间，居然达到了近 850 小时！一年十二个月,其中的一个月全是在车上度过！慎思极恐！柳比歇夫在每天每个周末每个月每个季度每年他都会汇总统计自己的时间支出，核算有效时间并加以总结，这一点我确实是没做到他这么极致，也是自己需要进行改进的地方，比如以后每个月总结一次时间的消费，看看自己有没有利用好这个月。 开始早起在记录了半年的时间消费后，看了统计后，深觉时间真的不够用，于是从六月份开始早起，工作日基本每天 5 点起床。不足的地方就是早起后，到7点洗漱吃饭去上班之前的这段时间，时间的利用不是很有效率，还有就是周末有时候变得很懒，经常快拖到 8 点多才起床，明年努力给自己找事情，提高时间的利用效率，周末督促自己早起。 开始认真对待博客其实从 2016 年，便开始用 Hexo 搭建了博客，并开始写博客，但是当时只是好奇心的驱使，随便写写觉得有趣，所以写的很水，但是从 2017 年开始，就约束自己要认真点写，对自己写博客付出的时间进行负责。我认为对于做技术的人而言，写不写博客是两种人，写的博客好不好也是两种人。不足之处就是总的博客文章不够多，大多也没有什么深度；明年多多写博客，提升技术能力，期望可以写一些比较有深度的博客。 自学新的技术技术的更迭速度很快，为了不被淘汰，只有通过不断学习，来提高技术能力。目前所在的公司是一家以业务驱动的公司，所以平时写的业务代码比较多一些，对于新的技术接触的比较少，但是领导比较鼓励我们学习新的技术用于工作中。公司的技术一成不变往往不是公司的问题，而是我们做技术的问题，就算公司想用新的技术，但是没有人会，这种技术也就不会被引入，因为使用成本太高；但是若这项技术，我们都会，而且又很有利于公司，相信一定会得到公司推行。为了保持进步，自学新兴的技术，对于自学新技术，垂直教育网站(编程) 慕课网 很不错！学习了新的技术，在恰当的时机合适的项目进行引入，也是对自己学习成果的一种实践！ 买 Kindle 看书年初买了 Kindle 用于看书，强制自己看书；看书的过程中，越发觉得自己的阅读能力实在是太差了，很多地方都读不懂，这也坚定了我要坚持看书的想法。今年看了 25 本左右的书，但是精读的太少，很多都是囫囵吞枣，纠结于读书的数量，而不是质量，来年尽量提高阅读的质量。 2017 做的不好的地方 除了上面提出的问题，身上还有一堆等着我去克服的缺点。 下半年的时候，搞了个服务器，给电脑装上了 SS 的客户端，得以随时可以访问被墙的 Google 。于是在便利查找问题的环境下，出现了要做什么工作的时候，不去思考，而是先去进行 Google 搜索一番的情况，太过依赖 Google ，随时番羽墙对于我也不是一件好事。来年希望可以养成万事先独立思考的习惯，不要成为一名只会 Google 代码的码农。 忘了从几月份开始，王者荣耀火了，我也不出意外地陷了进去，大概玩了两三个月，之后意识到自己的做法不对，方才卸载了游戏。多亏了定力的增长，否则还可能沉迷其中不可自拔，以后就尽量不玩游戏了吧，毕竟还要那么多的知识等待着我去学呢！ 每年都会看到程序员猝死的新闻，这个行业有的公司高强度无休止的加班，加上精神压力，以及不经常锻炼身体的习惯，导致悲剧的发生。在唏嘘的同时，却没有想到自己也没有锻炼身体的习惯，幸运的是公司不加班。也曾经坚持跑步锻炼了一段时间，但是没有坚持下来，还是要制定一个锻炼的计划。 2018 我想要做的事 锻炼身体，身体好是一切活动的前提 多写博客，多精读书，多写读书笔记 独立思考，多总结，争取做一个精进的人 重新学习英语，英语能力太差了，多学点英语 拾起日语的学习 结语资源越多越浪费，资源越少越珍贵，时间亦是，周末时间很多但是利用效率却低的可怕，每天早起一个多小时，能完成周末好几个小时能完成的事情。每每浪费了光阴，在余下不多的时间里想要开始认真时，却总是在时间不够用的抱怨声里，草草结束了本次周末，所以，时间都去哪儿呢？","tags":[{"name":"年终总结","slug":"年终总结","permalink":"http://nullpointer.pw/tags/年终总结/"}]},{"title":"mac编译安装Nginx","date":"2017-12-06T22:50:18.000Z","path":"mac编译安装nginx.html","text":"前言虽然后简单的方式安装Nginx，但是习惯了在linux 上的操作，为了保持一致，因此也使用编译安装的方法进行安装Nginx。 资源下载 nginx: http://nginx.org/download/nginx-1.12.2.tar.gz zlib: http://zlib.net/zlib-1.2.11.tar.gz pcre: ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.38.tar.gz openssl: https://www.openssl.org/source/openssl-1.1.0g.tar.gz 这些都是 Nginx 编译需要的依赖，下载后分别解压， 注意解压的文件要在同一个目录下面 1234567tar -zxvf nginx-1.12.2.tar.gztar -zxvf zlib-1.2.11.tar.gztar -zxvf pcre-8.38.tar.gztar -zxvf openssl-1.1.0g.tar.gz 进入nginx目录后执行12345678./configure --prefix=/usr/local/nginx --with-zlib=../zlib-1.2.11 --with-pcre=../pcre-8.38 --with-openssl=../openssl-1.1.0g// 依次执行以下命令makesudo make install 以上执行完毕后， nginx 就被安装到 /usr/local/nginx 目录下 Nginx的启停在 mac 上随便修改点东西，就提示没有权限也是醉了，只好每个命令前面都加上 sudo 了 启动sudo /usr/local/nginx/sbin/nginx 关闭sudo /usr/local/nginx/sbin/nginx -s stop 重启sudo /usr/local/nginx/sbin/nginx -s reload 参考 http://blog.csdn.net/tulies/article/details/77611409","tags":[{"name":"nginx安装","slug":"nginx安装","permalink":"http://nullpointer.pw/tags/nginx安装/"}]},{"title":"利用nginx解决跨域问题","date":"2017-12-06T22:31:02.000Z","path":"利用nginx解决跨域问题.html","text":"前言最近遇到了跨域问题，结合之前【微信支付开发本地接收异步通知回调】的经验，利用 Nginx 实现了跨域。 公司之前为了解决跨域问题，用的是 iFrame，反正对于只做后端的我而言，觉得很复杂，但是现在利用 nginx 如此简单就实现了跨域，感觉还挺有成就感，哈哈！ 为什么会出现跨域问题？前人已经总结得很好了，就借鉴一下吧！ 图片来源地址： http://www.cnblogs.com/gabrielchen/p/5066120.html 我们在开发项目中遇到的跨域问题具体是这样的，公司的域名若是 www.domain.com，那么如果是发送的 Ajax 请求就不通过这个域名走了，而是通过 a.domain.com，于是便出现了跨域问题。 比如在 www.domain.com 首页中需要通过 Ajax 获取用户登录信息。 准备工作下载演示项目为了演示这个跨域问题，我创建了一个 SpringBoot 项目，便于演示，如果不会 SpringBoot 也没有关系，因为重点在于 Nginx 配置上面。 【点我下载】 提取密码: 8e68 启动演示项目因为是SpringBoot 项目，因此可以通过 java -jar 的方式直接启动，为了演示跨域，因此需要启动两个项目，这里我们用两个端口来分别启动项目。 12345## A 项目，端口设置为8080java -jar -Dserver.port=8080 demo.jar## B 项目，端口设置为8090java -jar -Dserver.port=8090 demo.jar 配置本地 Host为了演示还需要两个域名，不用真正的域名，修改本地的 Host 即可，将两个域名的 Host 都执行本地。可以使用 SwitchHosts 来方便切换。 12127.0.0.1 www.domain.com127.0.0.1 a.domain.com 配置Nginx编译安装 Nginx后， 修改 nginx.conf 配置文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; keepalive_timeout 65; #gzip on; upstream webServer &#123; server 127.0.0.1:8080; &#125; upstream ajaxServer &#123; server 127.0.0.1:8090; &#125; server &#123; listen 80; server_name a.domain.com; location / &#123; proxy_pass http://ajaxServer; &#125; &#125; server &#123; listen 80; server_name www.domain.com; location / &#123; proxy_pass http://webServer; &#125; &#125;&#125; 这里来详细解释一下这里的配置，a.domain.com 代理到 ajaxServer，即端口设置为 8090 的 B 项目； 12345678server &#123; listen 80; server_name a.domain.com; location / &#123; proxy_pass http://ajaxServer; &#125;&#125; 再看看 www.domain.com 的配置，反向代理到 webServer , 即端口设置为 8080 的 A 项目。 12345678server &#123; listen 80; server_name www.domain.com; location / &#123; proxy_pass http://webServer; &#125;&#125; 测试访问项目地址 http://www.domain.com/page， 进入以下页面： 当输入框不输入或者输入的是当前域名 http://www.domain.com 时，可以正常提交，不会遇到跨域问题。 当输入 http://a.domain.com 时，点击按钮进行提交，会出现跨域问题。 修改 Nginx 配置文件修改 www.domain.com 的配置 12345678910111213server &#123; listen 80; server_name www.domain.com; location /proxy/ &#123; proxy_set_header Host a.domain.com; proxy_pass http://ajaxServer/; &#125; location / &#123; proxy_pass http://webServer; &#125;&#125; 其中这里这处配置是这篇文章的关键点。意思是 http://www.domain.com/proxy/ 开头的请求将会被反向代理到 B 项目 即 http://a.domain.com 域名，proxy_set_header Host 这一行是必须的。需要注意的是，这两行的域名需要保持一致，且第二行后面必须要有一个 /，至于原因，可以参考这篇文章 proxy_pass后的url加不加/的区别1234location /proxy/ &#123; proxy_set_header Host a.domain.com; proxy_pass http://ajaxServer/;&#125; 修改之后保存配置文件，重启 Nginx1./nginx -s reload 再次测试修改提交 URL 为 http://www.domain.com/proxy 点击按钮进行提交，发现此时没有出现跨域问题。虽然提交的URL 域名是 http://www.domain.com，而请求经过 Nginx 的反向代理之后，实际上提交到了 B 项目，即 http://a.domain.com， 也可以看到 B 项目打印了输出语句 userId: 12，由此便解决了跨域问题。 资源下载文章涉及的jar文件我打成一个压缩包，可以自己下载尝试一下。 【点我下载】 提取密码: 8e68","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"跨域问题","slug":"跨域问题","permalink":"http://nullpointer.pw/tags/跨域问题/"},{"name":"反向代理","slug":"反向代理","permalink":"http://nullpointer.pw/tags/反向代理/"}]},{"title":"java推广系统设计","date":"2017-12-03T08:06:26.000Z","path":"java推广系统设计.html","text":"前言公司新站由于运营需求，需要开发一套推广系统，用于网站用户引流，衍生出的推广套路目前便已经有四种，为了降低运营上手推广系统的难度，因此推广系统务必要便于使用，所以每种推广方式的配置需要保持足够明了简单。 聊聊推广常见的推广套路就是针对一个推广员生成推广链接，推广员然后其他用户通过推广链接点击进来注册，但是不可能推广的页面是注册页面，所以需要先记录下用户的来源。注册成功的同时，绑定用户与推广员的关系，便于日后通过用户的充值消费统计推广员的推广费用，以下为推广的简单流程图。 推广系统实现推广链接的生成整个推广流程最主要的地方就是推广链接的生成与使用，为了方便推广链接的管理，需要将生成的推广链接进行保存。首先进行推广链接表的设计。最后提供给推广员的推广链接格式如：http://m.domain.com/cp?code=xx，其中 code 的值为推广链接的 ID。 字段名称 字段类型 字段描述 id int 推广链接自增主键 title varcahr 推广标题 partner_id int 推广员ID params text 推广参数 type tinyint 推广类型 url_type tinyint URL类型 create_time timestamp 创建时间 update_time timestamp 更新时间 status tinyint 启用状态 字段解读： 我这里的 id 直接设置为了自增主键的值，当然为了安全点，也可以设置为其他的值，比如 UUID； title 对于同一个推广员，可能会生成多条推广链接，用于区别。 partner_id 这个就不必多说，表明推广链接属于哪一个推广员 params 注意这个参数类型为 text， 因为为了方便扩展业务参数，因此此字段存储的是一段 JSON，若使用 varchar 长度会不够用。业务参数在我司，可以是：赠送 VIP 天数，赠送阅读券数量，推广费用等。这个字段可以自由发挥，在绑定用户的时候获取该字段，解析成 JSON 对象，读取对应的业务参数，进行相对应的设置。 因为运营同学的脑洞一般都很大，因此推广方式会有很多种，也是为了日后分析推广效果，因此加上 type 字段用于区分推广类型，推广类型有比如： CPS推广、微信公众推广等； url_type 用于区分需要推广的页面是相对路径还是绝对路径。 status 用于标记此推广链接是否有效，比如终止推广合作，就可以设置为关闭，不再将用户绑定到此链接。 后台推广链接生成页面参考： 推广链接的跳转首先再看看推广的流程 首先用户通过点击推广链接，进入了推广的页面，我司就是小说的某一免费章节的阅读页面，之后到收费章节的时候，就会提示用户进行登录。注册的用户，如何知道是从哪一个链接过来的呢？这就需要在用户注册之前就将推广链接的标记进行存储，最好的方式就是写入 Cookie。之前公司项目写 Cookie 是在前端进行，但是运行之后，发现不太可靠，因此我们将写 Cookie 的操作放在后端进行。 推广链接地址是 http://m.domain.com/cp?code=xx，访问如下接口： 12345678910111213141516171819202122232425262728293031323334/** * 用于推广链接的统一跳转 */@RequestMapping (value = \"/cp\", method = RequestMethod.GET)public String companionRedirect(String code, HttpServletRequest request, Model m, HttpServletResponse response) &#123; try &#123; int id = Integer.parseInt(code); // 查询出推广链接记录 CompanionLink link = companionService.getCompanionLink(id); if (link != null &amp;&amp; link.getStatus() != Constants.STATUS_DELETE) &#123; JSONObject obj = JSONObject.parseObject(link.getParams()); // 获取推广页面的地址 String url = obj.getString(\"url\"); // 将 code 值即推广链接的 id 写入Cookie 中， EXP为有效期 3 天 CookieUtil.setCookie(request, response, Constants.COMPANION_COOKIE_NAME, code, EXP); // 跳转到推广页面 if (link.getUrlType() == Constants.ABS) &#123; // 绝对路径直接跳转 return \"redirect:\" + url; &#125;else &#123; // 相对路径拼接域名后跳转 return \"redirect:\" + Constants.MOBILE_SERVER_DOMAIN + url; &#125; &#125; &#125; catch (NumberFormatException e) &#123; &#125; // 出现异常，跳转到首页 return \"redirect:\" + Constants.MOBILE_SERVER_DOMAIN;&#125; 写这篇文章的时候，发现之前设计推广链接的时候有所疏漏，有的推广方式是相对路径，而有的是绝对路径。而我在代码中是通过 String 的 Contains方法是否包含域名来判断是相对的还是绝对的路径，其实没有必要，完全可以添加一个推广链接的 url 的路径类型。判断类型即可，如果是绝对路径直接跳转，否则拼接域名后跳转。 推广关系的绑定用户浏览器的 Cookie 中已经保存好了推广链接的 code 值，绑定关系就好办了，可以在用户注册的时候，读取浏览器的 Cookie，获取这个 Cookie，得到推广链接的 id, 将用户的 id 与推广的 partner_id 进行绑定，保存到绑定关系的表中即可，比如我司的赠送操作也一在绑定后执行。 至于绑定关系的表设计，主要将 partner_id 与 user_id 作为联合主键保存，其他字段如绑定时间就不用多说了吧。 后记如果你看完觉得有什么疑问，可以留言评论，有需要改进的地方也请提出，共同进步！","tags":[{"name":"推广系统","slug":"推广系统","permalink":"http://nullpointer.pw/tags/推广系统/"},{"name":"推广","slug":"推广","permalink":"http://nullpointer.pw/tags/推广/"},{"name":"链接推广","slug":"链接推广","permalink":"http://nullpointer.pw/tags/链接推广/"}]},{"title":"Maven 私服上传jar包","date":"2017-11-30T13:09:28.000Z","path":"Maven 私服上传jar包.html","text":"前言前段时间，公司开了新站，主要由我来负责新站的搭建工作，基础的系统比如登录系统，支付系统的搭建。 今天在11月末尾补上这一篇文章，不然放出去的话做不到就不好了（一个月至少两篇博客） 在接入支付宝支付的时候，遇到了一些问题，在此记录一下。 我们公司的之前项目接入的是支付宝 mapi 支付，这是比较早的支付接口，前一阵子，该接口的授权也到期了，我也负责后续接口的升级。 接口升级后，支付宝对其 SDK 也进行了升级，按照网站文档编写代码发现有些方法不存在，才知道是由于 项目中 SDK 的版本过低导致的。既然知道了问题，就要解决问题，于是在官网下载最新版的 SDK。 我们公司的项目全部基于 Maven 进行构建，但是官网的 SDK 只是 JAR 包，在中央仓库是没有的，因此使用的话需要上传到公司的 Maven 私服上。 环境准备 需要上传的 Jar 包，我这里是支付宝的 SDK 的 Jar 包 私服环境： Artifactory 我们公司的 maven 私服就是这货啦 上传 Jar 包到 Maven选择上传 Jar 包登录 Artifactory 后，选择 Deploy，选择要上传的 Jar 包，我这里选择上传支付宝的 SDK 的 Jar 包。 设置上传 Jar 包信息在这里选择要上传的仓库，以及设置 “GAV” 参数 由于支付宝的 Jar 包名称太长了，所以我这里精简了不必要的信息。 设置完之后，点击完成即可。 查找上传 Jar 的 Maven 坐标找到刚才保存的库，找到上传的 Jar 包 点击 对应的 pom 文件，查看 Maven 坐标 将此坐标写入 pom.xml 中即可，需要先将私服的地址写到 pom.xml 文件中。 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;repos&lt;/id&gt; &lt;url&gt;http://192.168.1.222:8088/artifactory/repos&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 上传源码 Jar 包到私服仓库想要在开发时看可以查看源码的时候，需要将对应的源码 Jar 包一起上传到私服中去，与前面步骤相同，此时选择对应的源码的 Jar 包。 需要注意这里的 目标仓库与 GAV 参数保持与上面上传对应的 Jar 包保持一致。唯一不同的地方是 Classifier 为 sources。 上传成功后如图，源码包与工程包在同一级目录中。此时在项目中可以刷新 pom.xml 文件后，对应的源码就会一同下载下载下来了，可以愉快的看源码了。 End.","tags":[{"name":"Maven私服","slug":"Maven私服","permalink":"http://nullpointer.pw/tags/Maven私服/"},{"name":"jar包上传","slug":"jar包上传","permalink":"http://nullpointer.pw/tags/jar包上传/"}]},{"title":"微信支付开发本地接收异步通知回调【实战】","date":"2017-11-08T14:18:34.000Z","path":"微信支付开发本地接收异步通知回调【实战】.html","text":"前言最近在调试微信相关的接口，但是由于微信官方出于安全的考虑，对于调用接口的域名有限制。微信授权与微信支付统一下单接口在本地可以通过更改 host 的方式来调试，微信服务器也能跳转回来，但是微信支付异步通知这里；微信官方强制让设置支付授权目录，且是通过 ICP 备案的域名，因此不采取特殊手段，本地是无法接收到微信的异步通知回调请求的，只能部署到线上环境测试。 前置条件为了解决这个问题，折腾了好几天，才搞出来了，究其根本原因，还是对于 Nginx 不太熟悉。所以在此记录一下。 内网转发如果想本地进行调试，并且想让微信服务器请求回调回来，则本地必须要有一个外网域名，这里我们是用 Ngrok内网转发 来实现；Ngrok 的配置就是将本地项目的端口映射到分配的外网域名，具体配置参考官网教程文档，此处不再赘述。 我的配置是：1http://vcmq.free.ngrok.cc -&gt; 127.0.0.1:8080 配置测试支付目录前面提到了，微信支付异步通知会对支付域名目录有要求，因此，支付时的域名必须要在设置的支付域名目录下。 首先需要在 微信商户平台 中配置 商户秘钥，支付授权目录。 这里支付授权目录配置的是以后线上的支付授权目录1http://pay.domain.com/wechat/public/ 然后再加一条本地测试支付的临时授权目录，这个目录最好在本地测试完成后，进行删除。1http://pay.domain.com/testpay/ 必知的 Nginx 语法proxy_pass 后的 url 加不加 / 的区别这里列举 nginx 的 proxy_pass 语法，是为了下一步针对 nginx 的配置进行修改。 这里访问 http://127.0.0.1/proxy/test.html 测试 proxy_pass 后面的 url 加与不加 / 的区别 12345678server &#123; listen 80; server_name 127.0.0.1; location /proxy/ &#123; proxy_pass http://192.168.0.100/; &#125;&#125; 以上 location 会代理到 http://192.168.0.100/test.html 即相当于是绝对根路径，则 Nginx 不会把 location 中的路径部分代理走。 12345678server &#123; listen 80; server_name 127.0.0.1; location /proxy/ &#123; proxy_pass http://192.168.0.100; &#125;&#125; 以上 location 会代理到 http://192.168.0.100/proxy/test.html 即相当于相对路径。 代理支付域名到第三方域名知道上一步 proxy_pass 后 / 的作用后，就开始来配置支付域名指向的服务器上的 nginx 的配置文件。 12345678910111213server &#123; listen 80; server_name pay.domain.com; location /testpay/ &#123; proxy_set_header Host vcmq.free.ngrok.cc; proxy_pass http://vcmq.free.ngrok.cc/wide/; &#125; location / &#123; proxy_pass http://payServer; &#125;&#125; 配置中的 wide 是我项目的名称；注意：这里的 proxy_set_header Host 必须配置，否则进入 location 块后，会提示 tunnel pay.domain.com not found。 当访问 pay.domain.com/testpay/create 微信统一下单接口时， 会代理到 http://vcmq.free.ngrok.cc/wide/create； 需要注意的是，在统一下单接口，设置 notify_url 的值要为配置的支付授权目录，比如我的 http://pay.domain.com/testpay/notify，这个异步通知回调 URL 限定是 POST 请求，可以在异步回调方法打断点，然后通过 postman 工具发送 post 请求到 http://pay.domain.com/testpay/notify，看是否可以正常进入断点，如果正常进入断点，恭喜你配置好了 Nginx 回调这一块。 注意： 统一下单接口参数中提交的参数 notify_url ，如果链接无法访问，商户将无法接收到微信通知。通知 url 必须为直接可访问的 url ，不能携带参数。示例：notify_url：“https://pay.weixin.qq.com/wxpay/pay.action” 另外要注意，如果支付目录中配置的是 https 的，那么 notify_url 也一定要保持一致是 https 的。 统一下单并完成支付完成后，微信服务器会请求统一下单接口中的 notify_url，经过服务器 nginx 进行代理后，会代理到 本地的内网转发域名异步通知回调接口 http://vcmq.free.ngrok.cc/wide/notify， 从而达到了本地接收微信支付异步通知回调请求的目的。 如果地址不在支付授权目录下，会提示当前 Url 未注册。因为是代理过去，因此地址栏的地址不会改变，这也是不能简单使用 nginx 的 rewrite 的原因。 如有疑问，可以联系我。 参考 nginx proxy_pass后的url加不加/的区别 跳转第三方域名","tags":[{"name":"实战","slug":"实战","permalink":"http://nullpointer.pw/tags/实战/"},{"name":"nginx转发","slug":"nginx转发","permalink":"http://nullpointer.pw/tags/nginx转发/"},{"name":"proxy_pass","slug":"proxy-pass","permalink":"http://nullpointer.pw/tags/proxy-pass/"},{"name":"本地接收异步通知","slug":"本地接收异步通知","permalink":"http://nullpointer.pw/tags/本地接收异步通知/"}]},{"title":"Nginx图片防盗链【实战】","date":"2017-10-29T06:28:46.000Z","path":"Nginx图片防盗链【实战】.html","text":"前言博主目前在一家原创小说网站公司工作，由于站内的作品全部是原创，于是乎不可避免地会被一些盗版网站爬取盗版，对于防盗版一直没有很好的对策，让公司很是苦恼。最近去一些盗版网站上搜索我们网站作品时，发现他们所用的封面图片的来源是我们网站的地址，即所谓的盗链，这对于我们网站服务器的带宽是一种损失，因此需要采取对应的手段应对。 环境准备这里我使用两台虚拟机 虚拟机的快速安装可以参考我另外一篇博文 CentOS快速安装 服务器 A：192.168.200.130 服务器 B：192.168.200.131 Nginx 安装参考博文 Nginx缓存配置 的 Nginx 环境搭建部分。 Nginx 防盗链设置一、配置 Nginx 的防盗链12345678910111213141516171819202122# 修改 nginx.conf 配置文件vim /usr/local/nginx/conf/nginx.conf# 添加 server 节点server &#123; listen 80; server_name www.vcmq.com; #access_log logs/host.access.log main; location ~* \\.(jpg|png|gif)$ &#123; root /home/resources; valid_referers none server_names *.test1.com ~\\.test2\\.; if ($invalid_referer)&#123; rewrite ^/ http://img12345.5-project.com/blog/20171028/214345352.png; &#125; &#125; location / &#123; root html; index index.html index.htm; &#125;&#125; 可以看到，我新增的 server 的地址是 www.vcmq.com。 1location ~* \\.(jpg|png|gif)$ 此行配置的作用是匹配以 jpg/png/gif 结尾的文件请求， 如果匹配就执行 括号内的代码。1234valid_referers none server_names *.test1.com ~\\.test2\\.;if ($invalid_referer)&#123; rewrite ^/ http://img12345.5-project.com/blog/20171028/214345352.png;&#125; 首先看第一行，这行的作用是配置可以识别 refer，即可以正常获取资源文件的请求，在这里配置加入白名单的请求 refer 域名。参数说明： none 代表请求的 refer 为空，也就是直接访问，比如在浏览器中直接访问图片 www.vcmq.com/test1.png，直接访问时，refer 会为空。 blocked refer 来源不为空，但是里面的值被代理或者防火墙删除了 server_names refer 来源包含当前的 server_names 即 location 的父节点 server 节点的 server_name 的值。 字符串 定义服务器名称，比如 *.test1.com，配置后，来源是从 test1.com 就会被认为是合法的请求。 正则表达式 匹配合法请求来源， 如 ~\\.test2\\. 当请求的 refer 是合法的，即可以被后面任一参数所匹配， $invalid_referer 的值为0， 若不匹配则值为 1， 进入 if 的代码中。我这里的设置是，如果是不合法的请求，就统一返回一张图片，也可以直接返回 403。 二、配置 html 文件1.在 /home 目录下创建目录 resources ,在 /home/resources 目录下放入一张图片 test.png。 2.修改 nginx 安装目录下 html 中的 index.html 文件。添加一个 img 标签，src 访问上一步中的图片文件1&lt;img src=&quot;http://www.vcmq.com/test.png&quot; /&gt; 3.启动 nginx1/usr/local/nginx/sbin/nginx 4.以上所有配置是对于服务器 A 的配置，以下讲解服务器 B 的配置 服务器 B 和 服务器 A一样安装好 nginx 修改 nginx.conf 配置文件, 添加两个 server 12345678910111213141516171819server &#123; listen 80; server_name www.test1.com; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125;&#125; server &#123; listen 80; server_name www.test2.com; location / &#123; root html; index index.html index.htm; &#125;&#125; 修改服务器 B Nginx 安装目录下 html 目录中的 index.html 文件,同样添加 img 标签。 1&lt;img src=&quot;http://www.vcmq.com/test.png&quot; /&gt; 启动 服务器 B 的 Nginx 配置注意事项 配置资源的 location 需要在 location / 之前。 如果在 location 或者其父节点 server 中没有配置 root 的值， 当请求时合法的时候，会 404 找不到资源文件，因此需要在 location 或者 server 节点设置 root 的值 注意调试的时候，务必使用 Ctrl + F5 进行刷新网页，因为 nginx 会缓存图片！ 测试我上面配置的 server_name 配置的域名是 www.vcmq.com ，因此测试时，需要将我电脑的 host 指向虚拟机的 ip, 可以使用 SwitchHosts 来便捷切换。 12345# 指向服务器A192.168.200.130 www.vcmq.com# 指向服务器B192.168.200.131 www.test1.com www.test2.com 浏览器访问 www.vcmq.com，图片正常加载 浏览器访问 www.test1.com 与 www.test2.com 还有直接访问图片 http://www.vcmq.com/test.png 都正常加载 修改 服务器 A 的 nginx.conf 配置文件 1valid_referers none server_names *.test1.com ~\\.test2\\.; 将此行修改为 1valid_referers none server_names; 即只允许当前 server_name 与 无 refer 的请求，其他请求都返回 rewrite 的图片, 然后重启 nginx 1/usr/local/nginx/sbin/nginx -s reload 再次分别访问 ,注意需要使用 ctrl+F5 刷新一下缓存 www.vcmq.com、www.test1.com、www.test2.com、http://www.vcmq.com/test.png。 发现只有 www.vcmq.com 与 http://www.vcmq.com/test.png 正常显示，其他请求皆返回 rewrite 的图片 同理再次修改 nginx.conf 文件, 允许来自 test1 的访问请求 1valid_referers none server_names *.test1.com; 结果发现www.test1.com可以获取正常图片，而www.test2.com还是 rewrite 之后的图片 至此，已经实现了基于 Nginx 的简单图片防盗链。 实际应用中，可以设置允许来自公司下的域名与空 refer 的请求通过。 这里允许 空 refer 的请求通过的原因是，有些合法的请求可能会没有带上 refer。 资源下载 文章涉及配置文件下载 参考文章 https://www.bbsmax.com/A/xl56LLwxzr/","tags":[{"name":"实战","slug":"实战","permalink":"http://nullpointer.pw/tags/实战/"},{"name":"图片防盗链","slug":"图片防盗链","permalink":"http://nullpointer.pw/tags/图片防盗链/"},{"name":"防盗链","slug":"防盗链","permalink":"http://nullpointer.pw/tags/防盗链/"}]},{"title":"Nginx配置资源下载目录","date":"2017-10-26T22:25:24.000Z","path":"Nginx配置资源下载目录.html","text":"之前在网上找 CentOs 的镜像的时候，发现了阿里云的这个镜像源，速度蛮快的。今天也来搭建一个类似的站，使用 nginx 作为资源下载服务器。图片详情： 安装 Nginx参考这篇教程的 Nginx 环境搭建部分 设置资源存储路径1、 资源准备 在 home 的用户目录下，建立文件夹，如 softs ,在 softs 目录中放入文件内容。 修改配置文件2、 修改 Nginx 配置文件 1vim conf/nginx.conf 添加 server 123456789101112server &#123; listen 80; server_name down.vcmq.com; #sendfile on; location / &#123; alias /home/softs/; sendfile on; autoindex on; autoindex_exact_size off; autoindex_localtime on; &#125;&#125; alias 设置为第一步中的资源目录，我这里是 /home/softs/ 3、 访问测试将本机 hosts 指向服务器，访问地址 down.vcmq.com 显示目录列表 注意不能是/root 目录下的文件路径 ，会显示403无权限","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"资源下载","slug":"资源下载","permalink":"http://nullpointer.pw/tags/资源下载/"}]},{"title":"如何做笔记？","date":"2017-10-26T14:10:02.000Z","path":"如何记笔记？.html","text":"最近在公司开会，我分享了一些个人日常关于如何记笔记的心得，结合在网上看的相关文章，整理一下，然后做了一些补充，写出这篇博文，文章涉及工具的选择和记笔记的步骤以及记笔记的技巧与注意事项，欢迎留言探讨。 工具的选择工欲善其事必先利其器，首先说说工具的选择，好的工具会让你记笔记得心应手，拥有良好的用户体验，不好的工具则做不到这点，达不到记笔记的目的。 常见的主流云笔记有： 有道云笔记 OneNote 为知笔记 印象笔记 先说说我最熟悉的为知笔记，在一年以前，一直用的是为知笔记，记了上千的笔记，用的很不错，逢人推荐那种；但是有一天，为知笔记突然就宣布必须要付费才能继续使用，否则的话，笔记将不能同步！对应一个云笔记产品来说，没有了最最基础的同步功能，和在本地文档里写有什么区别呢？没有合适的变现方式，就完全抛弃了免费用户，给我感觉这家公司不太靠谱，所以我就转战使用有道云笔记。但是之前的笔记还在为知笔记里，懒得去转移了。所以选择笔记软件的时候也要考虑转移成本，万一一不小心产品就黄了呢！因此尽量选择大公司的成熟笔记产品。 再说说现在用的有道云笔记，用一个词来形容就是很鸡贼，以前可以导出笔记为其他笔记软件识别的格式，现在只能导出有道能识别的格式。假如是重度用户，想转移笔记，很困难。好在有道云笔记支持 MarkDown 笔记，所以我现在尽量写的笔记都用 MarkDown 方式，因为 MarkDown 格式的直接在笔记中就能直接复制走，转移笔记很轻松。MarkDown，语法很简单，可以学习一下，轻松排版，语法可以参考有道云的文档， 记笔记不需要去学习太复杂的语法, 看下面链接的第二点就可以。 有道云笔记Markdown简明版 然后说说印象笔记，是国外 EverNote 的大陆版本，其免费版只能提供两个终端同步，假如你公司登陆一个，家里登陆一个，还想要在移动设备上登录就需要付费，还有每月上传60M流量的限制，付费版一年一百多，想要最稳定的可以考虑使用。 最后说说 OneNote, 巨硬出品，但是国内 GFW 的存在，同步很慢，然后也没太弄清楚是怎么用的，和上面介绍的笔记产品组织格式不同，也没怎么会用，就不置点评了。 我推荐使用有道云笔记和印象笔记(或EverNote) 记笔记的作用最大的作用就是对抗遗忘，用好了笔记，笔记就是你的第二大脑，人的记忆力有限，我们通过记笔记来拓展。 记笔记三步走： 收集 整理 输出 以下进行详细步骤叙述： 记笔记的步骤收集第一步是确定所要写的笔记内容，即收集，收集这里我分为两部分。 收集他人的内容他人的内容指的是他人所产生的内容，比如别人写的书，博客文章，公众号文章等等，当你遇到了比如一篇你觉得写的很有道理或者对你有用的文章，觉得有保存下来的价值，你可以将其纳入你的笔记当中去。 收集自己的内容自己的内容指的是自己所产生的内容，比如突发的灵光一现，如果不立即记录下来，可能就忘掉了，然后再想的时候无论如何也记不起来了，就感觉自己错过了一个亿。所以要趁着你的想法还没有消失之前，立即记录到笔记当中去。现在人手都有一副手机，而云笔记都有其对应的移动端，记录起来很方便。 收集的诀窍有时候在某个网站看到一篇文章想要保存下来时候，一般人的做法是讲文章选择拷贝到笔记当中去，这么做是可行，但是会完全没有了排版，看起来乱糟糟的一片，所以需要借助浏览器插件或者分享功能来实现快速美观地保存。有道云笔记的谷歌浏览器插件就很好用，在手机上可以关注公众号，分享到公众号，也可以直接分享到对应的笔记 App 中，从而实现快速收集。 整理收集完的内容，只是堆叠到了你的笔记当中，如果不加以整理总结，其实和散布在网络上的文章毫无区别，对你没有任何的意义，也没有任何的价值；也许你想找笔记里面某一篇文章，但是你却死活也找不到，这种情况下记与不记都一样；正如去一座图书馆看书，但是图书馆却没有图书查阅系统，在浩渺书海中想找到一本书的困难程度可想而知！因此收集后的整理十分重要。 整理分为重新排版与分类以及加标签（在下文详谈分类与标签）： 通过浏览器插件等工具抓取保存的内容可能会与原文中的内容排版相差甚远，比如有道云笔记对于代码块的抓取排版支持就很差（其他很不错），这种情况，就需要我们自己对内容重新排版，便于以后自己查看。个人一般是新建立一个 Markdown 笔记，拷贝原文进行编排，并在文末标注文章来源链接。 良好的笔记分类有利于快速准确地定位笔记，提升效率。 一篇笔记只能放在一个分类中（对应笔记的文件夹)，但是却能够添加多个标签，我认为标签是对分类的一种补充。当分类不足以准确定位笔记，此时需要通过添加标签来弥补。 输出单单只是将内容收集，归类加标签，并不能够对自己产生很大的用处，最多的是自己想起来就去笔记中寻找，此时笔记的作用最多是充当备忘录的作用。而真正想要利用好笔记，就是需要我们来对笔记进行总结思考，来达到在头脑中深化该篇笔记的作用。 举例而言，现在你写了一篇《拆掉思维里的墙》的读书笔记，但笔记的内容只有对于书中原文的部分摘抄；其实这样的笔记作用不大，时间长了，你甚至不记得你曾经读过这本书并写过读书笔记。但是你对文章进行了思考，概括书的主要中心思想，写了自己的感受，在豆瓣上留下了书评，就算时间久了，翻看笔记也能够回想起当时自己的想法，这书也就没有白读，笔记也就没有白写。 上文提到了在豆瓣中写书评，这和写笔记进行总结思考一样都是对于知识的一种输出。输出的方式还有很多种，比如在简书上、个人博客上、CSDN、博客园等地方发表文章。 当然，也并非所有的分类下面都需要进行这三步。 细谈笔记分类与标签分类按照个人的习惯与经验，我将笔记分为了五大类，如图： 1、 收集（指的是上图中的我的资源文件夹，因为是有道云笔记自动创建的文件夹，改名字之后他还会创建这个，所以就没有改名字）收集作为一个单独的分类，用于存放未进行整理的收集，有道云笔记生成的我的资源这个文件夹下，会根据你使用的插件，自动生成对应来源的子文件夹，比如来自网页剪报、微信收藏等，我新建了一个 思绪万千 文件夹，用于保存突发的想法。 2、 备忘 顾名思义，备忘分类用于一些需要查找使用，但又不需要完全记忆的内容，比如待办事项，待读书单，还有一些琐碎的文本数据备份等。 3、 工作 工作分类下面主要是记录一些业务需求，工作难点以及工作技巧。 4、 学习 主要是一些关于编程相关的子分类文件夹，这里和工作分类都因人而异。 5、 生活 生活分类主要放一些生活中学习的内容，比如读书笔记、投资指南，保险相关的知识。其实这个分类和学习分类很像，因为我关于代码的笔记比较多，所以将其他和代码不相关的笔记放在生活分类当中。 需要注意的是，分类的层级不宜嵌套太深，最好保持在三层以内，超过后不利于查找。(所以我把与代码不相关的放到生活分类下) 标签再谈笔记的标签，既然标签是对分类的一种补充，因此是非必要的，同时标签不能和分类重复，参考添加这些标签。 文章来源 （知乎、简书、CSDN等） 关键词 （笔记的关键点，可以提取几个词语出来作为标签） 不要太具体 （太具体就会导致标签过多，反而不利于标签的使用） 后言最后说说记笔记的原则 记笔记是为了用，不用的笔记没有任何价值 不能只收集不学习 定期回顾，整理与归档（定期回顾有利于笔记内容的记忆） 记笔记会遇到的一些问题？ 强迫症犯了，不知道该放在哪个分类下面， 原则是用，你认为放在哪个分类下面最可能被用到，就放在那个分类下面。 Nullpointer的博客 参考 https://www.zhihu.com/search?type=content&amp;q=%E7%AC%94%E8%AE%B0","tags":[{"name":"笔记","slug":"笔记","permalink":"http://nullpointer.pw/tags/笔记/"}]},{"title":"Mysql行转列输出【实战】","date":"2017-10-09T14:44:58.000Z","path":"Mysql行转列输出【实战】.html","text":"在很多场景下，需要将一行一行的数据转换成一列数据。比如每个人的每个科目都有一个分数，并分别对应一个分数，但是为了更加直观地看到一个人的所有科目成绩，所以需要将每个科目成绩由行转换成列。 原始数据 id stuid name subject score 1 110 xiaoA 语文 99 2 110 xiaoA 数学 88 3 112 xiaoB 语文 69 4 112 xiaoB 数学 100 行数据转换成列数据之后 stuid name 语文 数学 110 xiaoA 99 88 112 xiaoB 69 100 1234SELECT stuid, `name`, MAX(CASE `subject` WHEN 'C001' THEN score ELSE 0 END) as '语文', MAX(CASE `subject` WHEN 'C002' THEN score ELSE 0 END) as '数学'FROM `sys_user` GROUP BY stuid; 以下通过两个在工作中遇到的实例，来使用 Mysql 的行转列。 实例实例一网站进行推广，每个推广员都有一个唯一 ID 即 pid，现在要统计每个推广员的推广数据；需要统计推广过来的新增用户，以及新增用户的充值以及消费数据。这里简化了数据，便于理解。 数据说明： create_time： 数据生成时间 pid： 推广员唯一ID type： 数据类型 1： 新增用户 2： 新增充值 3： 新增消费 value： 对应的数据值 需求结果样式： SQL 查询： 1234567SELECT create_time,MAX(CASE type WHEN 1 THEN value ELSE 0 END) '新增用户',MAX(CASE type WHEN 2 THEN value ELSE 0 END) '新增充值',MAX(CASE type WHEN 3 THEN value ELSE 0 END) '新增订阅'FROM wings_companion_analytics GROUP BY create_time 实例二统计网站单本书的购买次数以及购买金额 数据说明： id： 主键 bookId： 书的 ID task_id： 数据类型 706： 购买次数 707： 购买金额 time： 数据生成时间 value： 对应的数据值 需求结果样式： SQL 查询： 12345678SELECT bookId as '作品ID', DATE_FORMAT(time,'%Y-%m-%d') as '日期', MAX(CASE task_id WHEN 706 THEN `value` ELSE 0 END) AS '购买次数', MAX(CASE task_id WHEN 707 THEN `value` ELSE 0 END) AS '购买金额'from `t_task_result_day`WHERE task_id IN (706, 707)GROUP BY time, bookIdORDER BY bookId, time; 资源下载 点我下载实例所用数据 参考 http://blog.csdn.net/wulantian/article/details/52687640 http://blog.csdn.net/u013938484/article/details/50552747","tags":[{"name":"mysql","slug":"mysql","permalink":"http://nullpointer.pw/tags/mysql/"},{"name":"行转列","slug":"行转列","permalink":"http://nullpointer.pw/tags/行转列/"},{"name":"实战","slug":"实战","permalink":"http://nullpointer.pw/tags/实战/"}]},{"title":"Nginx缓存配置之手动清除缓存","date":"2017-09-10T22:15:36.000Z","path":"Nginx缓存配置之手动清除缓存.html","text":"前言 前文介绍了利用 nginx 的 nginx_ngx_cache_purge 模块来实现缓存功能，并设置了缓存时间为一天。 但是如果前端修改了页面，比如首页，由于 Nginx 缓存的存在，网站首页并不会立即生效，而是要等到缓存过期。这样明显不是我们想要的结果，所以需要进行手动使 Nginx 缓存失效。以下为操作详解。 基础环境 已经设置缓存的 Nginx 服务器， 参照我之前博文 Nginx缓存配置 Nginx 配置文件拆分在企业 Nginx 应用中，Nginx 配置文件应该根据域名不同来进行拆分，然后在 nginx.conf中进行include引入。这样的好处是便于管理配置文件，便于修改配置文件， 而 nginx.conf 文件中只保留 upstream 以及其他通用配置信息。 在 Nginx 的 conf 目录下创建文件夹 include， 用于保存拆分的配置文件。 1mkdir include 拆分的规则可以如下： 桌面版（WEB）的 Nginx 配置文件，可以命名为 nginx_web.conf 手机版（WAP）的 Nginx 配置文件，可以命名为 nginx_wap.conf 安卓的 Nginx 配置文件，可以命名为 nginx_apk.conf 苹果的 Nginx 配置文件，可以命名为 nginx_ios.conf 清除缓存的配置文件，可以命名为 purge.conf 在 /usr/local/nginx/conf/nginx.conf 中引入拆分的配置文件， 在配置文件中的 http 节点下进行引入 123456789101112http&#123; #...... # 注意 purge.conf 必须要第一个引用！！！ include include/purge.conf; # 引入其他配置文件 include include/nginx_web.conf; #include include/nginx_wap.conf; #include include/nginx_apk.conf; #include include/nginx_ios.conf;&#125; 配置缓存清除12345678910111213141516171819cd /usr/local/nginx/conf/includevi purge.conf#purge.conf 文件内容为：server &#123; listen 80; server_name 192.168.200.129;# 此处我配置的是本机ip charset utf8; location ~ /purge(/.*) &#123; allow all;# 访问此接口的白名单，all代表所有人都可以访问该路径 # tmpcache 为前文中 proxy_cache_path 里配置的 keys_zone 的值 # www.domain.com$1$is_args$args 表示缓存key的名称，清除某一域名下的缓存,可以指定参数 # $1 代表正则匹配中的第一组 proxy_cache_purge cachefile www.domain.com$1$is_args$args; &#125;&#125;# 保存 purge.conf 文件 测试清除缓存如果要进行缓存清除，首先需要有缓存文件，此处参照前文 Nginx缓存配置 中，访问 www.domain.com/testpage2 ，刷新页面，直到缓存已经生成，即 X-Cache 的状态为 HIT。 进入 cd /tmp/cache/ ，查看已经生成了缓存文件 mark 123456# 测试 nginx 配置是否正确/usr/local/nginx/sbin/nginx -t# 如果提示 is ok，说明配置没有问题，否则需要修改# 重启 Nginx/usr/local/nginx/sbin/nginx -s reload 浏览器访问（也可以通过 curl 命令来访问）： 1234# 清除key为 www.domain.com/ 的缓存192.168.200.129/purge/ # 清除 key 为 www.domain.com/testpage2 页面缓存192.168.200.129/purge/testpage2 如果返回 则表示清除成功。 资源下载示例nginx配置文件","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"缓存","slug":"缓存","permalink":"http://nullpointer.pw/tags/缓存/"}]},{"title":"为什么我们会感觉到倦怠?","date":"2017-09-04T14:14:00.000Z","path":"为什么我们会感觉到倦怠？.html","text":"为什么产生倦怠？刚开始 接手新项目、学习新技能时，我们总是兴奋不已、踌躇满志、迫不及待地想大展身手。但是过了不久，新鲜感退却后，大部分热情就会消退，最终就会兴趣索然，积极性低，进展缓慢或者毫无进展。 这样的事情很多，比如： 刚开始学习驾驶，是不是很兴奋？过了多久之后，就不再真正关心你的车了？多久以后，你已经觉得它已经“老”了？ 雄心勃勃地想锻炼出一副好身材，办了健身卡后，去了几次之后就再没去过了？ …… 我们当开始倦怠，于是我们半途而废，努力寻找新事物，找回自己真正的激情。 开始的时候我们的兴趣最高，而动机在开始阶段可能不高，但是随着取得进展，我们的动机水平开始上升。早期的成功让你感觉到有动力，冲量则推动你前进。 但是随着时间的推移，缓慢的成果进展在不断消耗你的动机，最终会发现动机和兴趣都濒临谷底，这就是那堵墙，也就是我们感觉倦怠的原因。 但是如果我们穿过了这道墙，我们的动机、兴趣会随着成果的不断取得而不断上升加强，最终我们获得了最大的回报，只是能够穿过这道墙的人很少。 如何穿过这道墙？制定一个目标或计划，采取“定额工作法”，并坚决按照计划完成定额工作。 首先需要确立一个目标，规定自己要在预定确定的时间段内需要取得多大的进展。 比如： 每周跑步 3 次 每周写一篇博客 每天看半小时的书 每天弹半小时的吉他 将所有重复性工作的频率进行量化，有的一月一次，有的一周3次，有的则是每天两次。并承诺完成，无论刮风下雨。 承诺是定额工作法的核心，除了想方设法完成自己的工作，不给自己留下任何其他的选择。 失败不能被接受，因为你让自己失信一次，就会有第二次，很快定额在你眼中就会变得一文不值。 定额工作法的优点还可以帮你克服意志力薄弱的问题。通过预先设定好的必须要遵循的过程，消除需要作出决策的部分。 即不需要做判断——你知道必须要做，只需要遵从命令，避免了意志力耗尽的问题。","tags":[{"name":"读书","slug":"读书","permalink":"http://nullpointer.pw/tags/读书/"}]},{"title":"Nginx缓存配置","date":"2017-08-20T08:09:08.000Z","path":"Nginx缓存配置.html","text":"前言 本文介绍利用 nginx 的 nginx_ngx_cache_purge 模块来实现缓存功能，前几篇文章介绍了 Nginx 的动静分离以及 CDN 技术，在其基础上，再对整个页面进行缓存，将大大提高服务器的负载能力。 基础环境 CentOS 6.8 minimal Nginx 1.12.1 ngx_cache_purge-2.3 环境搭建Nginx 与 nginx_ngx_cache_purge 下载解压 安装 wget 1yum install wget 下载 Nginx 1wget http://nginx.org/download/nginx-1.12.1.tar.gz 解压 Nginx 到 /usr/local/src/ 目录 1tar -zxvf nginx-1.8.1.tar.gz -C /usr/local/src/ 下载 ngx_cache_purge 1wget http://labs.frickle.com/files/ngx_cache_purge-2.3.tar.gz 解压到 Nginx 同一级目录 1tar -zxvf ngx_cache_purge-2.3.tar.gz -C /usr/local/src/ 目录结构为123---/usr/local/src --nginx-1.8.1 --ngx_cache_purge-2.3 进入 Nginx 解压目录1cd nginx-1.8.1 安装必要依赖1yum -y install gcc pcre-devel openssl openssl-devel 检查安装环境,并指定将来要安装的路径和要安装的插件1./configure --prefix=/usr/local/nginx/ --with-http_realip_module --add-module=../ngx_cache_purge-2.3 参数说明：–prefix： 将 nginx 安装到此目录–with-http_realip_module 启用获取用户真实 IP 模块–add-module=../ngx_cache_purge-2.3 添加 ngx_cache_purge 模块，用于缓存 进行编译安装1make &amp;&amp; make install 编译完成后，没有出现 error 为编译安装成功 设置 Nginx 缓存编辑 nginx.conf 配置文件1vi /usr/local/nginx/conf/nginx.conf 添加如下配置 12345678910111213141516171819202122232425262728293031# 需要在 /tmp 目录下创建 cache 文件夹！#设置缓存区的名称为 tmpcache，内存缓存空间大小为 200MB，1 天没有被访问的内容自动清除，硬盘缓存空间大小为 5GB， 如果访问频率不高的话， inactive 时间可以设置长一些。proxy_cache_path /tmp/cache levels=1:2 keys_zone=tmpcache:200m inactive=1d max_size=5g;server &#123; listen 80; server_name www.domain.com;#你的域名 charset utf8; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Up-Calling-Line-Id $http_x_up_calling_line_id; proxy_ignore_headers X-Accel-Expires Expires Cache-Control Set-Cookie; access_log logs/access.log main; location / &#123; proxy_pass http://127.0.0.1:8080; #proxy_cache 的值是 proxy_cache_path 中的 keys_zone 的值 proxy_cache tmpcache; #缓存 key 的值 proxy_cache_key $host$uri$is_args$args; proxy_cache_valid 200 301 5m; proxy_cache_use_stale updating error timeout invalid_header http_500 http_502 http_503 http_504; expires 1m; # 将缓存的状态添加到 Header 中 add_header X-Cache '$upstream_cache_status'; &#125;&#125; 启动 Nginx1/usr/local/nginx/sbin/nginx 启动 Tomcat可以使用自己的项目，或者使用我提供的项目来进行测试。 1wget http://img12345.5-project.com/SpringDemo.war 将下载的 war 包放到 Tomcat 的 webapps 目录下，修改 war 包名称为 ROOT.war ，启动 Tomcat。 测试 Nginx 缓存缓存之前 查看 /tmp/cache 目录， 目录下面没有任何文件 修改本机（windows）的 hosts 文件，将配置的域名指向服务器 IP 1192.168.200.129 www.domain.com 打开浏览器访问 www.domain.com/testpage1 发现 X-Cache 的状态为 MISS ，说明没有缓存。 再次刷新页面后 X-Cache 的状态已经变成了 HIT 同时在服务器上查看 /tmp/cache 目录 cd /tmp/cache/ 发现该目录下面已经多了文件目录，进去之后打开文件，发现正是刚刚访问页面的源码，说明 Nginx 的缓存已经配置成功了。只要是配置的域名下面的页面， 如果没有进行过滤，都会缓存到服务器。当第二次访问的时候，请求经过 Nginx 服务器时，Nginx 直接返回已经缓存的静态页面，这样将大大降低应用服务器 Tomcat 的负载。 需要注意的是：并不是该域名下的所有页面都需要进行缓存，如果缓存有误，会出现很严重的问题。比如不正确的将域名下的用户中心进行了缓存，当其他用户也进入页面的时候，显示的全部都是第一个进入该页面用户的信息。因此建议做缓存时，最好根据域名来做， 根据用户不同显示不同的页面的地址就不要做缓存。当然，如果接口地址规范，亦可以通过 Nginx 正则匹配路径的方式，只将某些匹配的路径做缓存。","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"缓存","slug":"缓存","permalink":"http://nullpointer.pw/tags/缓存/"}]},{"title":"阿里云CDN实现网站文件动静分离","date":"2017-08-19T23:21:04.000Z","path":"阿里云CDN实现网站文件动静分离.html","text":"前言上一篇文章介绍了如何利用 Nginx 来实现静态资源请求与动态请求的分离，虽然不是所有的请求都压到 Tomcat 应用服务器，但是静态资源请求还是都会到 主机上给主机带来压力，因此本文通过利用阿里云的 CDN 技术来更加彻底地降低应用访问的负载。 配置静态资源域名建立一个只用于请求静态资源文件的二级域名，将动态请求与静态请求分离开。 比如静态资源请求的域名是 res.domain.com ， 动态请求的域名是 www.domain.com 配置 Nginx 静态资源设置参考我的上一篇文章 Nginx动静分离 阿里云 CDN 配置登录阿里云 CDN 控制台，添加域名。 需要注意的是资源文件只需要放到一台主机上即可，方便管理。 添加 CDN 域名成功后，回到域名管理页面， 在域名列表中找到新加的域名，光标悬浮到该域名上，会出现一个 复制 CNAME 的按钮，选择复制，会得到一个类似于 res.domain.com.w.kunlunle.com 的域名。 DNS 配置进入你的域名的 DNS 服务提供商控制台，我这里是 DNSPOD ， 找到静态资源域名，我的是 res.domain.com 修改记录类型为 CNAME ，记录值修改为上一步复制的 域名，如 res.domain.com.w.kunlunle.com。 选择保存，等待几分钟 DNS 生效后，访问域名试试吧！ 比如我的 res.domain.com/_res/a.jpg （需要事先在资源服务器上事先放置该静态文件） 如果访问不到说明配置有误或者缓存还未生效。这里有一个坑就是，自己的电脑有可能因为缓存没刷新访问不到该路径静态资源，而别人的电脑可以，可以换一台电脑尝试访问，或者通过手机浏览器访问尝试。 End。","tags":[{"name":"动静分离","slug":"动静分离","permalink":"http://nullpointer.pw/tags/动静分离/"}]},{"title":"Nginx动静分离","date":"2017-08-19T09:44:23.000Z","path":"Nginx动静分离.html","text":"前言最新公司新开了一个项目，项目由我来配置基础环境，会写一系列的教程来展示，一个网站从零到上线的整个过程。首先本文介绍资源文件的隔离。 为什么要实现动静分离为什么要将静态资源文件与动态请求进行分离？ 为了降低 Tomcat 服务器的负载，可以通过本文设置并结合 CDN 来实现来更彻底地降低服务器的负载。 如图所示，所有的请求都压到了同一台服务器上， 无论静态还是动态的请求，这样流量大的话势必会造成服务器的负载过高，因为动静分离是必不可少的。 环境介绍首先这是我们项目的目录结构， resource 目录和 _res 目录中都是静态资源文件，存放比如 js/css/img 等资源。这两个目录下的文件有所不同，_res 目录下面是压缩版本的，比如压缩的js/css。通过 Git 或者 SVN 将项目上传到服务器上后，配置 Nginx 来完成静态资源的分离。 首先看一张图(图的来源是：http://www.cnblogs.com/xiaoblog/p/4241086.html) 当请求经过 Nginx 时，如果是静态资源的请求，就直接返回主机上静态资源即可， 如果是动态请求，则去请求 Tomcat 服务器后再返回。如果流量不是很大，静态资源可以和 Tomcat 服务器放在同一台主机上，否则需要将静态资源与 Tomcat 服务器分开。 我司对于资源文件的做法是新建一个二级域名，该域名只用于加载静态资源文件。如 res.domain.com，这样做的好处是简化了在 Nginx 的 配置。 配置 Nginx首先需要安装 Nginx， 可以参考我的另外一篇文章 Nginx实现Tomcat集群。这里只需要参考到安装 Nginx 即可。 Nginx 静态资源配置 打开 Nginx 配置文件 vim /usr/local/nginx/conf/nginx.conf 添加 server 1234567891011121314151617181920212223242526272829303132333435server &#123; # 只接收静态资源请求 listen 80; server_name res.domain.com; charset utf8; default_type text/plain; add_header Access-Control-Allow-Origin *; #access_log logs/heiyan.image.access.log click; location ~* \\.(zip|rar|gz|tar.gz|gz|exe)$ &#123; return 500; &#125; location / &#123; #资源文件的目录 root /home/resources; index index.html; expires 30d; default_type text/plain; &#125;&#125;server &#123; # 接收动态请求 listen 80; server_name www.domain.com; charset utf8; access_log logs/access.log main; location / &#123; proxy_pass http://127.0.0.1:8080; &#125;&#125; 拷贝项目中资源文件到指定的资源目录在 Nginx 的配置文件中指定了资源文件的目录为 /home/resources ，所以需要将资源拷贝到该目录下，进入上文下载的项目的目录中，将资源目录（我这里是_res目录)拷贝到 /home/resources目录下面。 cp -rf _res /home/resources 拷贝完成后的目录应该是： /home/resources/_res 在 /home/resources/_res 目录下放一张图片 a.jpg ，用于测试。 测试静态资源获取启动 Nginx 打开浏览器访问： http://res.domain.com/_res/a.jpg 如果是动态请求，则使用 http://www.domain.com 域名 返回结果： 因为静态资源文件包含 css/js 等文件，修改提交后需要拷贝增量与修改的文件到 /home/resources 目录下，如果嫌麻烦，可以直接在 Nginx 配置文件中，配置 路径为项目的资源文件夹根路径，比如 /home/wide-api/src/main/java/resources/_res， 这种在更新代码之后，可以直接请求到新的资源文件。之后会写一篇通过阿里云CDN来完成静态分离。 注： 由于我的是在虚拟机上的 Nginx 进行测试，也没有域名，且是minimal版本系统，没有桌面环境，所以需要配置宿主机的hosts， 在宿主机的浏览器中访问。在真实服务器且有自己绑定到服务器上域名的就直接测试即可。 使用 SwitchHosts 软件 配置 hosts，我的虚拟机 ip 为192.168.200.129 1192.168.200.129 www.domain.com res.domain.com","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"动静分离","slug":"动静分离","permalink":"http://nullpointer.pw/tags/动静分离/"}]},{"title":"CentOS快速安装","date":"2017-08-18T23:40:27.000Z","path":"CentOS快速安装.html","text":"前言最近学习过程中需要使用到 Linux 系统，但是并没有服务器，因此通过虚拟机来完成。本文不从头开始介绍虚拟机的安装，只讲解 虚拟机自制镜像 备份的快速还原，如果你需要从头开始安装的，请另寻教程。本文中涉及的镜像会提供下载地址，以供参考。 学习过程中安装虚拟机时总是会遇到一些需要花费大量时间才能解决的问题，比如不能联网、数据库连接不上、ssh 连接不了等问题，为了便于学习，本人将安装好的系统导出为镜像文件，以后如果需要添加虚拟机时，直接还原镜像修改部分配置即可，以下为教程。 环境准备 虚拟机软件 VMware Workstation 11 系统镜像版本 CentOS 6.8 minimal (命令行版本，无桌面) 自定义镜像以及VMware下载 链接：http://pan.baidu.com/s/1nuKxhpf 密码：8dt8若链接失效，发邮件给我补链接 联系我 VMware 配置需要安装的虚拟机采取 NAT 方式联网，因此 需要在 VMware 中配置网关地址。 打开虚拟网络编辑器，设置子网 IP 点击 NAT 设置，设置网关地址IP （网关相当于一个路由器），注意网关 IP 要和子网 IP 处于同一个网段。如192.168.200.x 导入镜像文件依次选择菜单， 文件–&gt;打开–&gt; 找到下载镜像解压的文件夹，选择192.168.200.129.ovf 文件 设置虚拟机名称和虚拟机保存路径，选择导入。 稍等一会儿导入成功后，开启虚拟机。 虚拟机网络配置登录账号：root 密码：123456 刚安装完是无法 ping 通外网的，需要修改网络配置， 当前镜像已经设置了静态 ip，便于使用。 修改 70-persistent-net.rules 文件 vi /etc/udev/rules.d/70-persistent-net.rules 注释掉 eth0 所在行，结果应该为：1234# PCI device 0x8086:0x100f (e1000)#SUBSYSTEM==\"net\", ACTION==\"add\", DRIVERS==\"?*\", ATTR&#123;address&#125;==\"00:0c:29:30:a4:b5\", ATTR&#123;type&#125;==\"1\", KERNEL==\"eth*\", NAME=\"eth0\"# PCI device 0x8086:0x100f (e1000)SUBSYSTEM==\"net\", ACTION==\"add\", DRIVERS==\"?*\", ATTR&#123;address&#125;==\"00:0c:29:cd:05:ea\", ATTR&#123;type&#125;==\"1\", KERNEL==\"eth*\", NAME=\"eth1\" 修改网络配置文件，设置静态IP地址和网关地址vi /etc/sysconfig/network-scripts/ifcfg-eth0 1234567891011DEVICE=eth1 # 修改eth0 为eth1TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=192.168.200.132 # 修改为想要的静态ip地址， 如果网关地址为192.168.100.2 则ip地址为 192.168.100.x x为1-254，且不与网关地址重复GATEWAY=192.168.200.2 # 修改为你虚拟机的网关地址 IPNAME=\"System eth0\"NETMASK=255.255.255.0DNS1=8.8.8.8DNS2=4.4.4.4 重启虚拟机 reboot 这是必须的步骤！ 检验是否修改生效 重启登录成功后， 输入 ip addr， 如果你显示的和我一致，则是正确的。 如果你的出现了两条 inet， 说明第 1 步， 和第 2 步没有配置正确。此时 ping 一下百度，发现已经可以 ping 通了。 至此，虚拟机已经配置好了。可以通过 SSH 软件进行连接了。 建议：虚拟机安装完成后，对其进行快照，防止弄坏了还需要重新安装。 在快照管理器中，可以选择保存的快照，还原到快照时的系统。","tags":[{"name":"centos","slug":"centos","permalink":"http://nullpointer.pw/tags/centos/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://nullpointer.pw/tags/虚拟机/"}]},{"title":"Java之word导出下载","date":"2017-07-27T14:12:07.000Z","path":"Java之word导出下载.html","text":"前言最近遇到项目需求需要将数据库中的部分数据导出到 word 中，具体是在一个新闻列表中将选中的新闻导出到一个 word 中。参考了网上一些教程，实现了该功能，在此记录下来。 导出结果如下： 图中为导出的其中两条新闻。 搜索网上导出 word 的方式有很多种，但是很多都是一笔带过，有示例代码的只找到了 POI 导出，和通过 FreeMarker 方式导出，但是只是具有参考意义。本文采取使用 FreeMark 方式。 实现步骤 Maven 工程引入FreeMarker 的依赖，非 Maven 工程添加 jar 包 12345&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.26-incubating&lt;/version&gt;&lt;/dependency&gt; 创建 word 模板 新建 word 替换内容为占位符 另存模板为 XML 文件 使用 NotePad++ 打开 xml 文件 选中全部内容，到这里进行格式化 将原内容替换为格式化后的内容 因为我的 word 的内容是一个列表，所以需要添加一个 freemarer 标签标识 找到&lt;w:document&gt; 元素下面的 &lt;w:body&gt; 元素，添加&lt;#list newsList news&gt; ， 并在&lt;/w:body&gt;结束标签之前闭合 &lt;/#list&gt;， 此处的 newsList 为后台读取模板时需要需要渲染数据map集合的key， 其所对应的是一个list集合。 替换模板中的占位符 &lt;w:t&gt;${news.title}&lt;/w:t&gt; 与 &lt;w:t&gt;${news.content}&lt;/w:t&gt; 为遍历集合的值，这里的 FreeMarker 语法可以参考网上的教程。 保存为 FreeMarker 的模板文件，后缀为 ftl 格式，拷贝到项目中 编写代码 从数据中查询数据集合放入Map中， 调用工具方法，返回流 12345Map&lt;String, Object&gt; root = new HashMap&lt;String, Object&gt;();root.put(\"newsList\", newsList);//newsList为新闻对象集合String template = \"/temp.ftl\"; //模板文件的地址ByteArrayOutputStream outputStream = WordUtil.process(root, template);return outputStream; 调用下载工具类进行下载即可。 1DownloadUtil.download(byteArrayOutputStream, response, returnname); 注：在实现功能的时候，由于采取的是 ajax 请求方式，导致只是将流写入 Response 时， Response 为 xml 格式的数据。但是想要实现的效果是弹出下载框，下载 word 文档。最后查询资料，修改ajax请求为form表单提交方式(ajax form)，才弹出下载框实现了功能。 文章涉及工具类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//WordUtil.javaimport java.io.BufferedWriter;import java.io.ByteArrayOutputStream;import java.io.OutputStreamWriter;import java.io.Writer;import java.util.Map;import freemarker.template.Configuration;import freemarker.template.Template;public final class WordUtil &#123; private static Configuration configuration = null; private WordUtil() &#123; throw new AssertionError(); &#125; /** * 根据模板生成相应的文件 * @param root 保存数据的map * @param template 模板文件的地址 * @param path 生成的word文档输出地址 * @return */ public static synchronized ByteArrayOutputStream process(Map&lt;?, ?&gt; root, String template) &#123; if (null == root ) &#123; throw new RuntimeException(\"数据不能为空\"); &#125; if (null == template) &#123; throw new RuntimeException(\"模板文件不能为空\"); &#125; ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); String templatePath = template.substring(0, template.lastIndexOf(\"/\")); String templateName = template.substring(template.lastIndexOf(\"/\") + 1, template.length()); if (null == configuration) &#123; configuration = new Configuration(Configuration.VERSION_2_3_23); // 这里Configurantion对象不能有两个，否则多线程访问会报错 configuration.setDefaultEncoding(\"utf-8\"); configuration.setClassicCompatible(true); &#125; configuration.setClassForTemplateLoading(WordUtil.class, templatePath); Template t = null; try &#123; t = configuration.getTemplate(templateName); Writer w = new BufferedWriter(new OutputStreamWriter(outputStream, \"utf-8\")); t.process(root, w); // 这里w是一个输出地址，可以输出到任何位置，如控制台，网页等 w.close(); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; return outputStream; &#125;&#125; 1234567891011121314151617181920212223242526272829//DownloadUtil.javaimport java.io.ByteArrayOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.OutputStream;import javax.servlet.ServletOutputStream;import javax.servlet.http.HttpServletResponse;import org.apache.commons.io.FileUtils;public class DownloadUtil &#123; /** * @param byteArrayOutputStream 将文件内容写入ByteArrayOutputStream * @param response HttpServletResponse 写入response * @param returnName 返回的文件名 */ public static void download(ByteArrayOutputStream byteArrayOutputStream, HttpServletResponse response, String returnName) throws IOException&#123; response.setContentType(\"application/msword\"); response.setHeader(\"Content-Disposition\", \"attachment; filename=\" + returnName); response.setContentLength(byteArrayOutputStream.size()); OutputStream outputstream = response.getOutputStream(); //取得输出流 byteArrayOutputStream.writeTo(outputstream); //写到输出流 byteArrayOutputStream.close(); //关闭 outputstream.flush(); //刷数据 &#125;&#125; 源码下载点我下载 参考链接 http://itindex.net/detail/55080-springboot-freemarker-%E6%A0%BC%E5%BC%8F","tags":[{"name":"word","slug":"word","permalink":"http://nullpointer.pw/tags/word/"},{"name":"freemarker","slug":"freemarker","permalink":"http://nullpointer.pw/tags/freemarker/"}]},{"title":"Java配置分离之Spring远程配置","date":"2017-07-16T01:14:30.000Z","path":"Java配置分离之远程配置.html","text":"前言集群应用的配置文件如果写在项目的 resources 目录下面，当遇到需要修改某一个配置值时，需要将集群的所有应用的配置信息进行修改，并且将机密的配置信息比如数据库账号密码如果不进行加密配置在项目中很危险，一旦发生代码泄露问题，后果很严重。 为了避免上述情况发生，将配置信息存储到数据库中，比如数据库连接、用户名、以及密码，通过 Config 项目的一个接口提供获取配置信息。Config 项目只用于读取配置信息。 远程配置一）新建类 RemoteProperties 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;public class RemoteProperties implements InitializingBean, FactoryBean&lt;Properties&gt; &#123; private String url = null; private Properties properties = new Properties(); @Override public Properties getObject() throws Exception &#123; return properties; &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return properties.getClass(); &#125; @Override public boolean isSingleton() &#123; return true; &#125; @Override public void afterPropertiesSet() throws Exception &#123; loadProperty(); &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; private void loadProperty() &#123; if (StringUtil.strIsNull(url)) return; String content = HttpClientUtil.urlGet(url); JSONObject object = JSONObject.parseObject(content); JSONArray data = object.getJSONArray(\"datasource\"); for (Object obj : data) &#123; JSONObject jsonObject = (JSONObject) obj; String key = obj.getString(\"key\"); String value = obj.getString(\"value\"); properties.put(key, value); &#125; &#125;&#125; 此类用于发送请求获取配置信息，请求返回格式为 JSON 的配置信息, 如：1234567891011121314151617181920&#123; \"datasource\":[ &#123; \"value\":\"com.mysql.jdbc.Driver\", \"key\":\"jdbc.driver\" &#125;, &#123; \"value\":\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8\", \"key\":\"jdbc.url\" &#125;, &#123; \"value\":\"root\", \"key\":\"jdbc.username\" &#125;, &#123; \"value\":\"root\", \"key\":\"jdbc.password\" &#125; ]&#125; 二）编写 Spring 配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:p=\"http://www.springframework.org/schema/p\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd\"&gt; &lt;bean id=\"propertyConfigurerUserServer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"properties\"&gt; &lt;bean id=\"remoteProperties\" class=\"com.craft.partner.server.util.RemoteProperties\" p:url=\"http://api.xxx.com/config\"/&gt; &lt;!--远程配置提供接口--&gt; &lt;/property&gt; &lt;!--还可以加载本地的properties文件--&gt; &lt;property name=\"locations\"&gt; &lt;list&gt; &lt;value&gt;classpath:configure.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\" destroy-method=\"close\" p:driverClass=\"$&#123;jdbc.driver&#125;\" p:jdbcUrl=\"$&#123;jdbc.url&#125;\" p:user=\"$&#123;jdbc.username&#125;\" p:password=\"$&#123;jdbc.password&#125;\"&gt; &lt;property name=\"initialPoolSize\" value=\"10\" /&gt; &lt;property name=\"minPoolSize\" value=\"10\" /&gt; &lt;property name=\"maxPoolSize\" value=\"50\" /&gt; &lt;property name=\"maxStatements\" value=\"0\" /&gt; &lt;property name=\"maxIdleTime\" value=\"600\" /&gt; &lt;property name=\"idleConnectionTestPeriod\" value=\"300\" /&gt; &lt;property name=\"acquireIncrement\" value=\"5\" /&gt; &lt;property name=\"autoCommitOnClose\" value=\"true\" /&gt; &lt;property name=\"checkoutTimeout\" value=\"2000\" /&gt; &lt;/bean&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\" /&gt; &lt;tx:method name=\"update*\" propagation=\"REQUIRED\" /&gt; &lt;tx:method name=\"delete*\" propagation=\"REQUIRED\" /&gt; &lt;tx:method name=\"find*\" read-only=\"true\" /&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\" read-only=\"true\"/&gt;&lt;!--其他不符合规范的方法只允许读操作--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config expose-proxy=\"false\"&gt; &lt;aop:pointcut id=\"serviceMethod\" expression=\"execution(* com.craft.partner.server.service.*.*(..))\"/&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"serviceMethod\"/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 注意： 通过 p:url=”地址” 的方式，调用 RemoteProperties 的方法，发送请求获取配置信息，通过 Spring 进行注入。注入后，可以通过 p:属性 的方式获取配置。 参考 http://www.cnblogs.com/yjmyzz/p/how-to-load-remote-config-in-spring.html","tags":[{"name":"远程配置","slug":"远程配置","permalink":"http://nullpointer.pw/tags/远程配置/"}]},{"title":"Java上传图片到OSS的实现","date":"2017-05-06T15:01:57.000Z","path":"Java上传图片到OSS的实现.html","text":"前言： 以前做项目用的都是开源的分布式系统，如FastDFS。最近几年，随着云的发展，国内外的巨头公司也相应的推出了相关商用的类似产品。比如Amazon S3，阿里云的OSS，腾讯云的COS。今天我选择使用阿里云的OSS来作为例子，来实现图片的上传。 准备工作阿里云OSS官方文档地址 一、开通阿里云OSS 登录阿里云控制台开通OSS服务，并创建AccessKeyId和AccessKeySecret 在OSS控制面板上新建Bucket 进入新建的Bucket选择左侧的Object管理，新建一个img的文件夹，便于存储归类 二、maven项目中引入依赖在maven工程中的pom.xml添加依赖12345&lt;dependency&gt; &lt;groupId&gt;com.aliyun.oss&lt;/groupId&gt; &lt;artifactId&gt;aliyun-sdk-oss&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt; 编写代码 注：我所使用的的框架是：SpringMVC+Spring+Mybatis 一、上传文件到OSS之HelloWorld上传图片之前，先惯例来一个HelloWorld示例！1234567891011121314151617181920212223242526272829303132333435363738394041import com.aliyun.oss.OSSClient;import com.aliyun.oss.model.ObjectMetadata;import com.aliyun.oss.model.PutObjectResult;import com.nullpointer.common.Constants;import com.nullpointer.service.AliyunOSSService;import org.springframework.stereotype.Service;import org.springframework.web.multipart.MultipartFile;import javax.annotation.PostConstruct;import java.io.File;import java.io.IOException;import java.io.InputStream;@Servicepublic class AliyunOSSServiceImpl implements AliyunOSSService &#123; public static final String IMG = \"img/\";//对应Object的文件夹名称 private static final int maxLength = 5 * 1024 * 1024;//简单上传OSS限制为5G以下 public static final String ENDPOINT = File.separatorChar == '/' ? \"oss-cn-hangzhou-internal.aliyuncs.com\" : \"http://oss-cn-hangzhou.aliyuncs.com\";//用于选择不同的域名 private OSSClient client; private static String bucketName; /** * 初始化OSSClient */ @PostConstruct public void init() &#123; String accessId = \"xxxxxxxxxxxxx\";//你的AccessId， 在OSS控制台可以生成 String accessKey = \"xxxxxxxxxxxxxxxxx\";//你的AccessKey bucketName = \"xxxxx\";//创建的Bucket的名称，注意要一致区分大小写 client = new OSSClient(ENDPOINT, accessId, accessKey);//初始化 &#125; public String uploadIm() &#123; String content = \"Hello OSS\"; PutObjectResult test01 = client.putObject(bucketName, \"test01\", new ByteArrayInputStream(content.getBytes())); return test01.getETag(); &#125;&#125; 打开OSS控制台的Object管理，是不是发现新增了一个文件呢？ 二、编写图片上传服务类HelloWorld示例完成了，开始正式编写图片上传代码了。注释都有。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Servicepublic class AliyunOSSServiceImpl implements AliyunOSSService &#123; public static final String IMG = \"img/\";//对应Object的文件夹名称 private static final int maxLength = 5 * 1024 * 1024;//简单上传OSS限制为5G以下 public static final String ENDPOINT = File.separatorChar == '/' ? \"oss-cn-hangzhou-internal.aliyuncs.com\" : \"http://oss-cn-hangzhou.aliyuncs.com\";//用于选择不同的域名 private OSSClient client; private static String bucketName; /** * 初始化OSSClient */ @PostConstruct public void init() &#123; String accessId = \"xxxxxxxxxxxxx\";//你的AccessId， 在OSS控制台可以生成 String accessKey = \"xxxxxxxxxxxxxxxxx\";//你的AccessKey bucketName = \"xxxxx\";//创建的Bucket的名称，注意要一致区分大小写 client = new OSSClient(ENDPOINT, accessId, accessKey);//初始化 &#125; /*public String uploadIm() &#123; String content = \"Hello OSS\"; PutObjectResult test01 = client.putObject(bucketName, \"test01\", new ByteArrayInputStream(content.getBytes())); return test01.getETag(); &#125;*/ @Override public String saveImage(MultipartFile file) &#123; try &#123; String fileName = this.saveImage(file.getSize(), file.getOriginalFilename(), file.hashCode(), file.getInputStream()); return getHttpUrl(fileName); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String getHttpUrl(String fileName) &#123; return Constants.OSS_HTTP + \"/\" + IMG + fileName; &#125; @Override public String saveImage(long size, String extName, int hashCode, InputStream is) &#123; if (size &gt; maxLength)//超过5G return null; String name = extName.toLowerCase(); name = name.substring(name.indexOf(\".\"));//获得上传文件的后缀名称如 .jpg String fileName = System.currentTimeMillis() + \"_\" + hashCode + name;//为了防止重复， 使用时间戳+hashcode的方式来命名 return this.saveImage(size, fileName, is); &#125; @Override public String saveImage(long size, String fileName, InputStream is) &#123; String key = IMG + fileName;//保存到img文件夹下 this.uploadObject(key, size, is); return fileName; &#125; @Override public void uploadObject(String key, long size, InputStream is) &#123; ObjectMetadata metadata = new ObjectMetadata();//设置文件的meta信息 metadata.setContentLength(size); metadata.setContentType(\"image/jpeg\"); PutObjectResult result = client.putObject(bucketName, key, is);//调用SDK的上传方法 System.out.println(result); &#125;&#125; 三、编写Controller层的上传接口123456789101112@RequestMapping(value = \"/upload\", method = RequestMethod.POST)public void uploadOSS(MultipartFile pic, HttpServletResponse response, Model model) throws Exception &#123; String imgUrl = aliyunOSSService.saveImage(pic); System.out.println(\"path = \" + imgUrl); //图片回显 JSONObject jsonObject = new JSONObject(); jsonObject.put(\"path\", imgUrl); response.setContentType(\"application/json;charset=UTF-8\"); response.getWriter().write(jsonObject.toString());&#125; 需要注意的是：入参中的MultipartFIle是一个接口，需要在Spring的配置文件applicationContext.xml中将其实现类注入。1&lt;bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\"/&gt; 四、编写页面JSP12345678910111213141516171819202122232425262728293031323334353637&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"/static/js/jquery.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"/static/js/jquery.form.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;%@ page isELIgnored=\"false\" %&gt; &lt;title&gt;home&lt;/title&gt; &lt;script type=\"application/javascript\"&gt; function uploadPic() &#123; var options = &#123; url: \"/upload\", type: \"POST\", dataType: \"json\", success: function (data) &#123; $(\"#imgUrl\").attr(\"src\", data.path); &#125; &#125; $(\"#jform\").ajaxSubmit(options); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=\"container-fluid\"&gt; &lt;div class=\"row-fluid\" align=\"content\"&gt; &lt;form action=\"/uploadOSS\" method=\"POST\" id=\"jform\"&gt; &lt;div class=\"span12\"&gt; &lt;img alt=\"img\" class=\"img-polaroid\" id=\"imgUrl\"/&gt; &lt;input type=\"file\" name=\"pic\" onchange=\"uploadPic()\"/&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 页面不是我所擅长，能显示效果即可。 五、测试图片上传启动项目，进入图片上传的页面。 上传图片控制台打印了如下内容： 页面显示：","tags":[{"name":"OSS","slug":"OSS","permalink":"http://nullpointer.pw/tags/OSS/"},{"name":"对象存储系统","slug":"对象存储系统","permalink":"http://nullpointer.pw/tags/对象存储系统/"}]},{"title":"Nginx实现Tomcat集群","date":"2017-05-05T14:12:54.000Z","path":"Nginx实现Tomcat集群.html","text":"本文介绍了利用nginx搭建了一个简单的负载均衡。 一、准备工作Linux系统: CentOS 7 x64 Nginx版本:1.12.0 Tomcat版本: 7.0.77 二、安装CentOS 快速安装，见我另外一篇文章CentOS快速安装安装Nginx依赖1234567891011# gcc 用来编译nginxyum install gcc-c++# PCRE nginx的http模块使用pcre来解析正则表达式yum -y install pcre*# zlib nginx使用zlib对http的内容进行gzip压缩yum install -y zlib zlib-devel# openssl https支持yum -y install openssl 下载Nginx1234# 安装wgetyum install wget# 下载nginxwget http://nginx.org/download/nginx-1.12.0.tar.gz 编译安装Nginx123456789# 解压nginxtar -zxvf nginx-1.12.0.tar.gz# 进入解压目录cd nginx-1.12.0./configure# 若无错误，可以执行以下安装makemake install# 默认安装到/usr/local/nginx目录 Nginx测试12345678910111213141516# 启动nginx/usr/local/nginx/sbin/nginx# 访问localhostwget localhost# 此命令会下载一个index.html文件vim index.html# index.html中有关于nginx的类似于helloworld语句# 由于我的Linux安装在虚拟机中，想要外部访问虚拟机内，需要关闭防火墙或开放端口# 编辑防火墙， CentOS 7默认的防火墙不是iptables，需要另行安装，请自行解决vim /etc/sysconfig/iptables# 添加80端口-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT# 重启防火墙service iptables restart# 通过本机的浏览器访问虚拟机的iphttp://192.168.243.127(此处为虚拟机的ip) 三、配置用于集群的Tomcat1234567891011121314151617181920212223242526# 下载Tomcatwget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-7/v7.0.77/bin/apache-tomcat-7.0.77.tar.gz# 创建目录mkdir /usr/local/tomcats# 解压Tomcattar -zxvf apache-tomcat-7.0.77.tar.gz -C /usr/local/tomcats# 修改解压的Tomcat目录名称mv apache-tomcat-7.0.77 tomcat1# 为了测试nginx负载均衡,需要对tomcat的server.xml配置一下vim tomcat1/conf/server.xml# 显示行号:set nu# 在105行修改为如下， 并保存退出&lt;Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"jvm1\"&gt;# 修改示例程序echo 'tomcat1:&lt;%=session.getId()%&gt;' &gt; tomcat1/webapps/ROOT/index.jsp# 拷贝tomcat1目录为tomcat2cp -r tomcat1 tomcat2# 修改tomcat2的server.xml中的端口号vim tomcat2/conf/server.xml# 修改22行/71行/93行的端口# 修改105行&lt;Engine name=\"Catalina\" defaultHost=\"localhost\" jvmRoute=\"jvm2\"&gt;# 修改示例程序echo 'tomcat2:&lt;%=session.getId()%&gt;' &gt; tomcat1/webapps/ROOT/index.jsp 四、配置Nginx1vim /usr/local/nginx/conf/nginx.conf 启动tomcat123# 启动两个tomcat./tomat2/bin/startup.sh./tomat1/bin/startup.sh 测试外部访问虚拟机 http://192.168.243.127(此处为虚拟机的ip) 多刷新几次，可以看到页面的内容在变化，在tomcat1和tomcat2之间进行切换，至此完成了简单nginx负载均衡的搭建。","tags":[{"name":"nginx","slug":"nginx","permalink":"http://nullpointer.pw/tags/nginx/"},{"name":"tomcat","slug":"tomcat","permalink":"http://nullpointer.pw/tags/tomcat/"}]},{"title":"《聪明人都用方格笔记本》读书笔记","date":"2017-01-22T10:08:00.000Z","path":"《聪明人都用方格笔记本》读书笔记.html","text":"2017年读的第一本书，读完了一直没有写读书笔记，拖延到今天，才勉强写完。中途看到《如何有效阅读一本书》，就一起读了，学学读书笔记的写法。 此书主要讲了笔记本对于笔记的影响，好的笔记本让笔记发挥它应有的功能，而不好的笔记本体现不出笔记的价值所在。作者以三个著名机构使用的笔记本为例，列举了方格笔记本的种种益处，并通过配图的方式，让读者一目了然地了解方格笔记本记录笔记的方式。 书名 作者 开始时间 结束时间 聪明人都用方格笔记本 高桥政史 2017.01.01 2017.01.04 将方格笔记本“分为三部分”记录。从左向右依次将笔记本分为三部分，按照“事实→解释→行动”的顺序记录笔记。 一）笔记本的区别1. ☆笔记本的对比 使头脑变聪明的笔记本 阻碍能力发挥的笔记本 对比 看起来工整 第一印象给人脏乱的感觉，使人不愿再看第二眼 理解力、积极性低下 比A4纸大 比A6纸小、记事本型 思考复杂问题的能力、逻辑思维能力低下 颜色不超过三种 用了四种以上的颜色 判断优先顺序的能力、判断力低下 每一页都有一个主题 这也写、那也写 舍弃能力、整理能力低下 是黑板、白板板书内容的提炼 照抄黑板、白板板书 记忆力、独立思考能力低下 留有空白区域 无空隙，写得满满当当 理解能力、复习能力低下 画有许多图、表、画 无图、表、画，只有文字 视觉把握能力、表现力低下 日后看笔记时依然可再现当时的内容 日后看笔记时无法再现当时的内容 学习能力、理解力低下 2. ★好与坏的笔记本的区别好笔记本看上去一目了然，能够迅速把握重点，复现记笔记时的场景；而坏笔记就是让你根本没有看的欲望，即使是看了，也不能提取有效内容，大多数情况下，坏笔记本的作用就是没有作用。 二）记笔记的目的1. ☆记笔记的误区不管你参加的是多么优秀的培训、讲座、研讨，如果你的笔记无法再现当时的学习内容，那么，你好不容易学到的知识、技巧化为乌有的可能性将很大。 笔记的生命线是“再现性”，这是记笔记的最终目的。 可是，没有学习过笔记本使用方法的人日后再看自己的笔记时，只能看到上面的词语和句子，却回忆不出“为何需要这些信息”“这些信息有什么作用”这些更为重要的部分。 也就是说，他们用的是缺乏再现性的记笔记法。 为什么要做笔记？ 我是怕日后忘记才记的 我只是照抄板书罢了 我有记笔记的习惯 2. ★记笔记需要克服的困难作者说，抱着以上这几种目的而做笔记的人需要变化记笔记的方式，应当停止当前这种记笔记的方法。一直以来，记了很多笔记，但是我每次记完的笔记只是当成了备忘，把笔记本当成了字典。 需要使用以前记过的笔记的时候，就借助个人良好的分类习惯查找对应的笔记，以及借助软件的全文检索功能查找我所需要的笔记。 长而久之，便养成了依赖的习惯，记笔记记完也只是在脑海中保留一份关键字，而内容却一概忘记，这恐怕是我的记笔记方式的最大弊病，再现笔记的内容是个人需要克服的难点。 三）框架·黄金三分法1. ☆谁采取了这种框架 康奈尔笔记本分为“Note”（板书）、“Queue”（发现点）、“Summary”（总结）三部分。 东大录取生中的大多数学生将笔记本的左右两页作为一页使用，左侧记录“板书”，右侧写“发现点”和“总结”，和康奈尔笔记本的构造完全相同。 麦肯锡公司的“空·雨·伞”，埃森哲公司的“Point Sheet”，所有外资管理咨询公司的咨询顾问都是按照“事实→解释→行动”的三分法展开思考的。 2. ☆框架的组成 内容 对应 板书 事实 发现 解释 总结 行动 3. ★框架的使用步骤 摘抄原文：加深对原文的印象 提出问题，做出解释：基于原文进行思考，并给出结果 进行总结：对于整篇文章的总体概述 按照黄金三分法来做笔记，可以提高笔记对知识的再现性，知名机构都采取的是黄金三分法这种框架来做笔记。 四）笔记本的结构1. ☆笔记本的方向人的思考会受到眼睛构造的左右，所以进入视野的是“纵向”框架还是“横向”框架，能够影响人们的理解速度以及对信息的把握程度。 在把握整体情况时，需要解决的问题越复杂、参考的信息量越庞大，就越需要在短时间内迅速抓住重点。 随着学习、工作层次的提升，“既见树木，又见森林”这一思考方式的重要性也就更为突显。 2. ☆笔记本的大小“不善于提炼想法”“很难按逻辑性思维思考”“不会言简意赅地表达”的原因是什么？ 笔记本的大小意味着思考能力的强弱。 3. ☆笔记的颜色用黑色或蓝色书写笔记，用红色作为判断，也就是遇到非常重要或需要修改的地方时使用红色。 用此种简单方法区分使用颜色的话，便会在记笔记的同时自然舍弃掉无用的内容，将思考重点放在主要问题上，养成判断主次先后的习惯，逐步提高解决问题的速度和质量。 颜色最多用三种。 只要能做到这一点，事后再翻看笔记时，仅看颜色就能立刻区分出主次顺序，记笔记时的思路也能在大脑中清晰重现，进一步提升信息整理和输出的效率。 4. ☆笔记标题区域的利用通常会在笔记本上部的空白区域写入论点和结论。以解决工作问题的笔记本为例，“论点=问题的核心是什么”“结论=这样做便可以解决问题”。 5. ★笔记本结构对于笔记的影响 笔记采取横向结构，可以起到一目了然的作用，参考的文章信息便于提取到大脑中。 小笔记本不利于总结想法，来回地翻页会打断思考的书写。因此使用大笔记本来记笔记。 太多的颜色会导致找不到重点，处处是重点的情况， 合理标记标题，梳理核心内容，有助于快速理解笔记内容。 五）笔记本的功能1. ☆记忆性笔记本上课、研讨或是要记忆、理解学习内容时记录的笔记，为了记在脑子里而写的笔记。本书将这类笔记本称为“学习笔记本”。 2. ☆思考性笔记本为看出事物本质、理解重点、得出结论而写的笔记，常见于工作中。本书将这类笔记本称为“工作笔记本”。 3. ☆传达性笔记本传达性笔记即能够从丰富的信息中筛选出对方所需的改善策略及解决方法，为了说服对方而写的笔记。本书将写这类笔记的笔记本称为“提案笔记本”或“博弈笔记本”。 4. ★不同功能笔记本对的记法作者的这本书所描述的黄金三分法最适合于做思考性笔记，而对于记忆性笔记本的记法，采取这种方式，也能起到一定加深印象的作用。对于传达性笔记本而言，既列出事实，又给出合理解释，也是不错的方式。 六） 提高笔记本的作用1. ☆能形成“永久记忆”的用眼方法如果你之前是一边看着老师在黑板上写下的内容，一边原封不动地抄写在笔记本上的话，那么，请你改变一下抄写笔记时的“用眼方法”吧！ 将此前：×“看黑板→写笔记”的记录方法改为： “看黑板→印在大脑中→不看黑板，将印在大脑中的内容重现在笔记本上”。 不是一边看黑板一边抄写，而是在“看黑板→抄写”之间加入“印在大脑中的一秒钟”。 我把这一秒钟称为“空白的一秒钟”。 只需要这一秒钟，你的笔记本就能升级成能使大脑越变越聪明的笔记本。 2. ★方法的效果在抄写的这一步骤中加入映入大脑，可以加深对所抄写内容的记忆。 记得以前在背日语单词时，采取的就是这种方法，看一下单词，然后闭上眼睛，让单词在脑海中浮现出来，效果比直接抄写到本上的方法要好。 3. ☆逻辑连接词的种类与用途 4. ★连接词的效果找到适合自己的逻辑连接词，将思考进行流程化。（在中部区域使用） 5. ☆三个箭头的使用 6. ☆两个能力的实质 提问能力 概括能力 所谓“提问能力”，就是得到某个信息后并不会直接一口吞下，而是会提出类似“这到底是什么意思”“为什么会这么说”之类问题的能力。 所谓“概括能力”，就是能看透事物本质，并且将梳理后的重点内容记在脑子里，然后简单明了地讲给别人听。 7. ★两个能力的作用如何应用学习的知识，是由这两个能力来决定。这两个能力的提高与每天的脑力训练是密不可分的，只有不断地联系，逐渐强化，这二者的能力对于个人的提升就会越发明显。 8. ☆不要让你的笔记本变得代谢不良“代谢不良笔记本”是不分有用信息和无用信息、什么都往上写的、不进行整理总结的“阻碍能力发挥的笔记本”的典型例子。 如果这也记、那也记的话，那么不知不觉中，笔记本中的“结论和论点”就会变得模糊，什么是“该舍弃的无用信息”，什么是“应该留下的重点内容”，也会变得难以区分。 “这也记、那也记”“先记下再说”“说不定以后会用到”……按照这样的想法记笔记的话，记在笔记上的信息日后被使用的概率几乎为零。 9. ★代谢不良笔记本的坏处如果什么都记、也不进行整理总结，这样记录的笔记只是摘抄罢了，就像中学时使用的摘抄本， 如果不对摘抄进行分类，不时常去翻看，摘抄的内容是不会被完整地存储到记忆当中。","tags":[{"name":"读书","slug":"读书","permalink":"http://nullpointer.pw/tags/读书/"}]},{"title":"《如何有效阅读一本书:超实用读书笔记法》读书笔记","date":"2017-01-15T12:37:00.000Z","path":"《如何有效阅读一本书：超实用读书笔记法》读书笔记.html","text":"买完Kindle读的第三本书，作者的观点部分适用于现在的我，可能是由于环境的不同（作者在日本）。作者提倡使用纸质笔记本来记录读书笔记，但是对于一个出门不喜欢背包的人来说，这很muli~原文对于如何选书、购书说了很多，对于如何记笔记的内容不太多，不过，还是很有参考价值。 书名 作者 开始时间 结束时间 如何有效阅读一本书：超实用读书笔记法 奥野宣之 2017.01.11 2017.01.15 一、购书 ☆只有从日常生活中取材，才能列出反映自己真正需求的购书清单。 ★ 书单内容来源于生活，可能是别人的推荐、也可能是宣传、或者自己脑海中蹦出的关键词。收集这些内容，这样到了书店，在书海中就不会茫然若失，不知道想要买些什么样的书了。有目的地逛书店，买自己想要的书 二、如何做读书笔记1)坚持做读书笔记☆ 2013年8月5日，我读了《XXXX》。这本书比我想象中有意思。 如果写得这样简单，就更容易坚持下去了.第一步要做到的就是坚持下去，毫不夸张地说，只有坚持下去，读书笔记才能发挥作用。读书时应该时刻记得这个目的，正确地对待写读书笔记这件事。 人们经常说“你为别人讲解书中的内容时，才会真正理解它。”把记读书笔记作为目标去读书，得到的效果也是一样的。当你以思想输出为前提去读书时，思想输入的质量也会有所提升，而且亲手写文章的好处比口头叙述要多。 ★ 刚开始学习做读书笔记时，不必苛刻自己要做得多么好，总结得多么妙，最开始的时候，最重要的是培养自己做读书笔记的习惯。读书笔记主要是对自己读书的一种总结，吸收书中的精华，而并非是为了他人而写。 2)读书笔记从一句话开始☆ 书写这一动作，也有整理自己想法的含义。 举个例子，读完冒险题材的纪实文学以后，就会想在读书笔记里写上“这个人真厉害”。你拿起笔时，会想到。等一下，不写上这个人是哪里厉害、为什么厉害的话，下次再读到这句话时会看不懂吧”，于是你会这样写作者的体力和精神承受力都太强了。普通人在失明后一定会感到绝望，就算他比一般人强壮，在这样的环境下也很难生还。为什么作者可以如此坚强? 本来只想写一句话，却把自己的感受一股脑儿地写了下来，并由此展开思考，读书笔记就这样充实了起来。 ★ 一篇上万字的文章也是从一句话开始的，上百万字的小说可能也只是通过一份不过千字的大纲逐渐扩展而来。不怕写不出一万字的长文，只怕连一句最最简单的话都不写。 3)无法坚持做笔记的原因☆ 需要注意的是，千万不要为了坚持写读书笔记而走形式主义。不管采用什么方式，都要踏实地写笔记，并真正理解书的内容。 ★ 并非如作者所说，我们不可能真正理解所有读过的书的内容，但是写笔记一定要踏实地去做便是。 三) 做笔记的步骤1) 针对纸质书 ★ 刚开始认为这样是对于书的一种不爱护的表现，现在才觉得其实这么做的方式并没有多么不好，前提是书的主人是自己。与其看过一遍就扔掉或者再也不看；不如让看过的书留下自己读过的痕迹，更好的吸收书中的内容。 2) 针对电子书Kindle★ 以下针对Kindle的部分为个人总结，书中并未提及。 1.通读:把读起来觉得不错的那一页的中间部分的两个字加入笔记、笔记内容为1（1只是做的一个标记） 由于Kindle在添加笔记时会同时插入一条笔记和一条标注记录、所以需要删除标记那条记录。删除第一条记录、保留笔记内容为1的记录。 2.重读读完一遍之后，再把笔记列表中的几页重新读一遍。如果仍然觉得很好，添加书签 3.标记再重新读一遍加有书签的几页，如果第三次阅读后仍然觉得值得一读，就用标记功能在上面做记号。最后留下来的就是你认为最有用的部分了。这一页上会有通读步骤的笔记记录、有重读步骤加的书签、有标记步骤时划的标记。 可以通过Kindle Mate这个软件将标记的内容导出来，便于做读书笔记。读书笔记可以用有道云笔记，个人使用有道云的Markdown做笔记，如果想保存Kindle电子书中的图片，可以按住左下角和右上角进行截图，会自动保存，连接电脑后Kindel所在盘的根目录可以找到。 四)葱鲔[wěi]锅式读书笔记☆ 葱鲔火锅式读书笔记=摘抄+评论写读书笔记之前注意的三个事项: 写读书笔记的日期 书名 作者名 除了以上三个要素、也可以加上以下两个要素。 对自己来说重要的内容（摘抄） 自己对这篇文章的感想（评论） 步骤: 写上日期，每次换行都要留一行空白，而每个段落之间留两行空白，便于日后重读、插入文字和更正修改。 摘抄和写评论，重读做过记号的内容，严格筛选出自己认为可以多读几遍的部分、最后在摘抄完的文章后面留出一些空白，写上自己的评论。摘抄部分用☆表示，而用自己语言表达的感想和补充说明则标上★以此作为区分，交替标在每个段落前。 五)示例【080715】《决定人类未来的50件事》/杰西卡·威廉姆斯/草思社 ☆自杀者中有三分之二是因为抑郁。(P180) ★作者说，世界上的自杀者比在战争中死去的人还要多。人类在战争平代会死亡，在和平年代也会死亡，真是不容易。 ☆所谓奴隶，就是被剥夺了人权的人。即使是这样，奴隶社会中也存在对待奴隶人性化的情况，奴隶们在饥饿或者生病的时候是可以不工作的。但在现代社会、奴隶就是一次性的财产，被贱买贱卖。 ★以前的奴隶是长期雇佣制，而现在的则是因为抵押贷款而被人身买卖。为了事后不留麻烦，奴隶们被送去做合同工或零工，被任意驱使。一个人活在世上，最不可或缺的到底是人权还是金钱呢? “☆”。后面是摘抄。注意不要省略，要保持原汁原味。 “★”后面则是自己的评论，写一些对摘抄内容的感想 并非所有的书都要按照这种方法，所以我们要根据对作品的重视程度改变笔记的写法 比如像“文件整理技巧”一类的纯实用技巧书、可以不去摘抄，而是只标记“★”并总结要点。如果是小说等休闲读物，可以直接分条书写自己对这本书的感想。 六)寻找最具代表性的语句 ☆ 只有让自己感动的段落才值得摘抄，一定是摘抄让自己心动的语句。 另一方面，选择摘抄段落时，不是找让人觉得“理应如此”的文章，而是觉得“这么一说确实有道理”的内容。 读过一本书以后，对书中内容感同身受固然让人心情愉悦，但这也代表这次读书没有给你带来新东西。相反，如果一篇文章颠覆了你之前的想法，使你的认识发生动摇，在抄写和重读的过程中仍然会让人信服或是感觉震撼，这种文章才是值得摘抄的。 七)激发思想的火花☆ 在摘抄的同时，附上自己的感想和思考，把作者的话和自己的话放在一起，感受落差。好主意不会凭空出现。不论是什么想法，都一定是对某种刺激做出的回应。只有在读完书后对书中内容做出反应，进行主动思考，才能真正掌握这本书的内容。 两个概念 “吸取精华”:原封不动地吸取书上的知识。 “读书体验”: ● “书上写的这些，我是这么理解的” ● “以此为契机，我想到了一件事……” ★ 没有主动思考的阅读，书中的内容被掌握的极其有限，甚至于未读过一样。","tags":[{"name":"读书","slug":"读书","permalink":"http://nullpointer.pw/tags/读书/"}]},{"title":"点击按钮拷贝代码的实现","date":"2016-10-09T06:33:00.000Z","path":"点击按钮拷贝代码的实现.html","text":"例子： 1、 head中引入一个js clipboard.js 1&lt;script src=\"//cdn.bootcss.com/clipboard.js/1.5.15/clipboard.js\"&gt;&lt;/script&gt; 2、初始化对象 123&lt;script type=\"text/javascript\"&gt; new Clipboard('.btn');//此处btn是copy按钮的id&lt;/script&gt; 3、使用copy 1234567&lt;div class=\"col-lg-6 col-md-8 col-sm-8\"&gt; &lt;input type=\"text\" class=\"form-control\" id=\"murl\" value=\"$&#123;murl&#125;\" /&gt;&lt;/div&gt;&lt;button type=\"button\" class=\"btn\" data-clipboard-target=\"#murl\" &gt;复制&lt;/button&gt;&lt;!-- data-clipboard-target 要复制的目标--&gt; 参考 http://clipboardjs.52fhy.com/","tags":[{"name":"JS","slug":"JS","permalink":"http://nullpointer.pw/tags/JS/"}]},{"title":"Ons消息队列","date":"2016-10-02T04:17:00.000Z","path":"Ons消息队列.html","text":"Ons消息队列前引：公司项目需要写一个推广系统，推广系统是一个项目，和主项目分离，项目间的通信使用Ons来完成，以下是公司代码略微修改的结果，以供参考。 术语： Producer：消息生产者，负责产生消息，一般由业务系统负责产生消息。 Consumer：消息消费者，负责消费消息，一般是后台系统负责异步消费。 Producer ID：一类 Producer 的集合名称，这类 Producer 通常发送一类消息，丏发送逻辑一致。 Consumer ID：一类 Consumer 的集合名称，这类 Consumer 通常消费一类消息，丏消费逻辑一致。 广播消费：一条消息被多个 Consumer 消费，即使这些 Consumer 属于同一个 Consumer ID，消息也会被 Consumer ID中的每个 Consumer 都消费一次，广播消费中的 Consumer ID 概念可以讣为在消息划分方面无意义。 集群消费：一个 Consumer ID 中的 Consumer 实例平均分摊消费消息。 例如某个 Topic 有 9 条消息， 其中一个 ConsumerId 有 3 个实例（可能是 3 个进程，或者 3 台机器） ，那么每个实例只消费其中的 3 条消息 引入依赖 Maven方式 12345&lt;dependency&gt; &lt;groupId&gt;com.aliyun.openservices&lt;/groupId&gt; &lt;artifactId&gt;ons-client&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt;&lt;/dependency&gt; jar包方式 http://onsall.oss-cn-hangzhou.aliyuncs.com/aliyun-ons-client-java.tar.gz 一、发送消息1、在阿里云控制台的消息队列中配置 Topic、Producer ID， Consumer ID2、创建Ons消息生产者1234567891011121314151617181920212223242526272829303132333435363738394041@Servicepublic class ProducerService implements IProducerService &#123; private Producer producer; @Override public void sendMessage(String topic, String tag, String content) &#123; try &#123; byte[] bytes = content.getBytes(\"UTF-8\"); Message msg = new Message(topic, tag, bytes); SendResult sendResult = producer.send(msg); System.out.println(\"SendMessageIng... topic:\" + topic + \" tag:\" + tag + \" \"+ sendResult); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; @PostConstruct public void init() &#123; Properties properties = new Properties(); //PropertyKeyConst 是Ons官方定义的常量 //需要在阿里云控制台创建 AccessKey，SecretKey properties.put(PropertyKeyConst.ProducerId, ProducerConstants.PRODUCERID_PCHOME); //PRODUCERID_PCHOME是在控制台配置Topic 的 Producer ID properties.put(PropertyKeyConst.AccessKey, ProducerConstants.ACCESSID); properties.put(PropertyKeyConst.SecretKey, ProducerConstants.ACCESSKEY); producer = ONSFactory.createProducer(properties); // 在发送消息前，必须调用start方法来启动Producer，只需调用一次即可。 producer.start(); &#125; @PreDestroy public void destory() &#123; // 在应用退出前，销毁Producer对象 // 注意：如果不销毁也没有问题 producer.shutdown(); &#125;&#125; 3、创建发送消息类1234567891011121314151617181920212223242526272829import net.minidev.json.JSONObject;import com.aliyun.openservices.ons.api.Action;import com.aliyun.openservices.ons.api.ConsumeContext;import com.aliyun.openservices.ons.api.Message;import com.aliyun.openservices.ons.api.MessageListener;@Servicepublic class OnsService implements IOnsService, MessageListener &#123; @Resource private IProducerService producerService; public static final String TOPIC = \"TopicTestONS\"; public static final String TOPIC_TAG = \"TagA\"; @Override public Action consume(Message message, ConsumeContext context) &#123; return null; &#125; @Override public void sendBindObject(int uid, int tid, int eid, String pageUrl) &#123; JSONObject json = new JSONObject(); json.put(\"uid\", uid); json.put(\"tid\", tid); json.put(\"eid\", eid); json.put(\"pageUrl\", pageUrl); producerService.sendMessage(TOPIC, TOPIC_TAG, json.toJSONString()); //第三个参数就是需要发送的内容，公司使用的Json &#125;&#125; 二、订阅消息1、创建Ons消息消费者1234567891011121314151617181920212223242526272829303132@Service@Lazy(value=false)public class ConsumerService implements IConsumerService &#123; private Consumer consumer; @PostConstruct public void init() &#123; System.out.println(\"init....\"); Properties properties = new Properties(); properties.put(PropertyKeyConst.ConsumerId, ConsumerConstants.CONSUMERID); properties.put(PropertyKeyConst.AccessKey,ConsumerConstants.ACCESSID); properties.put(PropertyKeyConst.SecretKey,ConsumerConstants.ACCESSKEY); properties.put(PropertyKeyConst.MessageModel,PropertyValueConst.CLUSTERING); consumer = ONSFactory.createConsumer(properties); consumer.start(); &#125; //MessageListener 是阿里Ons包内的消息监听器 @Override public void subscribe(final String topic, final String subExpression, final MessageListener listener) &#123; consumer.subscribe(topic, subExpression, listener); &#125; @PreDestroy public void desotry() &#123; if (consumer != null) &#123; consumer.shutdown(); &#125; &#125;&#125; 2、创建消费消息类123456789101112131415161718192021222324@Servicepublic class OnsConsumerService implements IOnsConsumerService &#123; @Resource private IConsumerService consumerService; public static final String TOPIC = \"TopicTestONS\"; @PostConstruct public void init() &#123; System.out.println(\"starting...\"); onsConsumerService.subscribe(TOPIC, \"*\", new MessageListener() &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; JSONObject parse = (JSONObject) JSONValue.parse(message.getBody()); int uid = JsonUtil.getInt(parse, \"uid\"); int tid = JsonUtil.getInt(parse, \"tid\"); int eid = JsonUtil.getInt(parse, \"eid\"); String pageUrl = JsonUtil.getString(parse, \"pageUrl\"); System.out.println(uid + tid + eid + pageUrl); return Action.CommitMessage; &#125; &#125;); &#125;&#125; 以上为Ons的集群订阅消息代码。 参考： Ons官方文档 http://onsteam.oss-cn-hangzhou.aliyuncs.com/ALIYUN_ONS_USER_GUIDE.pdf","tags":[{"name":"Ons","slug":"Ons","permalink":"http://nullpointer.pw/tags/Ons/"},{"name":"消息队列","slug":"消息队列","permalink":"http://nullpointer.pw/tags/消息队列/"}]}]